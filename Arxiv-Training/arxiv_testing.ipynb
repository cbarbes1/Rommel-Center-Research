{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbarbes1/Rommel-Center-Research/blob/Arxiv-testing/Arxiv-Training/arxiv_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIvHveEDOarA",
        "outputId": "6d662b2d-5019-4b0c-c57e-1038712b7b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consistent eccentricities for gravitational wave astronomy: Resolving discrepancies between astrophysical simulations and waveform models\n",
            "Phonon and defect mediated quantum anomalous Hall insulator to metal transition in magnetically doped topological insulators\n",
            "Hybrid acousto-optical swing-up preparation of exciton and biexciton states in a quantum dot\n",
            "Equivalence of cost concentration and gradient vanishing for quantum circuits: an elementary proof in the Riemannian formulation\n",
            "On co-dimension 2 defect anomalies in N=4 SYM and (2,0) theory via brane probes in AdS/CFT\n",
            "Virtual Channel Purification\n",
            "Stabilizer entropy of quantum tetrahedra\n",
            "Exact lower bound of the uncertainty principle product for the harmonic oscillator with position-momentum coupling\n",
            "Encoding Majorana codes\n",
            "Surface gravity in spherically symmetric collapsing stars\n",
            "['Consistent eccentricities for gravitational wave astronomy: Resolving discrepancies between astrophysical simulations and waveform models', 'Phonon and defect mediated quantum anomalous Hall insulator to metal transition in magnetically doped topological insulators', 'Hybrid acousto-optical swing-up preparation of exciton and biexciton states in a quantum dot', 'Equivalence of cost concentration and gradient vanishing for quantum circuits: an elementary proof in the Riemannian formulation', 'On co-dimension 2 defect anomalies in N=4 SYM and (2,0) theory via brane probes in AdS/CFT', 'Virtual Channel Purification', 'Stabilizer entropy of quantum tetrahedra', 'Exact lower bound of the uncertainty principle product for the harmonic oscillator with position-momentum coupling', 'Encoding Majorana codes', 'Surface gravity in spherically symmetric collapsing stars']\n",
            "http://arxiv.org/abs/cond-mat/0603029v1\n",
            "From stripe to checkerboard order on the square lattice in the presence of quenched disorder\n"
          ]
        }
      ],
      "source": [
        "#%pip install arxiv\n",
        "import arxiv\n",
        "\n",
        "client = arxiv.Client()\n",
        "\n",
        "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
        "search = arxiv.Search(\n",
        "  query = \"quantum\",\n",
        "  max_results = 10,\n",
        "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
        ")\n",
        "\n",
        "results = client.results(search)\n",
        "\n",
        "# `results` is a generator; you can iterate over its elements one by one...\n",
        "for r in client.results(search):\n",
        "  print(r.title)\n",
        "# ...or exhaust it into a list. Careful: this is slow for large results sets.\n",
        "all_results = list(results)\n",
        "print([r.title for r in all_results])\n",
        "\n",
        "# For advanced query syntax documentation, see the arXiv API User Manual:\n",
        "# https://arxiv.org/help/api/user-manual#query_details\n",
        "search = arxiv.Search(query = \"au:del_maestro AND ti:checkerboard\")\n",
        "first_result = next(client.results(search))\n",
        "print(first_result)\n",
        "\n",
        "# Search for the paper with ID \"1605.08386v1\"\n",
        "search_by_id = arxiv.Search(id_list=[\"1605.08386v1\"])\n",
        "# Reuse client to fetch the paper, then print its title.\n",
        "first_result = next(client.results(search))\n",
        "print(first_result.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65BgW50xVsMF",
        "outputId": "3ba6e4e8-fe4c-4229-f43b-29f64ec74d77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/linuxlab/home/cbarbes1/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-02-17 14:13:29.304534: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-02-17 14:13:29.332546: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-02-17 14:13:29.333216: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-17 14:13:29.861544: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['The article retraces major events and milestones in the mutual influences\\nbetween mathematical logic and computer science since the 1950s.', 'Although data science builds on knowledge from computer science, mathematics,\\nstatistics, and other disciplines, data science is a unique field with many\\nmysteries to unlock: challenging scientific questions and pressing questions of\\nsocietal importance. This article starts with meta-questions about data science\\nas a discipline and then elaborates on ten ideas for the basis of a research\\nagenda for data science.', 'Data science is gaining more and more and widespread attention, but no\\nconsensus viewpoint on what data science is has emerged. As a new science, its\\nobjects of study and scientific issues should not be covered by established\\nsciences. Data in cyberspace have formed what we call datanature. In the\\npresent paper, data science is defined as the science of exploring datanature.', \"Through online anecdotal evidence and online communities, there is an\\nin-group idea of trans people (specifically trans-feminine individuals)\\ndisproportionately entering computer science education & fields. Existing data\\nsuggests this is a plausible trend, yet no research has been done into exactly\\nwhy. As computer science education (traditional schooling or self-taught\\nmethods) is integral to working in computer science fields, a simple research\\nsurvey was conducted to gather data on 138 trans people's experiences with\\ncomputer science & computer science education. This article's purpose is to\\nshed insight on the motivations for trans individuals choosing computer science\\npaths, while acting as a basis and call to action for further research.\", 'MEMICS provides a forum for doctoral students interested in applications of\\nmathematical and engineering methods in computer science. Besides a rich\\ntechnical programme (including invited talks, regular papers, and\\npresentations), MEMICS also offers friendly social activities and exciting\\nopportunities for meeting like-minded people. MEMICS submissions traditionally\\ncover all areas of computer science (such as parallel and distributed\\ncomputing, computer networks, modern hardware and its design, non-traditional\\ncomputing architectures, information systems and databases, multimedia and\\ngraphics, verification and testing, computer security, as well as all related\\nareas of theoretical computer science).', 'In computational science and in computer science, research software is a\\ncentral asset for research. Computational science is the application of\\ncomputer science and software engineering principles to solving scientific\\nproblems, whereas computer science is the study of computer hardware and\\nsoftware design.\\n  The Open Science agenda holds that science advances faster when we can build\\non existing results. Therefore, research software has to be reusable for\\nadvancing science. Thus, we need proper research software engineering for\\nobtaining reusable and sustainable research software. This way, software\\nengineering methods may improve research in other disciplines. However,\\nresearch in software engineering and computer science itself will also benefit\\nfrom reuse when research software is involved.\\n  For good scientific practice, the resulting research software should be open\\nand adhere to the FAIR principles (findable, accessible, interoperable and\\nrepeatable) to allow repeatability, reproducibility, and reuse. Compared to\\nresearch data, research software should be both archived for reproducibility\\nand actively maintained for reusability. The FAIR data principles do not\\nrequire openness, but research software should be open source software.\\nEstablished open source software licenses provide sufficient licensing options,\\nsuch that it should be the rare exception to keep research software closed.\\n  We review and analyze the current state in this area in order to give\\nrecommendations for making computer science research software FAIR and open. We\\nobserve that research software publishing practices in computer science and in\\ncomputational science show significant differences.', 'We develop a polynomial translation from finite control pi-calculus processes\\nto safe low-level Petri nets. To our knowledge, this is the first such\\ntranslation. It is natural in that there is a close correspondence between the\\ncontrol flows, enjoys a bisimulation result, and is suitable for practical\\nmodel checking.', 'Computational medical XR (extended reality) brings together life sciences and\\nneuroscience with mathematics, engineering, and computer science. It unifies\\ncomputational science (scientific computing) with intelligent extended reality\\nand spatial computing for the medical field. It significantly extends previous\\nClinical XR, by integrating computational methods from neural simulation to\\ncomputational geometry, computational vision and computer graphics up to\\ntheoretical computer science to solve hard problems in medicine and\\nneuroscience: from low-code/no-code authoring medical XR platforms to deep\\nlearning systems for diagnostics, therapeutics, rehabilitation and from\\nsurgical planning to real-time operative navigation in XR.', 'Possible for science itself, conceptually, to have and will understand\\ndifferently, let alone science also seen as technology, such as computer\\nscience. After all, science and technology are viewpoints diverse by either\\nindividual, community, or social. Generally, it depends on socioeconomic\\ncapabilities. So it is with computer science has become a phenomenon and\\nfashionable, where based on the stream of documents, various issues arise in\\neither its theory or implementation, adapting different communities, or\\ndesigning curriculum holds in the education system.', 'We show that the higher-order matching problem is decidable using a\\ngame-theoretic argument.', 'This special issue is dedicated to get a better picture of the relationships\\nbetween computational linguistics and cognitive science. It specifically raises\\ntwo questions: \"what is the potential contribution of computational language\\nmodeling to cognitive science?\" and conversely: \"what is the influence of\\ncognitive science in contemporary computational linguistics?\"', 'This article discusses two books on the topic of alternative logics in\\nscience: \"Deviant Logic\", by Susan Haack, and \"Alternative Logics: Do Sciences\\nNeed Them?\", edited by Paul Weingartner.', 'In contrast to many other scientific disciplines, computer science considers\\nconference publications. Conferences have the advantage of providing fast\\npublication of papers and of bringing researchers together to present and\\ndiscuss the paper with peers. Previous work on knowledge mapping focused on the\\nmap of all sciences or a particular domain based on ISI published JCR (Journal\\nCitation Report). Although this data covers most of important journals, it\\nlacks computer science conference and workshop proceedings. That results in an\\nimprecise and incomplete analysis of the computer science knowledge. This paper\\npresents an analysis on the computer science knowledge network constructed from\\nall types of publications, aiming at providing a complete view of computer\\nscience research. Based on the combination of two important digital libraries\\n(DBLP and CiteSeerX), we study the knowledge network created at\\njournal/conference level using citation linkage, to identify the development of\\nsub-disciplines. We investigate the collaborative and citation behavior of\\njournals/conferences by analyzing the properties of their co-authorship and\\ncitation subgraphs. The paper draws several important conclusions. First,\\nconferences constitute social structures that shape the computer science\\nknowledge. Second, computer science is becoming more interdisciplinary. Third,\\nexperts are the key success factor for sustainability of journals/conferences.', 'We investigate conditions under which a co-computably enumerable closed set\\nin a computable metric space is computable and prove that in each locally\\ncomputable computable metric space each co-computably enumerable compact\\nmanifold with computable boundary is computable. In fact, we examine the notion\\nof a semi-computable compact set and we prove a more general result: in any\\ncomputable metric space each semi-computable compact manifold with computable\\nboundary is computable. In particular, each semi-computable compact\\n(boundaryless) manifold is computable.', 'Theoretical computer science discusses foundational issues about\\ncomputations. It asks and answers questions such as \"What is a computation?\",\\n\"What is computable?\", \"What is efficiently computable?\",\"What is\\ninformation?\", \"What is random?\", \"What is an algorithm?\", etc. We will present\\nmany of the major themes and theorems with the basic language of category\\ntheory. Surprisingly, many interesting theorems and concepts of theoretical\\ncomputer science are easy consequences of functoriality and composition when\\nyou look at the right categories and functors connecting them.', 'Since its inception at the beginning of the twentieth century, quantum\\nmechanics has challenged our conceptions of how the universe ought to work;\\nhowever, the equations of quantum mechanics can be too computationally\\ndifficult to solve using existing computers for even modestly large systems.\\nHere I will show that quantum computers can sometimes be used to address such\\nproblems and that quantum computer science can assign formal complexities to\\nlearning facts about nature. Hence, computer science should not only be\\nregarded as an applied science; it is also of central importance to the\\nfoundations of science.', 'This paper outlines ongoing dissertation research located in the intersection\\nof science fiction, human-computer interaction and computer science. Through an\\ninterdisciplinary perspective, drawing from fields such as human-computer\\ninteraction, film theory and studies of science and technology, qualitative and\\nquantitative content analysis techniques are used to contextually analyze\\nexpressions of science fiction in peer-reviewed computer science research\\nrepositories, such as the ACM or IEEE Xplore Digital Libraries. This paper\\nconcisely summarizes and introduces the relationship of science fiction and\\ncomputer science research and presents the research questions, aims and\\nimplications in addition to prior work and study methodology. In the latter\\npart of this work-in-progress report, preliminary results, current limitations,\\nfuture work and a post-dissertation trajectory are outlined.', 'The problem of synthesis in computer sciences, including cybernetics,\\nartificial intelligence and system analysis, is analyzed. Main methods of\\nrealization this problem are discussed. Ways of search universal method of\\ncreation universal synthetic science are represented. As example of such\\nuniversal method polymetric analysis is given. Perspective of further\\ndevelopment of this research, including application polymetric method for the\\nresolution main problems of computer sciences, is analyzed too.', 'Data and Science has stood out in the generation of results, whether in the\\nprojects of the scientific domain or business domain. CERN Project, Scientific\\nInstitutes, companies like Walmart, Google, Apple, among others, need data to\\npresent their results and make predictions in the competitive data world. Data\\nand Science are words that together culminated in a globally recognized term\\ncalled Data Science. Data Science is in its initial phase, possibly being part\\nof formal sciences and also being presented as part of applied sciences,\\ncapable of generating value and supporting decision making. Data Science\\nconsiders science and, consequently, the scientific method to promote decision\\nmaking through data intelligence. In many cases, the application of the method\\n(or part of it) is considered in Data Science projects in scientific domain\\n(social sciences, bioinformatics, geospatial projects) or business domain\\n(finance, logistic, retail), among others. In this sense, this article\\naddresses the perspectives of Data Science as a multidisciplinary area,\\nconsidering science and the scientific method, and its formal structure which\\nintegrate Statistics, Computer Science, and Business Science, also taking into\\naccount Artificial Intelligence, emphasizing Machine Learning, among others.\\nThe article also deals with the perspective of applied Data Science, since Data\\nScience is used for generating value through scientific and business projects.\\nData Science persona is also discussed in the article, concerning the education\\nof Data Science professionals and its corresponding profiles, since its\\nprojection changes the field of data in the world.', \"With parallel and distributed computing (PDC) now wide-spread, modern\\ncomputing programs must incorporate PDC within the curriculum. ACM and IEEE\\nComputer Society's Computer Science curricular guidelines have recommended\\nexposure to PDC concepts since 2013. More recently, a variety of initiatives\\nhave made PDC curricular content, lectures, and labs freely available for\\nundergraduate computer science programs. Despite these efforts, progress in\\nensuring computer science students graduate with sufficient PDC exposure has\\nbeen uneven.\\n  This paper discusses the impact of ABET's revised criteria that have required\\nexposure to PDC to achieve accreditation for computer science programs since\\n2018. The authors reviewed 20 top ABET-accredited computer science programs and\\nanalyzed how they covered the required PDC components in their curricula. Using\\ntheir own institutions as case studies, the authors examine in detail how three\\ndifferent ABET-accredited computer science programs covered PDC using different\\napproaches, yet meeting the PDC requirements of these ABET criteria. The paper\\nalso shows how ACM/IEEE Computer Society curricular guidelines for computer\\nengineering and software engineering programs, along with ABET accreditation\\ncriteria, can cover PDC.\", 'I show that, if a term is $SN$ for $\\\\beta$, it remains $SN$ when some\\npermutation rules are added.', 'A summary of work on distributed games and strategies done within the first\\nthree years of the ERC project ECSYM is presented.', 'This note provides conditions under which the union of three well-founded\\nbinary relations is also well-founded.', 'The Cloud has become a principal paradigm of computing in the last ten years,\\nand Computer Science curricula must be updated to reflect that reality. This\\npaper examines simple ways to accomplish curriculum cloudification using Amazon\\nWeb Services (AWS), for Computer Science and other disciplines such as\\nBusiness, Communication and Mathematics.', 'The influence of Alfred Tarski on computer science was indirect but\\nsignificant in a number of directions and was in certain respects fundamental.\\nHere surveyed is the work of Tarski on the decision procedure for algebra and\\ngeometry, the method of elimination of quantifiers, the semantics of formal\\nlanguages, modeltheoretic preservation theorems, and algebraic logic; various\\nconnections of each with computer science are taken up.', 'This paper describes the solution of Hello World transformations in MOLA\\ntransformation language. Transformations implementing the task are relatively\\nstraightforward and easily inferable from the task specification. The required\\nadditional steps related to model import and export are also described.', 'Oxford-style debating is a well-known tool in social sciences. Such formal\\ndiscussions on particular topics are widely used by historians and\\nsociologists. However, when we try to go beyond standard thinking, it turns out\\nthat Oxford-style debating can be a great educational tool in telecommunication\\nand computer science. This article presents this unusual method of education at\\ntechnical universities and in the IT industry, and describes its features and\\nchallenges. Best practices and examples of debating are provided, taking into\\naccount emerging topics in telecommunications and computer science, such as\\ncybersecurity. The article also contains feedback from IT engineers who\\nparticipated in Oxford-style debates. All this aims to encourage this form of\\neducation in telecommunication and computer science.', 'Modern society is permeated with computers, and the software that controls\\nthem can have latent, long-term, and immediate effects that reach far beyond\\nthe actual users of these systems. This places researchers in Computer Science\\nand Software Engineering in a critical position of influence and\\nresponsibility, more than any other field because computer systems are vital\\nresearch tools for other disciplines. This essay presents several key ethical\\nconcerns and responsibilities relating to research in computing. The goal is to\\npromote awareness and discussion of ethical issues among computer science\\nresearchers. A hypothetical case study is provided, along with questions for\\nreflection and discussion.', 'The \"IMP Science Gateway Portal\" (http://scigate.imp.kiev.ua) for complex\\nworkflow management and integration of distributed computing resources (like\\nclusters, service grids, desktop grids, clouds) is presented. It is created on\\nthe basis of WS-PGRADE and gUSE technologies, where WS-PGRADE is designed for\\nscience workflow operation and gUSE - for smooth integration of available\\nresources for parallel and distributed computing in various heterogeneous\\ndistributed computing infrastructures (DCI). The typical scientific workflow\\nwith possible scenarios of its preparation and usage is considered. Several\\ntypical science applications (scientific workflows) are considered for\\nmolecular dynamics (MD) simulations of complex behavior of various\\nnanostructures (nanoindentation of graphene layers, defect system relaxation in\\nmetal nanocrystals, thermal stability of boron nitride nanotubes, etc.). The\\nadvantages and drawbacks of the solution are shortly analyzed in the context of\\nits practical applications for MD simulations in materials science, physics and\\nnanotechnologies with available heterogeneous DCIs.', 'From a computer science perspective, addressing on-line hate speech is a\\nchallenging task that is attracting the attention of both industry (mainly\\nsocial media platform owners) and academia. In this chapter, we provide an\\noverview of state-of-the-art data-science approaches - how they define hate\\nspeech, which tasks they solve to mitigate the phenomenon, and how they address\\nthese tasks. We limit our investigation mostly to (semi-)automatic detection of\\nhate speech, which is the task that the majority of existing computer science\\nworks focus on. Finally, we summarize the challenges and the open problems in\\nthe current data-science research and the future directions in this field. Our\\naim is to prepare an easily understandable report, capable to promote the\\nmultidisciplinary character of hate speech research. Researchers from other\\ndomains (e.g., psychology and sociology) can thus take advantage of the\\nknowledge achieved in the computer science domain but also contribute back and\\nhelp improve how computer science is addressing that urgent and socially\\nrelevant issue which is the prevalence of hate speech in social media.', 'The ubiquitous presence of computer simulations in all kinds of research\\nareas evidence their role as the new driving force for the advancement of\\nscience and engineering research. Nothing seems to escape the image of success\\nthat computer simulations project onto the research community and the general\\npublic. One simple way to illustrate this consists of asking ourselves how\\nwould contemporary science and engineering look like without the use of\\ncomputer simulations. The answer would certainly diverge from the current image\\nwe have of scientific and engineering research.\\n  As much as computer simulations are successful, they are also methods that\\nfail in their purpose of inquiring about the world; and as much as researchers\\nmake use of them, computer simulations raise important questions that are at\\nthe heart of contemporary science and engineering practice. In this respect,\\ncomputer simulations make a fantastic subject of research for the natural\\nsciences, the social sciences, engineering and, as in our case, also for\\nphilosophy. Studies on computer simulations touch upon many different facets of\\nscientific and engineering research and evoke philosophically inclined\\nquestions of interpretation with close ties to problems in experimental\\nsettings and engineering applications (...)', \"The rapid progress of computer science has been accompanied by a\\ncorresponding evolution of computation, from classical computation to quantum\\ncomputation. As quantum computing is on its way to becoming an established\\ndiscipline of computing science, much effort is being put into the development\\nof new quantum algorithms. One of quantum algorithms is Grover algorithm, which\\nis used for searching an element in an unstructured list of N elements with\\nquadratic speed-up over classical algorithms. In this work, Quantum Computer\\nLanguage (QCL) is used to make a Grover's quantum search simulation in a\\nclassical computer\", 'The fields of computing and biology have begun to cross paths in new ways. In\\nthis paper a review of the current research in biological computing is\\npresented. Fundamental concepts are introduced and these foundational elements\\nare explored to discuss the possibilities of a new computing paradigm. We\\nassume the reader to possess a basic knowledge of Biology and Computer Science', \"The National Science Foundation's Transdisciplinary Research in Principles of\\nData Science (TRIPODS) program aims to integrate three areas central to the\\nfoundations of data by uniting the statistics, mathematics, and theoretical\\ncomputer science research communities. The program aims to provide a model for\\nfunding cross-cutting research and facilitating interactions among the three\\ndisciplines. Challenges associated with orchestrating fruitful interactions are\\ndescribed.\", 'Computer science enrollments have started to rise again, but the percentage\\nof women undergraduates in computer science is still low. Some studies indicate\\nthis might be due to a lack of awareness of computer science at the high school\\nlevel. We present our experiences running a 5-year, high school outreach\\nprogram that introduces information about computer science within the context\\nof required chemistry courses. We developed interactive worksheets using\\nMolecular Workbench that help the students learn chemistry and computer science\\nconcepts related to relevant events such as the gulf oil spill. Our evaluation\\nof the effectiveness of this approach indicates that the students do become\\nmore aware of computer science as a discipline, but system support issues in\\nthe classroom can make the approach difficult for teachers and discouraging for\\nthe students.', 'The theory of quantum computation is presented in a self contained way from a\\ncomputer science perspective. The basics of classical computation and quantum\\nmechanics is reviewed. The circuit model of quantum computation is presented in\\ndetail. Throughout there is an emphasis on the physical as well as the abstract\\naspects of computation and the interplay between them.\\n  This report is presented as a Master\\'s thesis at the department of Computer\\nScience and Engineering at G{\\\\\"o}teborg University, G{\\\\\"o}teborg, Sweden.\\n  The text is part of a larger work that is planned to include chapters on\\nquantum algorithms, the quantum Turing machine model and abstract approaches to\\nquantum computation.', 'Motivated by an intention to remedy current complications with Dutch\\nterminology concerning informatics, the term informaticology is positioned to\\ndenote an academic counterpart of informatics where informatics is conceived of\\nas a container for a coherent family of practical disciplines ranging from\\ncomputer engineering and software engineering to network technology, data\\ncenter management, information technology, and information management in a\\nbroad sense.\\n  Informaticology escapes from the limitations of instrumental objectives and\\nthe perspective of usage that both restrict the scope of informatics. That is\\nachieved by including fiction science in informaticology and by ranking fiction\\nscience on equal terms with computer science and data science, and framing (the\\nstudy of) game design, evelopment, assessment and distribution, ranging from\\nserious gaming to entertainment gaming, as a chapter of fiction science. A\\nsuggestion for the scope of fiction science is specified in some detail.\\n  In order to illustrate the coherence of informaticology thus conceived, a\\npotential application of fiction to the ontology of instruction sequences and\\nto software quality assessment is sketched, thereby highlighting a possible\\nrole of fiction (science) within informaticology but outside gaming.', 'Emerging research frontiers and computational advances have gradually\\ntransformed cognitive science into a multidisciplinary and data-driven field.\\nAs a result, there is a proliferation of cognitive theories investigated and\\ninterpreted from different academic lens and in different levels of\\nabstraction. We formulate this applied aspect of this challenge as the\\ncomputational cognitive inference, and describe the major routes of\\ncomputational approaches. To balance the potential optimism alongside the speed\\nand scale of the data-driven era of cognitive science, we propose to inspect\\nthis trend in more empirical terms by identifying the operational challenges,\\nsocietal impacts and ethical guidelines in conducting research and interpreting\\nresults from the computational inference in cognitive science.', \"The unprecedented growth in the availability of data of all types and\\nqualities and the emergence of the field of data science has provided an\\nimpetus to finally realizing the implementation of the full breadth of the\\nNolan and Temple Lang proposed integration of computing concepts into\\nstatistics curricula at all levels in statistics and new data science programs\\nand courses. Moreover, data science, implemented carefully, opens accessible\\npathways to stem for students for whom neither mathematics nor computer science\\nare natural affinities, and who would traditionally be excluded. We discuss a\\nproposal for the stealth development of computational skills in students' first\\nexposure to data science through careful, scaffolded exposure to computation\\nand its power. The intent of this approach is to support students, regardless\\nof interest and self-efficacy in coding, in becoming data-driven learners, who\\nare capable of asking complex questions about the world around them, and then\\nanswering those questions through the use of data-driven inquiry. This\\ndiscussion is presented in the context of the International Data Science in\\nSchools Project which recently published computer science and statistics\\nconsensus curriculum frameworks for a two-year secondary school data science\\nprogram, designed to make data science accessible to all.\", 'Continuous developments in data science have brought forth an exponential\\nincrease in complexity of machine learning models. Additionally, data\\nscientists have become ubiquitous in the private market, academic environments\\nand even as a hobby. All of these trends are on a steady rise, and are\\nassociated with an increase in power consumption and associated carbon\\nfootprint. The increasing carbon footprint of large-scale advanced data science\\nhas already received attention, but the latter trend has not. This work aims to\\nestimate the contribution of the increasingly popular \"common\" data science to\\nthe global carbon footprint. To this end, the power consumption of several\\ntypical tasks in the aforementioned common data science tasks will be measured\\nand compared to: large-scale \"advanced\" data science, common computer-related\\ntasks, and everyday non-computer related tasks. This is done by converting the\\nmeasurements to the equivalent unit of \"km driven by car\". Our main findings\\nare: \"common\" data science consumes $2.57$ more power than regular computer\\nusage, but less than some common everyday power-consuming tasks such as\\nlighting or heating; large-scale data science consumes substantially more power\\nthan common data science.', 'Advances in Artificial Intelligence require progress across all of computer\\nscience.', 'This volume represents the proceedings of the 5th Workshop on Membrane\\nComputing and Biologically Inspired Process Calculi (MeCBIC 2011), held\\ntogether with the 12th International Conference on Membrane Computing on 23rd\\nAugust 2011 in Fontainebleau, France.', 'Cloud computing has recently evolved as a popular computing infrastructure\\nfor many applications. Scientific computing, which was mainly hosted in private\\nclusters and grids, has started to migrate development and deployment to the\\npublic cloud environment. eScience as a service becomes an emerging and\\npromising direction for science computing. We review recent efforts in\\ndeveloping and deploying scientific computing applications in the cloud. In\\nparticular, we introduce a taxonomy specifically designed for scientific\\ncomputing in the cloud, and further review the taxonomy with four major kinds\\nof science applications, including life sciences, physics sciences, social and\\nhumanities sciences, and climate and earth sciences. Our major finding is that,\\ndespite existing efforts in developing cloud-based eScience, eScience still has\\na long way to go to fully unlock the power of cloud computing paradigm.\\nTherefore, we present the challenges and opportunities in the future\\ndevelopment of cloud-based eScience services, and call for collaborations and\\ninnovations from both the scientific and computer system communities to address\\nthose challenges.', 'This text provides with an introduction to the modern approach of\\nartificiality and simulation in social sciences. It presents the relationship\\nbetween complexity and artificiality, before introducing the field of\\nartificial societies which greatly benefited from the computer power fast\\nincrease, gifting social sciences with formalization and experimentation tools\\npreviously owned by \"hard\" sciences alone. It shows that as \"a new way of doing\\nsocial sciences\", artificial societies should undoubtedly contribute to a\\nrenewed approach in the study of sociality and should play a significant part\\nin the elaboration of original theories of social phenomena.', 'Scientific discovery evolves from the experimental, through the theoretical\\nand computational, to the current data-intensive paradigm. Materials science is\\nno exception, especially for computational materials science. In recent years,\\ngreat achievements have been made in the field of materials science and\\nengineering (MSE). Here, we review the previous paradigms of materials science\\nand some classical MSE models. Then, our data-intensive MSE (DIMSE) model is\\nproposed to reshape future materials innovations. This work will help to\\naddress the global challenge for materials discovery in the era of artificial\\nintelligence (AI), and essentially contribute to accelerating future materials\\ncontinuum.', 'Adopting cryptography has given rise to a significant evolution in Artificial\\nIntelligence (AI). This paper studies the path and stages of this evolution. We\\nstart with reviewing existing relevant surveys, noting their shortcomings,\\nespecially the lack of a close look at the evolution process and solid future\\nroadmap. These shortcomings justify the work of this paper. Next, we identify,\\ndefine and discuss five consequent stages in the evolution path, including\\nCrypto-Sensitive AI, Crypto-Adapted AI, Crypto-Friendly AI, Crypto-Enabled AI,\\nCrypto-Protected AI. Then, we establish a future roadmap for further research\\nin this area, focusing on the role of quantum-inspired and bio-inspired AI.', 'The self-similar representation for the Schr\\\\\"{o}dinger equation is derived.', 'These are the proceedings of the 7th European Conference on Python in\\nScience, EuroSciPy 2014, that was held in Cambridge, UK (27-30 August 2014).', 'Spatial data science has emerged in recent years as an interdisciplinary\\nfield. This position paper discusses the importance of building and sharing\\nhigh-quality datasets for spatial data science.', 'Recognizable languages of finite words are part of every computer science\\ncursus, and they are routinely described as a cornerstone for applications and\\nfor theory. We would like to briefly explore why that is, and how this\\nword-related notion extends to more complex models, such as those developed for\\nmodeling distributed or timed behaviors.', 'The extension of the Wagner hierarchy to blind counter automata accepting\\ninfinite words with a Muller acceptance condition is effective. We determine\\nprecisely this hierarchy.', 'This is a position paper written as an introduction to the special volume on\\nquantum algorithms I edited for the journal Mathematical Structures in Computer\\nScience (Volume 20 - Special Issue 06 (Quantum Algorithms), 2010).', 'Typed operational semantics is a method developed by H. Goguen to prove\\nmeta-theoretic properties of type systems. This paper studies the metatheory of\\na type system with dependent record types, using the approach of typed\\noperational semantics. In particular, the metatheoretical properties we have\\nproved include strong normalisation, Church-Rosser and subject reduction.', 'Connections between the sequentiality/concurrency distinction and the\\nsemantics of proofs are investigated, with particular reference to games and\\nLinear Logic.', 'The paper addresses some fundamental and hotly debated issues for high-stakes\\nevent predictions underpinning the computational approach to social sciences.\\nWe question several prevalent views against machine learning and outline a new\\nparadigm that highlights the promises and promotes the infusion of\\ncomputational methods and conventional social science approaches.', 'We introduce a new ACL2 feature, the abstract stobj, and show how to apply it\\nto modeling the instruction set architecture of a microprocessor. Benefits of\\nabstract stobjs over traditional (\"concrete\") stobjs can include faster\\nexecution, support for symbolic simulation, more efficient reasoning, and\\nresilience of proof developments under modeling optimization.', 'The PrePost (Pre- and Post-Deployment Verification Techniques) workshop aimed\\nat bringing together researchers working in the field of computer-aided\\nvalidation and verification to discuss the connections and interplay between\\npre- and post-deployment verification techniques. Examples of the topics\\ncovered by the workshop are the relationships between classic model checking\\nand testing on the one hand and runtime verification and statistical model\\nchecking on the other, and between type systems that may be checked either\\nstatically or dynamically through techniques such as runtime monitoring.', 'This paper reviews literature in cognitive science, human-computer\\ninteraction (HCI) and natural-language processing (NLP) to consider how\\nanalogical reasoning (AR) could help inform the design of communication and\\nlearning technologies, as well as online communities and digital platforms.\\nFirst, analogical reasoning (AR) is defined, and use-cases of AR in the\\ncomputing sciences are presented. The concept of schema is introduced, along\\nwith use-cases in computing. Finally, recommendations are offered for future\\nwork on using analogical reasoning and schema methods in the computing\\nsciences.', \"This paper presents CS-Insights, an interactive web application to analyze\\ncomputer science publications from DBLP through multiple perspectives. The\\ndedicated interfaces allow its users to identify trends in research activity,\\nproductivity, accessibility, author's productivity, venues' statistics, topics\\nof interest, and the impact of computer science research on other fields.\\nCS-Insightsis publicly available, and its modular architecture can be easily\\nadapted to domains other than computer science.\", 'Inspired by computer assisted proofs in analysis, we present an interval\\napproach to real-number computations.', 'The twenty-first century has ushered in the age of big data and data economy,\\nin which data DNA, which carries important knowledge, insights and potential,\\nhas become an intrinsic constituent of all data-based organisms. An appropriate\\nunderstanding of data DNA and its organisms relies on the new field of data\\nscience and its keystone, analytics. Although it is widely debated whether big\\ndata is only hype and buzz, and data science is still in a very early phase,\\nsignificant challenges and opportunities are emerging or have been inspired by\\nthe research, innovation, business, profession, and education of data science.\\nThis paper provides a comprehensive survey and tutorial of the fundamental\\naspects of data science: the evolution from data analysis to data science, the\\ndata science concepts, a big picture of the era of data science, the major\\nchallenges and directions in data innovation, the nature of data analytics, new\\nindustrialization and service opportunities in the data economy, the profession\\nand competency of data education, and the future of data science. This article\\nis the first in the field to draw a comprehensive big picture, in addition to\\noffering rich observations, lessons and thinking about data science and\\nanalytics.', \"This paper studies the complexity of classical modal logics and of their\\nextension with fixed-point operators, using translations to transfer results\\nacross logics. In particular, we show several complexity results for\\nmulti-agent logics via translations to and from the mu-calculus and modal\\nlogic, which allow us to transfer known upper and lower bounds. We also use\\nthese translations to introduce a terminating tableau system for the logics we\\nstudy, based on Kozen's tableau for the mu-calculus, and the one of Fitting and\\nMassacci for modal logic.\", 'Nolan and Temple Lang (2010) argued for the fundamental role of computing in\\nthe statistics curriculum. In the intervening decade the statistics education\\ncommunity has acknowledged that computational skills are as important to\\nstatistics and data science practice as mathematics. There remains a notable\\ngap, however, between our intentions and our actions. In this special issue of\\nthe *Journal of Statistics and Data Science Education* we have assembled a\\ncollection of papers that (1) suggest creative structures to integrate\\ncomputing, (2) describe novel data science skills and habits, and (3) propose\\nways to teach computational thinking. We believe that it is critical for the\\ncommunity to redouble our efforts to embrace sophisticated computing in the\\nstatistics and data science curriculum. We hope that these papers provide\\nuseful guidance for the community to move these efforts forward.', \"Language understanding is a key scientific issue in the fields of cognitive\\nand computer science. However, the two disciplines differ substantially in the\\nspecific research questions. Cognitive science focuses on analyzing the\\nspecific mechanism of the brain and investigating the brain's response to\\nlanguage; few studies have examined the brain's language system as a whole. By\\ncontrast, computer scientists focus on the efficiency of practical applications\\nwhen choosing research questions but may ignore the most essential laws of\\nlanguage. Given these differences, can a combination of the disciplines offer\\nnew insights for building intelligent language models and studying language\\ncognitive mechanisms? In the following text, we first review the research\\nquestions, history, and methods of language understanding in cognitive and\\ncomputer science, focusing on the current progress and challenges. We then\\ncompare and contrast the research of language understanding in cognitive and\\ncomputer sciences. Finally, we review existing work that combines insights from\\nlanguage cognition and language computation and offer prospects for future\\ndevelopment trends.\", 'We investigate conditions under which a co-computably enumerable set in a\\ncomputable metric space is computable. Using higher-dimensional chains and\\nspherical chains we prove that in each computable metric space which is locally\\ncomputable each co-computably enumerable sphere is computable and each co-c.e.\\ncell with co-c.e. boundary sphere is computable.', 'This volume contains papers presented at the Ninth International Symposium on\\nSymbolic Computation in Software Science, SCSS 2021.\\n  Symbolic Computation is the science of computing with symbolic objects\\n(terms, formulae, programs, representations of algebraic objects, etc.).\\nPowerful algorithms have been developed during the past decades for the major\\nsubareas of symbolic computation: computer algebra and computational logic.\\nThese algorithms and methods are successfully applied in various fields,\\nincluding software science, which covers a broad range of topics about software\\nconstruction and analysis.\\n  Meanwhile, artificial intelligence methods and machine learning algorithms\\nare widely used nowadays in various domains and, in particular, combined with\\nsymbolic computation. Several approaches mix artificial intelligence and\\nsymbolic methods and tools deployed over large corpora to create what is known\\nas cognitive systems. Cognitive computing focuses on building systems that\\ninteract with humans naturally by reasoning, aiming at learning at scale.\\n  The purpose of SCSS is to promote research on theoretical and practical\\naspects of symbolic computation in software science, combined with modern\\nartificial intelligence techniques. These proceedings contain the keynote paper\\nby Bruno Buchberger and ten contributed papers. Besides, the conference program\\nincluded three invited talks, nine short and work-in-progress papers, and a\\nspecial session on computer algebra and computational logic. Due to the\\nCOVID-19 pandemic, the symposium was held completely online. It was organized\\nby the Research Institute for Symbolic Computation (RISC) of the Johannes\\nKepler University Linz on September 8--10, 2021.', 'Terms like \"Science of Cyber\" or \"Cyber Science\" have been appearing in\\nliterature with growing frequency, and influential organizations initiated\\nresearch initiatives toward developing such a science even though it is not\\nclearly defined. We propose to define the domain of the science of cyber\\nsecurity by noting the most salient artifact within cyber security -- malicious\\nsoftware -- and defining the domain as comprised of phenomena that involve\\nmalicious software (as well as legitimate software and protocols used\\nmaliciously) used to compel a computing device or a network of computing\\ndevices to perform actions desired by the perpetrator of malicious software\\n(the attacker) and generally contrary to the intent (the policy) of the\\nlegitimate owner or operator (the defender) of the computing device(s). We\\nfurther define the science of cyber security as the study of relations --\\npreferably expressed as theoretically-grounded models -- between attributes,\\nstructures and dynamics of: violations of cyber security policy; the network of\\ncomputing devices under attack; the defenders\\' tools and techniques; and the\\nattackers\\' tools and techniques where malicious software plays the central\\nrole. We offer a simple formalism of these key objects within cyber science and\\nsystematically derive a classification of primary problem classes within cyber\\nscience.', \"We investigate the development of scientific content knowledge of volunteers\\nparticipating in online citizen science projects in the Zooniverse\\n(www.zooniverse.org), including the astronomy projects Galaxy Zoo\\n(www.galaxyzoo.org) and Planet Hunters (www.planethunters.org). We use\\neconometric methods to test how measures of project participation relate to\\nsuccess in a science quiz, controlling for factors known to correlate with\\nscientific knowledge. Citizen scientists believe they are learning about both\\nthe content and processes of science through their participation. Won't don't\\ndirectly test the latter, but we find evidence to support the former - that\\nmore actively engaged participants perform better in a project-specific science\\nknowledge quiz, even after controlling for their general science knowledge. We\\ninterpret this as evidence of learning of science content inspired by\\nparticipation in online citizen science.\", 'Leveraging the prevailing interest in computer games among college students,\\nboth for entertainment and as a possible career path, is a major reason for the\\nincreasing prevalence of computer game design courses in computer science\\ncurricula. Because implementing a computer game requires strong programming\\nskills, game design courses are most often restricted to more advanced computer\\nscience students. This paper reports on a ready-made game design and\\nexperimentation framework, implemented in Java, that makes game programming\\nmore widely accessible. This framework, called Labyrinth, enables students at\\nall programming skill levels to participate in computer game design. We\\ndescribe the architecture of the framework, and discuss programming projects\\nsuitable for a wide variety of computer science courses, from capstone to\\nnon-major.', 'The halting problem is considered to be an essential part of the theoretical\\nbackground to computing. That halting is not in general computable has\\nsupposedly been proved in many text books and taught on many computer science\\ncourses, in order to illustrate the limits of computation. However, Eric Hehner\\nhas a dissenting view, in which the specification of the halting problem is\\ncalled into question.', 'Using an iterative tree construction we show that for simple computable\\nsubsets of the Cantor space Hausdorff, constructive and computable dimensions\\nmight be incomputable.', 'The notion of Schnorr randomness refers to computable reals or computable\\nfunctions. We propose a version of Schnorr randomness for subcomputable classes\\nand characterize it in different ways: by Martin L\\\\\"of tests, martingales or\\nmeasure computable machines.', \"This dissertation gives an overview of Martin Lof's dependant type theory,\\nfocusing on its computational content and addressing a question of possibility\\nof fully canonical and computable semantic presentation.\", 'As we are fast approaching the beginning of a paradigm shift in the field of\\nscience, Data driven science (the so called fourth science paradigm) is going\\nto be the driving force in research and innovation. From medicine to\\nbiodiversity and astronomy to geology, all these terms are somehow going to be\\naffected by this paradigm shift. The huge amount of data to be processed under\\nthis new paradigm will be a major concern in the future and one will strongly\\nrequire cloud based services in all the aspects of these computations (from\\nstorage to compute and other services). Another aspect will be energy\\nconsumption and performance of prediction jobs and tasks within such a\\nscientific paradigm which will change the way one sees computation. Data\\nscience has heavily impacted or rather triggered the emergence of Machine\\nLearning, Signal/Image/Video processing related algorithms, Artificial\\nintelligence, Robotics, health informatics, geoinformatics, and many more such\\nareas of interest. Hence, we envisage an era where Data science can deliver its\\npromises with the help of the existing cloud based platforms and services with\\nthe addition of new services. In this article, we discuss about data driven\\nscience and Machine learning and how they are going to be linked through cloud\\nbased services in the future. It also discusses the rise of paradigms like\\napproximate computing, quantum computing and many more in recent times and\\ntheir applicability in big data processing, data science, analytics, prediction\\nand machine learning in the cloud environments.', 'This paper presents results of topic modeling and network models of topics\\nusing the International Conference on Computational Science corpus, which\\ncontains domain-specific (computational science) papers over sixteen years (a\\ntotal of 5695 papers). We discuss topical structures of International\\nConference on Computational Science, how these topics evolve over time in\\nresponse to the topicality of various problems, technologies and methods, and\\nhow all these topics relate to one another. This analysis illustrates\\nmultidisciplinary research and collaborations among scientific communities, by\\nconstructing static and dynamic networks from the topic modeling results and\\nthe keywords of authors. The results of this study give insights about the past\\nand future trends of core discussion topics in computational science. We used\\nthe Non-negative Matrix Factorization topic modeling algorithm to discover\\ntopics and labeled and grouped results hierarchically.', \"An inclusive science, technology, engineering and mathematics (STEM)\\nworkforce is needed to maintain America's leadership in the scientific\\nenterprise. Increasing the participation of underrepresented groups in STEM,\\nincluding persons with disabilities, requires national attention to fully\\nengage the nation's citizens in transforming its STEM enterprise. To address\\nthis need, a number of initiatives, such as AccessCSforALL, Bootstrap, and\\nCSforAll, are making efforts to make Computer Science inclusive to the 7.4\\nmillion K-12 students with disabilities in the U.S. Of special interest to our\\nproject are those K-12 students with hearing impairments. American Sign\\nLanguage (ASL) is the primary means of communication for an estimated 500,000\\npeople in the United States, yet there are limited online resources providing\\nComputer Science instruction in ASL. This paper introduces a new project\\ndesigned to support Deaf and Hard of Hearing (DHH) K-12 students and sign\\ninterpreters in acquiring knowledge of complex Computer Science concepts. We\\ndiscuss the motivation for the project and an early design of the accessible\\nblock-based Computer Science curriculum to engage DHH students in hands-on\\ncomputing education.\", 'Computer Science for Future (CS4F) is an initiative in the Department of\\nComputer Science at HAW Hamburg. The aim of the initiative is a paradigm shift\\nin the discipline of computer science, thus establishing sustainability goals\\nas a primary leitmotif for teaching and research. The focus is on teaching\\nsince the most promising multipliers are the students of a university. The\\nchange in teaching influences our research, the transfer to business and civil\\nsociety as well as the change in our own institution. In this article, we\\npresent the initiative CS4F and reflect primarily on the role of students as\\namplifiers in the transformation process of computer science.', 'The distinction between sciences is becoming increasingly more artificial --\\nan approach from one area can be easily applied to the other. More exciting\\nresearch nowadays is happening perhaps at the interfaces of disciplines like\\nPhysics, Mathematics and Computer Science. How do these interfaces emerge and\\ninteract? For instance, is there a specific pattern in which these fields cite\\neach other? In this article, we investigate a collection of more than 1.2\\nmillion papers from three different scientific disciplines -- Physics,\\nMathematics, and Computer Science. We show how over a timescale the citation\\npatterns from the core science fields (Physics, Mathematics) to the applied and\\nfast-growing field of Computer Science have drastically increased. Further, we\\nobserve how certain subfields in these disciplines are shrinking while others\\nare becoming tremendously popular. For instance, an intriguing observation is\\nthat citations from Mathematics to the subfield of machine learning in Computer\\nScience in recent times are exponentially increasing.', 'The availability of large amounts of data together with advances in\\nanalytical techniques afford an opportunity to address difficult challenges in\\nensuring that healthcare is safe, effective, efficient, patient-centered,\\nequitable, and timely. Surgical care and training stand to tremendously gain\\nthrough surgical data science. Herein, we discuss a few perspectives on the\\nscope and objectives for surgical data science.', 'Environment Agencies from Europe and the US are setting up a network of\\nLinked Environment Data and are looking to crosslink it with Linked Data\\ncontributions from the life sciences.', 'Data science is creating very exciting trends as well as significant\\ncontroversy. A critical matter for the healthy development of data science in\\nits early stages is to deeply understand the nature of data and data science,\\nand to discuss the various pitfalls. These important issues motivate the\\ndiscussions in this article.', 'There has been a remarkable increase in work at the interface of computer\\nscience and game theory in the past decade. In this article I survey some of\\nthe main themes of work in the area, with a focus on the work in computer\\nscience. Given the length constraints, I make no attempt at being\\ncomprehensive, especially since other surveys are also available, and a\\ncomprehensive survey book will appear shortly.', 'Simulations - utilizing computers to solve complicated science and\\nengineering problems - are a key ingredient of modern science. The U.S.\\nDepartment of Energy (DOE) is a world leader in the development of\\nhigh-performance computing (HPC), the development of applied math and\\nalgorithms that utilize the full potential of HPC platforms, and the\\napplication of computing to science and engineering problems. An interesting\\ngeneral question is whether the DOE can strategically utilize its capability in\\nsimulations to advance innovation more broadly. In this article, I will argue\\nthat this is certainly possible.', 'Over 15 years of teaching, advising students and coordinating scientific\\nresearch activities and projects in computer science, we have observed the\\ndifficulties of students to write scientific papers to present the results of\\ntheir research practices. In addition, they repeatedly have doubts about the\\npublishing process. In this article we propose a conceptual framework to\\nsupport the writing and publishing of scientific papers in computer science,\\nproviding a kind of guide for computer science students to effectively present\\nthe results of their research practices, particularly for experimental\\nresearch.', 'Generative artificial intelligence (AI) has revolutionized the field of\\ncomputational social science, unleashing new possibilities for analyzing\\nmultimodal data, especially for scholars who may not have extensive programming\\nexpertise. This breakthrough carries profound implications for the realm of\\nsocial sciences. Firstly, generative AI can significantly enhance the\\nproductivity of social scientists by automating the generation, annotation, and\\ndebugging of code. Secondly, it empowers researchers to delve into\\nsophisticated data analysis through the innovative use of prompt engineering.\\nLastly, the educational sphere of computational social science stands to\\nbenefit immensely from these tools, given their exceptional ability to annotate\\nand elucidate complex codes for learners, thereby simplifying the learning\\nprocess and making the technology more accessible.', 'We argue for the need for a new generation of data science solutions that can\\ndemocratize recent advances in data engineering and artificial intelligence for\\nnon-technical users from various disciplines, enabling them to unlock the full\\npotential of these solutions. To do so, we adopt an approach whereby\\ncomputational creativity and conversational computing are combined to guide\\nnon-specialists intuitively to explore and extract knowledge from data\\ncollections. The paper introduces MATILDA, a creativity-based data science\\ndesign platform, showing how it can support the design process of data science\\npipelines guided by human and computational creativity.', 'A semi-computable set S in a computable metric space need not be computable.\\nHowever, in some cases, if S has certain topological properties, we can\\nconclude that S is computable. It is known that if a semi-computable set S is a\\ncompact manifold with boundary, then the computability of \\\\deltaS implies the\\ncomputability of S. In this paper we examine the case when S is a 1-manifold\\nwith boundary, not necessarily compact. We show that a similar result holds in\\nthis case under assumption that S has finitely many components.', 'Symbolic computation is the science of computing with symbolic objects\\n(terms, formulae, programs, algebraic objects, geometrical objects, etc).\\nPowerful symbolic algorithms have been developed during the past decades and\\nhave played an influential role in theorem proving, automated reasoning,\\nsoftware verification, model checking, rewriting, formalisation of mathematics,\\nnetwork security, Groebner bases, characteristic sets, etc.\\n  The international Symposium on \"Symbolic Computation in Software Science\" is\\nthe fourth in the SCSS workshop series. SCSS 2008 and 2010 took place at the\\nResearch Institute for Symbolic Computation (RISC), Hagenberg, Austria, and,\\nSCSS 2009 took place in Gammarth, Tunisia. These symposium grew out of internal\\nworkshops that bring together researchers from: a) SCORE (Symbolic Computation\\nResearch Group) at the University of Tsukuba, Japan, b) Theorema Group at the\\nResearch Institute for Symbolic Computation, Johannes Kepler University Linz,\\nAustria, c) SSFG (Software Science Foundation Group) at Kyoto University,\\nJapan, and d) Sup\\'Com (Higher School of Communication of Tunis) at the\\nUniversity of Carthage, Tunisia.', 'There is a cognitive limit in Human Mind. This cognitive limit has played a\\ndecisive role in almost all fields including computer sciences. The cognitive\\nlimit replicated in computer sciences is responsible for inherent Computational\\nComplexity. The complexity starts decreasing if certain conditions are met,\\neven sometime it does not appears at all. Very simple Mechanical computing\\nsystems are designed and implemented to demonstrate this idea and it is further\\nsupported by Electrical systems. These verifiable and consistent systems\\ndemonstrate the idea of computational complexity reduction. This work explains\\na very important but invisible connection from Mind to Mathematical axioms\\n(Peano Axioms etc.) and Mathematical axioms to computational complexity. This\\nstudy gives a completely new perspective that goes well beyond Cognitive\\nScience, Mathematics, Physics, Computer Sciences and Philosophy. Based on this\\nnew insight some important predictions are made.', 'Quantum computation and quantum information are of great current interest in\\ncomputer science, mathematics, physical sciences and engineering. They will\\nlikely lead to a new wave of technological innovations in communication,\\ncomputation and cryptography. As the theory of quantum physics is fundamentally\\nstochastic, randomness and uncertainty are deeply rooted in quantum\\ncomputation, quantum simulation and quantum information. Consequently quantum\\nalgorithms are random in nature, and quantum simulation utilizes Monte Carlo\\ntechniques extensively. Thus statistics can play an important role in quantum\\ncomputation and quantum simulation, which in turn offer great potential to\\nrevolutionize computational statistics. While only pseudo-random numbers can be\\ngenerated by classical computers, quantum computers are able to produce genuine\\nrandom numbers; quantum computers can exponentially or quadratically speed up\\nmedian evaluation, Monte Carlo integration and Markov chain simulation. This\\npaper gives a brief review on quantum computation, quantum simulation and\\nquantum information. We introduce the basic concepts of quantum computation and\\nquantum simulation and present quantum algorithms that are known to be much\\nfaster than the available classic algorithms. We provide a statistical\\nframework for the analysis of quantum algorithms and quantum simulation.', 'A consistently specified halting function may be computed.', 'Despite great advances in computation, materials design is still science\\nfiction. The construction of structure-property relations on the quantum scale\\nwill turn computational empiricism into true design.', 'Computer science is a relatively young discipline combining science,\\nengineering, and mathematics. The main flavors of computer science research\\ninvolve the theoretical development of conceptual models for the different\\naspects of computing and the more applicative building of software artifacts\\nand assessment of their properties. In the computer science publication\\nculture, conferences are an important vehicle to quickly move ideas, and\\njournals often publish deeper versions of papers already presented at\\nconferences. These peculiarities of the discipline make computer science an\\noriginal research field within the sciences, and, therefore, the assessment of\\nclassical bibliometric laws is particularly important for this field. In this\\npaper, we study the skewness of the distribution of citations to papers\\npublished in computer science publication venues (journals and conferences). We\\nfind that the skewness in the distribution of mean citedness of different\\nvenues combines with the asymmetry in citedness of articles in each venue,\\nresulting in a highly asymmetric citation distribution with a power law tail.\\nFurthermore, the skewness of conference publications is more pronounced than\\nthe asymmetry of journal papers. Finally, the impact of journal papers, as\\nmeasured with bibliometric indicators, largely dominates that of proceeding\\npapers.', 'In this note, we have shown special case on Routh stability criterion, which\\nis not discussed, in previous literature. This idea can be useful in computer\\nscience applications.', 'This volume contains the proceedings of LINEARITY 2009: the first\\nInternational Workshop on Linearity, which took place 12th September 2009 in\\nCoimbra, Portugal. The workshop was a satellite event of CSL 2009, the 18th\\nEACSL Annual Conference on Computer Science Logic.', 'This file summarizes the plenary talk on laboratory experiments on logic at\\nthe TARK 2013 - 14th Conference on Theoretical Aspects of Rationality and\\nKnowledge.', \"We provide a simple proof of Kamp's theorem.\", \"Process algebra has been successful in many ways; but we don't yet see the\\nlineaments of a fundamental theory. Some fleeting glimpses are sought from\\nPetri Nets, physics and geometry.\", 'We present a survey of the saturation method for model-checking pushdown\\nsystems.', 'This short note discusses the role of syntax vs. semantics and the interplay\\nbetween logic, philosophy, and language in computer science and game theory.', 'A method for computation of the matrix Mittag-Leffler function is presented.\\nThe method is based on Jordan canonical form and implemented as a Matlab\\nroutine.', 'We design games for truly concurrent bisimilarities, including strongly truly\\nconcurrent bisimilarities and branching truly concurrent bisimilarities, such\\nas pomset bisimilarities, step bisimilarities, history-preserving\\nbisimilarities and hereditary history-preserving bisimilarities.', 'This a biographical essay about Edsger Wybe Dijkstra.', \"Italian master's thesis in Computer Science. It is an overview of the\\nstandard tecniques developed in the field of Proof Theory, ending with some\\nresults in the new field of Deep Inference, plus an original contribution\\ntrying to relate Deep Inference and Process Algebras.\", \"We give a short appreciation of Robin Milner's seminal contributions to the\\ntheory of concurrency.\", 'This short note establishes positionality of mean-payoff games over infinite\\ngame graphs by constructing a well-founded monotone universal graph.', 'Science fiction literature, comics, cartoons and, in particular, audio-visual\\nmaterials, such as science fiction movies and shows, can be a valuable addition\\nin Human-computer interaction (HCI) Education. In this paper, we present an\\noverview of research relative to future directions in HCI Education, distinct\\ncrossings of science fiction in HCI and Computer Science teaching and the\\nFramework for 21st Century Learning. Next, we provide examples where science\\nfiction can add to the future of HCI Education. In particular, we argue herein\\nfirst that science fiction, as tangible and intangible cultural artifact, can\\nserve as a trigger for creativity and innovation and thus, support us in\\nexploring the design space. Second, science fiction, as a means to analyze\\nyet-to-come HCI technologies, can assist us in developing an open-minded and\\nreflective dialogue about technological futures, thus creating a singular base\\nfor critical thinking and problem solving. Provided that one is cognizant of\\nits potential and limitations, we reason that science fiction can be a\\nmeaningful extension of selected aspects of HCI curricula and research.', 'Material science literature is a rich source of factual information about\\nvarious categories of entities (like materials and compositions) and various\\nrelations between these entities, such as conductivity, voltage, etc.\\nAutomatically extracting this information to generate a material science\\nknowledge base is a challenging task. In this paper, we propose MatSciRE\\n(Material Science Relation Extractor), a Pointer Network-based encoder-decoder\\nframework, to jointly extract entities and relations from material science\\narticles as a triplet ($entity1, relation, entity2$). Specifically, we target\\nthe battery materials and identify five relations to work on - conductivity,\\ncoulombic efficiency, capacity, voltage, and energy. Our proposed approach\\nachieved a much better F1-score (0.771) than a previous attempt using\\nChemDataExtractor (0.716). The overall graphical framework of MatSciRE is shown\\nin Fig 1. The material information is extracted from material science\\nliterature in the form of entity-relation triplets using MatSciRE.', \"We present polygraphic programs, a subclass of Albert Burroni's polygraphs,\\nas a computational model, showing how these objects can be seen as first-order\\nfunctional programs. We prove that the model is Turing complete. We use\\npolygraphic interpretations, a termination proof method introduced by the\\nsecond author, to characterize polygraphic programs that compute in polynomial\\ntime. We conclude with a characterization of polynomial time functions and\\nnon-deterministic polynomial time functions.\", 'In this note, I develop my personal view on the scope and relevance of\\nsymbolic computation in software science. For this, I discuss the interaction\\nand differences between symbolic computation, software science, automatic\\nprogramming, mathematical knowledge management, artificial intelligence,\\nalgorithmic intelligence, numerical computation, and machine learning. In the\\ndiscussion of these notions, I allow myself to refer also to papers (1982,\\n1985, 2001, 2003, 2013) of mine in which I expressed my views on these areas at\\nearly stages of some of these fields.', 'We highlight the role of Data Science in Biomedicine. Our manuscript goes\\nfrom the general to the particular, presenting a global definition of Data\\nScience and showing the trend for this discipline together with the terms of\\ncloud computing and big data. In addition, since Data Science is mostly related\\nto areas like economy or business, we describe its importance in biomedicine.\\nBiomedical Data Science (BDS) presents the challenge of dealing with data\\ncoming from a range of biological and medical research, focusing on\\nmethodologies to advance the biomedical science discoveries, in an\\ninterdisciplinary context.', 'Data Science is currently a popular field of science attracting expertise\\nfrom very diverse backgrounds. Current learning practices need to acknowledge\\nthis and adapt to it. This paper summarises some experiences relating to such\\nlearning approaches from teaching a postgraduate Data Science module, and draws\\nsome learned lessons that are of relevance to others teaching Data Science.', 'ChatGPT, an implementation and application of large language models, has\\ngained significant popularity since its initial release. Researchers have been\\nexploring ways to harness the practical benefits of ChatGPT in real-world\\nscenarios. Educational researchers have investigated its potential in various\\nsubjects, e.g., programming, mathematics, finance, clinical decision support,\\netc. However, there has been limited attention given to its application in data\\nscience education. This paper aims to bridge that gap by utilizing ChatGPT in a\\ndata science course, gathering perspectives from students, and presenting our\\nexperiences and feedback on using ChatGPT for teaching and learning in data\\nscience education. The findings not only distinguish data science education\\nfrom other disciplines but also uncover new opportunities and challenges\\nassociated with incorporating ChatGPT into the data science curriculum.', 'Data Science is a modern Data Intelligence practice, which is the core of\\nmany businesses and helps businesses build smart strategies around to deal with\\nbusinesses challenges more efficiently. Data Science practice also helps in\\nautomating business processes using the algorithm, and it has several other\\nbenefits, which also deliver in a non-profitable framework. In regards to data\\nscience, three key components primarily influence the effective outcome of a\\ndata science project. Those are 1.Availability of Data 2.Algorithm 3.Processing\\npower or infrastructure', 'Materials design and development typically takes several decades from the\\ninitial discovery to commercialization with the traditional trial and error\\ndevelopment approach. With the accumulation of data from both experimental and\\ncomputational results, data based machine learning becomes an emerging field in\\nmaterials discovery, design and property prediction. This manuscript reviews\\nthe history of materials science as a disciplinary the most common machine\\nlearning method used in materials science, and specifically how they are used\\nin materials discovery, design, synthesis and even failure detection and\\nanalysis after materials are deployed in real application. Finally, the\\nlimitations of machine learning for application in materials science and\\nchallenges in this emerging field is discussed.', \"The discipline of data science emerged to combine statistical methods with\\ncomputing. At Aalto University, Finland, we have taken first steps to bring\\neducational data science as a part of daily operations of Management\\nInformation Services. This required changes in IT environment: we enhanced data\\nwarehouse infrastructure with a data science lab, where we can read predictive\\nmodel training data from data warehouse database and use the created predictive\\nmodels in database queries. We then conducted a data science pilot with an\\nobjective to predict students' graduation probability and time-to-degree with\\nstudent registry data. Further ethical and legal considerations are needed\\nbefore using predictions in daily operations of the university.\", 'This volume contains the proceedings of the Tenth International Workshop on\\nFixed Points in Computer Science (FICS 2015) which took place on September 11th\\nand 12th, 2015 in Berlin, Germany, as a satellite event of the conference\\nComputer Science Logic (CSL 2015).\\n  Fixed points play a fundamental role in several areas of computer science.\\nThey are used to justify (co)recursive definitions and associated reasoning\\ntechniques. The construction and properties of fixed points have been\\ninvestigated in many different settings such as: design and implementation of\\nprogramming languages, logics, verification, databases. The aim of this\\nworkshop is to provide a forum for researchers to present their results to\\nthose members of the computer science and logic communities who study or apply\\nthe theory of fixed points.\\n  Each of the 11 contributed papers of this volume were evaluated by three or\\nfour reviewers. Some of the papers were re-reviewed after revision.\\n  Additionally, this volume contains the abstracts of the FICS 2015 invited\\ntalks given by Bartek Klin and James Worrell.', \"In this paper I briefly discuss the importance of home automation system.\\nGoing in to the details I briefly present a real time designed and implemented\\nsoftware and hardware oriented house automation research project, capable of\\nautomating house's electricity and providing a security system to detect the\\npresence of unexpected behavior.\", 'We improve the answer to the question: what set of excluded middles for\\npropositional variables in a formula suffices to prove the formula in\\nintuitionistic propositional logic whenever it is provable in classical\\npropositional logic.', 'Theoretical computer science (TCS) is a subdiscipline of computer science\\nthat studies the mathematical foundations of computational and algorithmic\\nprocesses and interactions. Work in this field is often recognized by its\\nemphasis on mathematical technique and rigor. At the heart of the field are\\nquestions surrounding the nature of computation: What does it mean to compute?\\nWhat is computable? And how efficiently?\\n  Every ten years or so the TCS community attends visioning workshops to\\ndiscuss the challenges and recent accomplishments in the TCS field. The\\nworkshops and the outputs they produce are meant both as a reflection for the\\nTCS community and as guiding principles for interested investment partners.\\nConcretely, the workshop output consists of a number of nuggets, each\\nsummarizing a particular point, that are synthesized in the form of a white\\npaper and illustrated with graphics/slides produced by a professional graphic\\ndesigner. The second TCS Visioning Workshop was organized by the SIGACT\\nCommittee for the Advancement of Theoretical Computer Science and took place\\nduring the week of July 20, 2020. Despite the conference being virtual, there\\nwere over 76 participants, mostly from the United States, but also a few from\\nEurope and Asia who were able to attend due to the online format. Workshop\\nparticipants were divided into categories as reflected in the sections of this\\nreport: (1) models of computation; (2) foundations of data science; (3)\\ncryptography; and (4) using theoretical computer science for other domains.\\nEach group participated in a series of discussions that produced the nuggets\\nbelow.', 'Computational philosophy is the use of mechanized computational techniques to\\nunearth philosophical insights that are either difficult or impossible to find\\nusing traditional philosophical methods. Computational metaphysics is\\ncomputational philosophy with a focus on metaphysics. In this paper, we (a)\\ndevelop results in modal metaphysics whose discovery was computer assisted, and\\n(b) conclude that these results work not only to the obvious benefit of\\nphilosophy but also, less obviously, to the benefit of computer science, since\\nthe new computational techniques that led to these results may be more broadly\\napplicable within computer science. The paper includes a description of our\\nbackground methodology and how it evolved, and a discussion of our new results.', 'One of the fundamental questions in science is how scientific disciplines\\nevolve and sustain progress in society. No studies to date allows us to explain\\nthe endogenous processes that support the evolution of scientific disciplines\\nand emergence of new scientific fields in applied sciences of physics. This\\nstudy confronts this problem here by investigating the evolution of\\nexperimental physics to explain and generalize some characteristics of the\\ndynamics of applied sciences. Empirical analysis suggests properties about the\\nevolution of experimental physics and in general of applied sciences, such as:\\na) scientific fission, the evolution of scientific disciplines generates a\\nprocess of division into two or more research fields that evolve as autonomous\\nentities over time; b) ambidextrous drivers of science, the evolution of\\nscience via scientific fission is due to scientific discoveries or new\\ntechnologies; c) new driving research fields, the drivers of scientific\\ndisciplines are new research fields rather than old ones; d) science driven by\\ndevelopment of general purpose technologies, the evolution of experimental\\nphysics and applied sciences is due to the convergence of experimental and\\ntheoretical branches of physics associated with the development of computer,\\ninformation systems and applied computational science. Results also reveal that\\naverage duration of the upwave of scientific production in scientific fields\\nsupporting experimental physics is about 80 years. Overall, then, this study\\nbegins the process of clarifying and generalizing, as far as possible, some\\ncharacteristics of the evolutionary dynamics of scientific disciplines that can\\nlay a foundation for the development of comprehensive properties explaining the\\nevolution of science as a whole for supporting fruitful research policy\\nimplications directed to advancement of science and technological progress in\\nsociety.', \"A substantial fraction of students who complete their college education at a\\npublic university in the United States begin their journey at one of the 935\\npublic two-year colleges. While the number of four-year colleges offering\\nbachelor's degrees in data science continues to increase, data science\\ninstruction at many two-year colleges lags behind. A major impediment is the\\nrelative paucity of introductory data science courses that serve multiple\\nstudent audiences and can easily transfer. In addition, the lack of pre-defined\\ntransfer pathways (or articulation agreements) for data science creates a\\ngrowing disconnect that leaves students who want to study data science at a\\ndisadvantage. We describe opportunities and barriers to data science transfer\\npathways. Five points of curricular friction merit attention: 1) a first course\\nin data science, 2) a second course in data science, 3) a course in scientific\\ncomputing, data science workflow, and/or reproducible computing, 4) lab\\nsciences, and 5) navigating communication, ethics, and application domain\\nrequirements in the context of general education and liberal arts course\\nmappings. We catalog existing transfer pathways, efforts to align curricula\\nacross institutions, obstacles to overcome with minimally-disruptive solutions,\\nand approaches to foster these pathways. Improvements in these areas are\\ncritically important to ensure that a broad and diverse set of students are\\nable to engage and succeed in undergraduate data science programs.\", \"We believe that economic design and computational complexity---while already\\nimportant to each other---should become even more important to each other with\\neach passing year. But for that to happen, experts in on the one hand such\\nareas as social choice, economics, and political science and on the other hand\\ncomputational complexity will have to better understand each other's\\nworldviews.\\n  This article, written by two complexity theorists who also work in\\ncomputational social choice theory, focuses on one direction of that process by\\npresenting a brief overview of how most computational complexity theorists view\\nthe world. Although our immediate motivation is to make the lens through which\\ncomplexity theorists see the world be better understood by those in the social\\nsciences, we also feel that even within computer science it is very important\\nfor nontheoreticians to understand how theoreticians think, just as it is\\nequally important within computer science for theoreticians to understand how\\nnontheoreticians think.\", 'Between 1984 and 2011, the percentage of US bachelor degrees awarded in\\nphysics declined by 25%, in chemistry declined by 33%, and overall in physical\\nsciences and engineering fell 40%. Data suggest that these declines are\\ncorrelated to a deemphasis in most states of practicing computation skills in\\nmathematics. Analysis of state standards put into place between 1990 and 2010\\nfind that most states directed teachers to deemphasize both memorization and\\nstudent practice in computational problem solving. Available state test score\\ndata show a significant decline in student computation skills. In recent\\ninternational testing, scores for US 16 to 24 year olds in numeracy finished\\nlast among 22 tested nations in the OECD. Recent studies in cognitive science\\nhave found that to solve well-structured problems in the sciences, students\\nmust first memorize fundamental facts and procedures in mathematics and science\\nuntil they can be recalled with automaticity, then practice applying those\\nskills in a variety of distinctive contexts. Actions are suggested to improve\\nUS STEM graduation rates by aligning US math and science curricula with the\\nrecommendations of cognitive science.', 'Is the universe computable? If so, it may be much cheaper in terms of\\ninformation requirements to compute all computable universes instead of just\\nours. I apply basic concepts of Kolmogorov complexity theory to the set of\\npossible universes, and chat about perceived and true randomness, life,\\ngeneralization, and learning in a given universe.', 'In recent years, the existence of a significant cross-impact between Cloud\\ncomputing and Internet of Things (IoT) has lead to a dichotomy that gives raise\\nto Cloud-Assisted IoT (CAIoT) and IoT-Based Cloud (IoTBC). Although it is\\npertinent to study both technologies, this paper focuses on CAIoT, and\\nespecially its security issues, which are inherited from both Cloud computing\\nand IoT. This study starts with reviewing existing relevant surveys, noting\\ntheir shortcomings, which motivate a comprehensive survey in this area. We\\nproceed to highlight existing approaches towards the design of Secure CAIoT\\n(SCAIoT) along with related security challenges and controls. We develop a\\nlayered architecture for SCAIoT. Furthermore, we take a look at what the future\\nmay hold for SCAIoT with a focus on the role of Artificial Intelligence(AI).', '\"Science gateway\" (SG) ideology means a user-friendly intuitive interface\\nbetween scientists (or scientific communities) and different software\\ncomponents + various distributed computing infrastructures (DCIs) (like grids,\\nclouds, clusters), where researchers can focus on their scientific goals and\\nless on peculiarities of software/DCI. \"IMP Science Gateway Portal\"\\n(http://scigate.imp.kiev.ua) for complex workflow management and integration of\\ndistributed computing resources (like clusters, service grids, desktop grids,\\nclouds) is presented. It is created on the basis of WS-PGRADE and gUSE\\ntechnologies, where WS-PGRADE is designed for science workflow operation and\\ngUSE - for smooth integration of available resources for parallel and\\ndistributed computing in various heterogeneous distributed computing\\ninfrastructures (DCI). The typical scientific workflows with possible scenarios\\nof its preparation and usage are presented. Several typical use cases for these\\nscience applications (scientific workflows) are considered for molecular\\ndynamics (MD) simulations of complex behavior of various nanostructures\\n(nanoindentation of graphene layers, defect system relaxation in metal\\nnanocrystals, thermal stability of boron nitride nanotubes, etc.). The user\\nexperience is analyzed in the context of its practical applications for MD\\nsimulations in materials science, physics and nanotechnologies with available\\nheterogeneous DCIs. In conclusion, the \"science gateway\" approach - workflow\\nmanager (like WS-PGRADE) + DCI resources manager (like gUSE)- gives opportunity\\nto use the SG portal (like \"IMP Science Gateway Portal\") in a very promising\\nway, namely, as a hub of various virtual experimental labs (different software\\ncomponents + various requirements to resources) in the context of its practical\\nMD applications in materials science, physics, chemistry, biology, and\\nnanotechnologies.', \"We provide a computer verified exact monadic functional implementation of the\\nRiemann integral in type theory. Together with previous work by O'Connor, this\\nmay be seen as the beginning of the realization of Bishop's vision to use\\nconstructive mathematics as a programming language for exact analysis.\", 'This volume contains the papers presented at the 6th Membrane Computing and\\nBiologically Inspired Process Calculi (MeCBIC 2012), a satellite workshop of\\nthe 23rd International Conference on Concurrency Theory (CONCUR) held on 8th\\nSeptember 2012 in Newcastle upon Tyne, UK.', 'Scientists in all domains face a data avalanche - both from better\\ninstruments and from improved simulations. We believe that computer science\\ntools and computer scientists are in a position to help all the sciences by\\nbuilding tools and developing techniques to manage, analyze, and visualize\\npeta-scale scientific information. This article is summarizes our experiences\\nover the last seven years trying to bridge the gap between database technology\\nand the needs of the astronomy community in building the World-Wide Telescope.', 'This contribution argues that the notion of time used in the scientific\\nmodeling of reality deprives time of its real nature. Difficulties from logic\\nparadoxes to mathematical incompleteness and numerical uncertainty ensue. How\\ncan the emergence of novelty in the Universe be explained? How can the\\ncreativity of the evolutionary process leading to ever more complex forms of\\nlife be captured in our models of reality? These questions are deeply related\\nto our understanding of time. We argue here for a computational framework of\\nmodeling that seems to us the only currently known type of modeling available\\nin Science able to capture aspects of the nature of time required to better\\nmodel and understand real phenomena.', 'In the ML fairness literature, there have been few investigations through the\\nviewpoint of philosophy, a lens that encourages the critical evaluation of\\nbasic assumptions. The purpose of this paper is to use three ideas from the\\nphilosophy of science and computer science to tease out blind spots in the\\nassumptions that underlie ML fairness: abstraction, induction, and measurement.\\nThrough this investigation, we hope to warn of these methodological blind spots\\nand encourage further interdisciplinary investigation in fair-ML through the\\nframework of philosophy.', 'Philosophy of science attempts to describe all parts of the scientific\\nprocess in a general way in order to facilitate the description, execution and\\nimprovements of this process.\\n  So far, all proposed philosophies have only covered existing processes and\\ndisciplines partially and imperfectly. In particular logical approaches have\\nalways received a lot of attention due to attempts to fundamentally address\\nissues with the definition of science as a discipline with reductionist\\ntheories.\\n  We propose a new way to approach the problem from the perspective of\\ncomputational complexity and argue why this approach may be better than\\nprevious propositions based on pure logic and mathematics.', 'We describe the development of a scientific cloud computing (SCC) platform\\nthat offers high performance computation capability. The platform consists of a\\nscientific virtual machine prototype containing a UNIX operating system and\\nseveral materials science codes, together with essential interface tools (an\\nSCC toolset) that offers functionality comparable to local compute clusters. In\\nparticular, our SCC toolset provides automatic creation of virtual clusters for\\nparallel computing, including tools for execution and monitoring performance,\\nas well as efficient I/O utilities that enable seamless connections to and from\\nthe cloud. Our SCC platform is optimized for the Amazon Elastic Compute Cloud\\n(EC2). We present benchmarks for prototypical scientific applications and\\ndemonstrate performance comparable to local compute clusters. To facilitate\\ncode execution and provide user-friendly access, we have also integrated cloud\\ncomputing capability in a JAVA-based GUI. Our SCC platform may be an\\nalternative to traditional HPC resources for materials science or quantum\\nchemistry applications.', 'Many problems in engineering, chemistry and physics require the\\nrepresentation of solutions in complex geometries. In the paper we deal with a\\nproblem of unstructured mesh generation for the control volume method. We\\npropose an algorithm which bases on the spheres generation in central points of\\nthe control volumes.', 'Biology-derived algorithms are an important part of computational sciences,\\nwhich are essential to many scientific disciplines and engineering\\napplications. Many computational methods are derived from or based on the\\nanalogy to natural evolution and biological activities, and these biologically\\ninspired computations include genetic algorithms, neural networks, cellular\\nautomata, and other algorithms.', \"How can we justify the validity of our computer security methods? This\\nmeta-methodological question is related to recent explorations on the science\\nof computer security, which have been hindered by computer security's unique\\nproperties. We confront this by developing a taxonomy of properties and\\nmethods. Interdisciplinary foundations provide a solid grounding for a set of\\nessential concepts, including a decision tree for characterizing adversarial\\ninteraction. Several types of invalidation and general ways of addressing them\\nare described for technical methods. An interdisciplinary argument from theory\\nexplains the role that meta-methodological validation plays in the adversarial\\nscience of computer security.\", \"The notion of programming paradigms, with associated programming languages\\nand methodologies, is a well established tenet of Computer Science pedagogy,\\nenshrined in international curricula. However, this notion sits ill with Kuhn's\\nclassic conceptualisation of a scientific paradigm as a dominant world view,\\nwhich supersedes its predecessors through superior explanatory power.\\nFurthermore, it is not at all clear how programming paradigms are to be\\ncharacterised and differentiated. Indeed, on closer inspection, apparently\\ndisparate programming paradigms are very strongly connected. Rather, they\\nshould be viewed as different traditions of a unitary Computer Science paradigm\\nof Turing complete computation complemented by Computational Thinking.\", 'We live in exceptional times in which the entire world is witnessing the\\nexponential spread of a pandemic, which requires to adopt new habits of mind\\nand behaviors. In this paper, I introduce the term exponential competence,\\nwhich encompasses these cognitive and social skills, and describe a course for\\ncomputer science and software engineering students in which emphasis is placed\\non exponential competence. I argue that exponential competence is especially\\nimportant for computer science and software engineering students, since many of\\nthem will, most likely, be required to deal with exponential phenomena in their\\nfuture professional development.', \"This paper delves into the evolving relationship between humans and computers\\nin the realm of programming. Historically, programming has been a dialogue\\nwhere humans meticulously crafted communication to suit machine understanding,\\nshaping the trajectory of computer science education. However, the advent of\\nAI-based no-code platforms is revolutionizing this dynamic. Now, humans can\\nconverse in their natural language, expecting machines to interpret and act.\\nThis shift has profound implications for computer science education. As\\neducators, it's imperative to integrate this new dynamic into curricula. In\\nthis paper, we've explored several pertinent research questions in this\\ntransformation, which demand continued inquiry and adaptation in our\\neducational strategies.\", 'In a process algebra with hiding and recursion it is possible to create\\nprocesses which compute internally without ever communicating with their\\nenvironment. Such processes are said to diverge or livelock. In this paper we\\nshow how it is possible to conservatively classify processes as livelock-free\\nthrough a static analysis of their syntax. In particular, we present a\\ncollection of rules, based on the inductive structure of terms, which guarantee\\nlivelock-freedom of the denoted process. This gives rise to an algorithm which\\nconservatively flags processes that can potentially livelock. We illustrate our\\napproach by applying both BDD-based and SAT-based implementations of our\\nalgorithm to a range of benchmarks, and show that our technique in general\\nsubstantially outperforms the model checker FDR whilst exhibiting a low rate of\\ninconclusive results.', 'Computing technologies have become pervasive in daily life, sometimes\\nbringing unintended but harmful consequences. For students to learn to think\\nnot only about what technology they could create, but also about what\\ntechnology they should create, computer science curricula must expand to\\ninclude ethical reasoning about the societal value and impact of these\\ntechnologies. This paper presents Embedded EthiCS, a novel approach to\\nintegrating ethics into computer science education that incorporates ethical\\nreasoning throughout courses in the standard computer science curriculum. It\\nthus changes existing courses rather than requiring wholly new courses. The\\npaper describes a pilot Embedded EthiCS program that embeds philosophers\\nteaching ethical reasoning directly into computer science courses. It discusses\\nlessons learned and challenges to implementing such a program across different\\ntypes of academic institutions.', 'Access to the work of others is something that is too often taken for\\ngranted, yet problematic and difficult to be obtained unless someone pays for\\nit. Green and gold open access are claimed to be a solution to this problem.\\nWhile open access is gaining momentum in some fields, there is a limited and\\nseasoned knowledge about self-archiving in computer science. In particular,\\nthere is an inadequate understanding of author-based self-archiving awareness,\\npractice, and inhibitors. This article reports an exploratory study of the\\nawareness of self-archiving, the practice of self-archiving, and the inhibitors\\nof self-archiving among authors in an Italian computer science faculty.\\nForty-nine individuals among interns, PhD students, researchers, and professors\\nwere recruited in a questionnaire (response rate of 72.8%). The quantitative\\nand qualitative responses suggested that there is still work needed in terms of\\nadvocating green open access to computer science authors who seldom\\nself-archive and when they do, they often infringe the copyright transfer\\nagreements (CTAs) of the publishers. In addition, tools from the open-source\\ncommunity are needed to facilitate author-based self-archiving, which should\\ncomprise of an automatic check of the CTAs. The study identified nine factors\\ninhibiting the act of self-archiving among computer scientists. As a first\\nstep, this study proposes several propositions regarding author-based\\nself-archiving in computer science that can be further investigated.\\nRecommendations to foster self-archiving in computer science, based on the\\nresults, are provided.', 'Data-driven science is heralded as a new paradigm in materials science. In\\nthis field, data is the new resource, and knowledge is extracted from materials\\ndata sets that are too big or complex for traditional human reasoning -\\ntypically with the intent to discover new or improved materials or materials\\nphenomena. Multiple factors, including the open science movement, national\\nfunding, and progress in information technology, have fueled its development.\\nSuch related tools as materials databases, machine learning, and\\nhigh-throughput methods are now established as parts of the materials research\\ntoolset. However, there are a variety of challenges that impede progress in\\ndata-driven materials science: data veracity, integration of experimental and\\ncomputational data, data longevity, standardization, and the gap between\\nindustrial interests and academic efforts. In this perspective article, we\\ndiscuss the historical development and current state of data-driven materials\\nscience, building from the early evolution of open science to the rapid\\nexpansion of materials data infrastructures. We also review key successes and\\nchallenges so far, providing a perspective on the future development of the\\nfield.', \"This paper was presented as the 8th annual Transactions in GIS plenary\\naddress at the American Association of Geographers annual meeting in\\nWashington, DC. The spatial sciences have recently seen growing calls for more\\naccessible software and tools that better embody geographic science and theory.\\nUrban spatial network science offers one clear opportunity: from multiple\\nperspectives, tools to model and analyze nonplanar urban spatial networks have\\ntraditionally been inaccessible, atheoretical, or otherwise limiting. This\\npaper reflects on this state of the field. Then it discusses the motivation,\\nexperience, and outcomes of developing OSMnx, a tool intended to help address\\nthis. Next it reviews this tool's use in the recent multidisciplinary spatial\\nnetwork science literature to highlight upstream and downstream benefits of\\nopen-source software development. Tool-building is an essential but poorly\\nincentivized component of academic geography and social science more broadly.\\nTo conduct better science, we need to build better tools. The paper concludes\\nwith paths forward, emphasizing open-source software and reusable computational\\ndata science beyond mere reproducibility and replicability.\", 'This thesis presents a series of theoretical results and practical\\nrealisations about the theory of computation in distributive categories.\\nDistributive categories have been proposed as a foundational tool for Computer\\nScience in the last years, starting from the papers of R.F.C. Walters. We shall\\nfocus on two major topics: distributive computability, i.e., a generalized\\ntheory of computability based on distributive categories, and the Imp(G)\\nlanguage, which is a language based on the syntax of distributive categories.\\nThe link between the former and the latter is that the functions computed by\\nImp(G) programs are exactly the distributively computable functions.', 'Vagueness is a linguistic phenomenon as well as a property of physical\\nobjects. Fuzzy set theory is a mathematical model of vagueness that has been\\nused to define vague models of computation. The prominent model of vague\\ncomputation is the fuzzy Turing machine. This conceptual computing device gives\\nan idea of what computing under vagueness means, nevertheless, it is not the\\nmost natural model. Based on the properties of this and other models of vague\\ncomputing, it is aimed to formulate a basis for a philosophy of a theory of\\nfuzzy computation.', 'Unique sense: Smart computing prototype is a part of unique sense computing\\narchitecture, which delivers alternate solution for todays computing\\narchitecture. This computing is one step towards future generation needs, which\\nbrings extended support to the ubiquitous environment. This smart computing\\nprototype is the light weight compact architecture which is designed to satisfy\\nall the needs of this society. The proposed solution is based on the hybrid\\ncombination of cutting edge technologies and techniques from the various\\nlayers. In addition it achieves low cost architecture and eco-friendly to meet\\nall the levels of peoples needs.', 'Analysing historical patterns of artificial intelligence (AI) adoption can\\ninform decisions about AI capability uplift, but research to date has provided\\na limited view of AI adoption across various fields of research. In this study\\nwe examine worldwide adoption of AI technology within 333 fields of research\\nduring 1960-2021. We do this by using bibliometric analysis with 137 million\\npeer-reviewed publications captured in The Lens database. We define AI using a\\nlist of 214 phrases developed by expert working groups at the Organisation for\\nEconomic Cooperation and Development (OECD). We found that 3.1 million of the\\n137 million peer-reviewed research publications during the entire period were\\nAI-related, with a surge in AI adoption across practically all research fields\\n(physical science, natural science, life science, social science and the arts\\nand humanities) in recent years. The diffusion of AI beyond computer science\\nwas early, rapid and widespread. In 1960 14% of 333 research fields were\\nrelated to AI (many in computer science), but this increased to cover over half\\nof all research fields by 1972, over 80% by 1986 and over 98% in current times.\\nWe note AI has experienced boom-bust cycles historically: the AI \"springs\" and\\n\"winters\". We conclude that the context of the current surge appears different,\\nand that interdisciplinary AI application is likely to be sustained.', 'Recent advancements in artificial intelligence, particularly with the\\nemergence of large language models (LLMs), have sparked a rethinking of\\nartificial general intelligence possibilities. The increasing human-like\\ncapabilities of AI are also attracting attention in social science research,\\nleading to various studies exploring the combination of these two fields. In\\nthis survey, we systematically categorize previous explorations in the\\ncombination of AI and social science into two directions that share common\\ntechnical approaches but differ in their research objectives. The first\\ndirection is focused on AI for social science, where AI is utilized as a\\npowerful tool to enhance various stages of social science research. While the\\nsecond direction is the social science of AI, which examines AI agents as\\nsocial entities with their human-like cognitive and linguistic capabilities. By\\nconducting a thorough review, particularly on the substantial progress\\nfacilitated by recent advancements in large language models, this paper\\nintroduces a fresh perspective to reassess the relationship between AI and\\nsocial science, provides a cohesive framework that allows researchers to\\nunderstand the distinctions and connections between AI for social science and\\nsocial science of AI, and also summarized state-of-art experiment simulation\\nplatforms to facilitate research in these two directions. We believe that as AI\\ntechnology continues to advance and intelligent agents find increasing\\napplications in our daily lives, the significance of the combination of AI and\\nsocial science will become even more prominent.', 'In this paper we seek to contribute to the debate about the nature of citizen\\ninvolvement in real scientific projects by the means of online tools that\\nfacilitate crowdsourcing and collaboration. We focus on an understudied area,\\nthe impact of online citizen science participation on the science education of\\nschool age children. We present a binary tree of online citizen science process\\nflows and the results of an anonymous survey among primary school teachers in\\nNew Zealand that are known advocates of science education. Our findings reveal\\nwhy teachers are interested in using online citizen science in classroom\\nactivities and what they are looking for when making their choice for a\\nparticular project to use. From these characteristics we derive recommendations\\nfor the optimal embedding of online citizen science in education related to the\\nprocess, the context, and the dissemination of results.', \"This paper examines the international nature of science fiction. The focus of\\nthis research is to determine whether science fiction is primarily English\\nspeaking and Western or global; being created and consumed by people in\\nnon-Western, non-English speaking countries? Science fiction's international\\npresence was found in three ways, by network analysis, by examining a online\\nretailer and with a survey. Condor, a program developed by GalaxyAdvisors was\\nused to determine if science fiction is being talked about by non-English\\nspeakers. An analysis of the international Amazon.com websites was done to\\ndiscover if it was being consumed worldwide. A survey was also conducted to see\\nif people had experience with science fiction. All three research methods\\nrevealed similar results. Science fiction was found to be international, with\\nscience fiction creators originating in different countries and writing in a\\nhost of different languages. English and non-English science fiction was being\\ncreated and consumed all over the world, not just in the English speaking West.\", 'The trustworthiness of data science systems in applied and real-world\\nsettings emerges from the resolution of specific tensions through situated,\\npragmatic, and ongoing forms of work. Drawing on research in CSCW, critical\\ndata studies, and history and sociology of science, and six months of immersive\\nethnographic fieldwork with a corporate data science team, we describe four\\ncommon tensions in applied data science work: (un)equivocal numbers,\\n(counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We\\nshow how organizational actors establish and re-negotiate trust under messy and\\nuncertain analytic conditions through practices of skepticism, assessment, and\\ncredibility. Highlighting the collaborative and heterogeneous nature of\\nreal-world data science, we show how the management of trust in applied\\ncorporate data science settings depends not only on pre-processing and\\nquantification, but also on negotiation and translation. We conclude by\\ndiscussing the implications of our findings for data science research and\\npractice, both within and beyond CSCW.', 'Science of science has become a popular topic that attracts great attentions\\nfrom the research community. The development of data analytics technologies and\\nthe readily available scholarly data enable the exploration of data-driven\\nprediction, which plays a pivotal role in finding the trend of scientific\\nimpact. In this paper, we analyse methods and applications in data-driven\\nprediction in the science of science, and discuss their significance. First, we\\nintroduce the background and review the current state of the science of\\nscience. Second, we review data-driven prediction based on paper citation\\ncount, and investigate research issues in this area. Then, we discuss methods\\nto predict scholar impact, and we analyse different approaches to promote the\\nscholarly collaboration in the collaboration network. This paper also discusses\\nopen issues and existing challenges, and suggests potential research\\ndirections.', \"The traditional university science curriculum was designed to train\\nspecialists in specific disciplines. However, in universities all over the\\nworld, science students are going into increasingly diverse careers and the\\ncurrent model does not fit their needs. Advances in technology also make\\ncertain modes of learning obsolete. In the last 10 years, the Faculty of\\nScience of the University of Hong Kong has undertaken major curriculum reforms.\\nA sequence of science foundation courses required of all incoming science\\nstudents are designed to teach science in an integrated manner, and to\\nemphasize the concepts and utilities, not computational techniques, of\\nmathematics. A number of non-discipline specific common core courses have been\\ndeveloped to broaden students' awareness of the relevance of science to society\\nand the interdisciplinary nature of science. By putting the emphasis on the\\nscientific process rather than the outcome, students are taught how to\\nidentify, formulate, and solve diverse problems.\", 'In the data science courses at the University of British Columbia, we define\\ndata science as the study, development and practice of reproducible and\\nauditable processes to obtain insight from data. While reproducibility is core\\nto our definition, most data science learners enter the field with other\\naspects of data science in mind, for example predictive modelling, which is\\noften one of the most interesting topic to novices. This fact, along with the\\nhighly technical nature of the industry standard reproducibility tools\\ncurrently employed in data science, present out-of-the gate challenges in\\nteaching reproducibility in the data science classroom. Put simply, students\\nare not as intrinsically motivated to learn this topic, and it is not an easy\\none for them to learn. What can a data science educator do? Over several\\niterations of teaching courses focused on reproducible data science tools and\\nworkflows, we have found that providing extra motivation, guided instruction\\nand lots of practice are key to effectively teaching this challenging, yet\\nimportant subject. Here we present examples of how we deeply motivate,\\neffectively guide and provide ample practice opportunities to data science\\nstudents to effectively engage them in learning about this topic.', 'Data Science is a complex and evolving field, but most agree that it can be\\ndefined as a combination of expertise drawn from three broad areascomputer\\nscience and technology, math and statistics, and domain knowledge -- with the\\npurpose of extracting knowledge and value from data. Beyond this, the field is\\noften defined as a series of practical activities ranging from the cleaning and\\nwrangling of data, to its analysis and use to infer models, to the visual and\\nrhetorical representation of results to stakeholders and decision-makers. This\\nessay proposes a model of data science that goes beyond laundry-list\\ndefinitions to get at the specific nature of data science and help distinguish\\nit from adjacent fields such as computer science and statistics. We define data\\nscience as an interdisciplinary field comprising four broad areas of expertise:\\nvalue, design, systems, and analytics. A fifth area, practice, integrates the\\nother four in specific contexts of domain knowledge. We call this the 4+1 model\\nof data science. Together, these areas belong to every data science project,\\neven if they are often unconnected and siloed in the academy.', 'Large Language Models (LLMs) create exciting possibilities for powerful\\nlanguage processing tools to accelerate research in materials science. While\\nLLMs have great potential to accelerate materials understanding and discovery,\\nthey currently fall short in being practical materials science tools. In this\\nposition paper, we show relevant failure cases of LLMs in materials science\\nthat reveal current limitations of LLMs related to comprehending and reasoning\\nover complex, interconnected materials science knowledge. Given those\\nshortcomings, we outline a framework for developing Materials Science LLMs\\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\\ngeneration followed by hypothesis testing. The path to attaining performant\\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\\nsourced from scientific literature where various information extraction\\nchallenges persist. As such, we describe key materials science information\\nextraction challenges which need to be overcome in order to build large-scale,\\nmulti-modal datasets that capture valuable materials science knowledge.\\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\\nLaboratories.', 'Under the name of Citizen Science, many innovative practices in which\\nvolunteers partner with scientist to pose and answer real-world questions are\\nquickly growing worldwide. Citizen Science can furnish ready made solutions\\nwith the active role of citizens. However, this framework is still far from\\nbeing well stablished to become a standard tool for Computational Social\\nSciences research. We present our experience in bridging Computational Social\\nSciences with Citizen Science philosophy, which in our case has taken the form\\nof what we call Pop-Up Experiments: Non-permanent, highly participatory\\ncollective experiments which blend features developed by Big Data methodologies\\nand Behavioural Experiments protocols with ideals of Citizen Science. The main\\nissues to take into account whenever planning experiments of this type are\\nclassified and discused grouped in three categories: public engagement, light\\ninfrastructure and knowledge return to citizens. We explain the solutions\\nimplemented providing practical examples grounded in our own experience in\\nurban contexts (Barcelona, Spain). We hope that this work serves as guideline\\nto groups willing to adopt and expand such \\\\emph{in-vivo} practices and opens\\nthe debate about the possibilities (but also the limitations) that the Citizen\\nScience framework can offer to study social phenomena.', 'The recently initiated approach called computability logic is a formal theory\\nof interactive computation. See a comprehensive online source on the subject at\\nhttp://www.cis.upenn.edu/~giorgi/cl.html . The present paper contains a\\nsoundness and completeness proof for the deductive system CL3 which axiomatizes\\nthe most basic first-order fragment of computability logic called the\\nfinite-depth, elementary-base fragment. Among the potential application areas\\nfor this result are the theory of interactive computation, constructive applied\\ntheories, knowledgebase systems, systems for resource-bound planning and\\naction. This paper is self-contained as it reintroduces all relevant\\ndefinitions as well as main motivations.', 'We describe how to use propositional model counting for a quantitative\\nanalysis of product configuration data. Our approach computes valuable meta\\ninformation such as the total number of valid configurations or the relative\\nfrequency of components. This information can be used to assess the severity of\\ndocumentation errors or to measure documentation quality. As an application\\nexample we show how we apply these methods to product documentation formulas of\\nthe Mercedes-Benz line of vehicles. In order to process these large formulas we\\ndeveloped and implemented a new model counter for non-CNF formulas. Our model\\ncounter can process formulas, whose CNF representations could not be processed\\nup till now.', 'Understanding Quantum Technologies 2023 is a creative-commons ebook that\\nprovides a unique 360 degrees overview of quantum technologies from science and\\ntechnology to geopolitical and societal issues. It covers quantum physics\\nhistory, quantum physics 101, gate-based quantum computing, quantum computing\\nengineering (including quantum error corrections and quantum computing\\nenergetics), quantum computing hardware (all qubit types, including quantum\\nannealing and quantum simulation paradigms, history, science, research,\\nimplementation and vendors), quantum enabling technologies (cryogenics, control\\nelectronics, photonics, components fabs, raw materials), unconventional\\ncomputing (potential alternatives to quantum and classical computing), quantum\\ntelecommunications and cryptography, quantum sensing, quantum computing\\nalgorithms, software development tools and use cases, quantum technologies\\naround the world, quantum technologies societal impact and even quantum fake\\nsciences. The main audience are computer science engineers, developers and IT\\nspecialists as well as quantum scientists and students who want to acquire a\\nglobal view of how quantum technologies work, and particularly quantum\\ncomputing. This version is an update to the 2022 and 2021 editions published\\nrespectively in October 2022 and October 2021. An update log is provided at the\\nend of the book.', 'With the help of link diagrams with decorated crossings, I explain\\ncomputations in emergent algebras, introduced in arXiv:0907.1520, as the kind\\nof computations done in the front end visual system.', 'We argue that computation is an abstract algebraic concept, and a computer is\\na result of a morphism (a structure preserving map) from a finite universal\\nsemigroup.', 'We construct a computable, computably categorical field of infinite\\ntranscendence degree over the rational numbers, using the Fermat polynomials\\nand assorted results from algebraic geometry. We also show that this field has\\nan intrinsically computable (infinite) transcendence basis.', 'This white paper argues that formal methods need to be better rooted in\\nhigher education curricula for computer science and software engineering\\nprogrammes of study. To this end, it advocates (i) improved teaching of formal\\nmethods; (ii) systematic highlighting of formal methods within existing,\\n`classical\\' computer science courses; and (iii) the inclusion of a compulsory\\nformal methods course in computer science and software engineering curricula.\\n  These recommendations are based on the observations that (a) formal methods\\nare an essential and cost-effective means to increase software quality; however\\n(b) computer science and software engineering programmes typically fail to\\nprovide adequate training in formal methods; and thus (c) there is a lack of\\ncomputer science graduates who are qualified to apply formal methods in\\nindustry.\\n  This white paper is the result of a collective effort by authors and\\nparticipants of the 1st International Workshop on \"Formal Methods, Fun for\\nEverybody\" which was held in Bergen, Norway, 2-3 December 2019. As such, it\\nrepresents insights based on learning and teaching computer science and\\nsoftware engineering (with or without formal methods) at various universities\\nacross Europe.', 'Large-scale science instruments, such as the distributed radio telescope\\nLOFAR, show that we are in an era of data-intensive scientific discovery. Such\\ninstruments rely critically on significant computing resources, both hardware\\nand software, to do science. Considering limited science budgets, and the small\\nfraction of these that can be dedicated to compute hardware and software, there\\nis a strong and obvious desire for low-cost computing. However, optimising for\\ncost is only part of the equation; the value potential over the lifetime of the\\nsolution should also be taken into account. Using a tangible example, compute\\nhardware, we introduce a conceptual model to approximate the lifetime relative\\nscience value of such a system. While the introduced model is not intended to\\nresult in a numeric value for merit, it does enumerate some components that\\ndefine this metric. The intent of this paper is to show how compute system\\nrelated design and procurement decisions in data-intensive science projects\\nshould be weighed and valued. By using both total cost and science value as a\\ndriver, the science output per invested Euro is maximised. With a number of\\ncase studies, focused on computing applications in radio astronomy past,\\npresent and future, we show that the hardware-based analysis can be, and has\\nbeen, applied more broadly.', \"In his Discourse on the Method of Rightly Conducting the Reason, and Seeking\\nTruth in the Sciences, Rene Descartes sought ``clear and certain knowledge of\\nall that is useful in life.'' Almost three centuries later, in ``The\\nfoundations of mathematics,'' David Hilbert tried to ``recast mathematical\\ndefinitions and inferences in such a way that they are unshakable.'' Hilbert's\\nprogram relied explicitly on formal systems (equivalently, computational\\nsystems) to provide certainty in mathematics. The concepts of computation and\\nformal system were not defined in his time, but Descartes' method may be\\nunderstood as seeking certainty in essentially the same way.\\n  In this article, I explain formal systems as concrete artifacts, and\\ninvestigate the way in which they provide a high level of certainty---arguably\\nthe highest level achievable by rational discourse. The rich understanding of\\nformal systems achieved by mathematical logic and computer science in this\\ncentury illuminates the nature of programs, such as Descartes' and Hilbert's,\\nthat seek certainty through rigorous analysis.\", 'Epsilon is an extensible platform of integrated and task-specific languages\\nfor model management. With solutions to the 2011 TTC Hello World case, this\\npaper demonstrates some of the key features of the Epsilon Object Language (an\\nextension and reworking of OCL), which is at the core of Epsilon. In addition,\\nthe paper introduces several of the task-specific languages provided by Epsilon\\nincluding the Epsilon Generation Language (for model-to-text transformation),\\nthe Epsilon Validation Language (for model validation) and Epsilon Flock (for\\nmodel migration).', 'Many simulation based Bounded Model Checking approaches to System Level\\nFormal Verification (SLFV) have been devised. Typically such approaches exploit\\nthe capability of simulators to save computation time by saving and restoring\\nthe state of the system under simulation. However, even though such approaches\\naim to (bounded) formal verification, as a matter of fact, the simulator\\nbehaviour is not formally modelled and the proof of correctness of the proposed\\napproaches basically relies on the intuitive notion of simulator behaviour.\\nThis gap makes it hard to check if the optimisations introduced to speed up the\\nsimulation do not actually omit checking relevant behaviours of the system\\nunder verification.\\n  The aim of this paper is to fill the above gap by presenting a formal\\nsemantics for simulators.', 'Computational Social Choice is an interdisciplinary research area involving\\nEconomics, Political Science, and Social Science on the one side, and\\nMathematics and Computer Science (including Artificial Intelligence and\\nMultiagent Systems) on the other side. Typical computational problems studied\\nin this field include the vulnerability of voting procedures against attacks,\\nor preference aggregation in multi-agent systems. Parameterized Algorithmics is\\na subfield of Theoretical Computer Science seeking to exploit meaningful\\nproblem-specific parameters in order to identify tractable special cases of in\\ngeneral computationally hard problems. In this paper, we propose nine of our\\nfavorite research challenges concerning the parameterized complexity of\\nproblems appearing in this context.', \"Although the computer science community successfully harnessed exponential\\nincreases in computer performance to drive societal and economic change, the\\nexponential growth in publications is proving harder to accommodate. To gain a\\ndeeper understanding of publication growth and inform how the computer science\\ncommunity should handle this growth, we analyzed publication practices from\\nseveral perspectives: ACM sponsored publications in the ACM Digital Library as\\na whole: subdisciplines captured by ACM's Special Interest Groups (SIGs); ten\\ntop conferences; institutions; four top U.S. departments; authors; faculty; and\\nPhDs between 1990 and 2012. ACM publishes a large fraction of all computer\\nscience research. We first summarize how we believe our main findings inform\\n(1) expectations on publication growth, (2) how to distinguish research quality\\nfrom output quantity; and (3) the evaluation of individual researchers. We then\\nfurther motivate the study of computer science publication practices and\\ndescribe our methodology and results in detail.\", 'Computer Science education has been evolving over the years to reflect\\napplied realities. Until about a decade ago, theory of computation, algorithm\\ndesign and system software dominated the curricula. Most courses were\\nconsidered core and were hence mandatory; the programme structure did not allow\\nmuch of a choice or variety. This column analyses why this changed Circa 2010\\nwhen elective subjects across scores of topics become part of mainstream\\neducation to reflect the on-going lateral acceleration of Computer Science.\\nFundamental discoveries in artificial intelligence, machine learning,\\nvirtualization and cloud computing are several decades old. Many core theories\\nin data science are centuries old. Yet their leverage exploded only after Circa\\n2010, when the stage got set for people-centric problem solving in massive\\nscale. This was due in part to the rush of innovative real-world applications\\nthat reached the common man through the ubiquitous smart phone. AI/ML modules\\narrived in popular programming languages; they could be used to build and train\\nmodels on powerful - yet affordable - compute on public clouds reachable\\nthrough high-speed Internet connectivity. Academia responded by adapting\\nComputer Science curricula to align it with the changing technology landscape.\\nThe goal of this experiential piece is to trigger a lively discussion on the\\npast and future of Computer Science education.', 'Social science concerns issues on individuals, relationships, and the whole\\nsociety. The complexity of research topics in social science makes it the\\namalgamation of multiple disciplines, such as economics, political science, and\\nsociology, etc. For centuries, scientists have conducted many studies to\\nunderstand the mechanisms of the society. However, due to the limitations of\\ntraditional research methods, there exist many critical social issues to be\\nexplored. To solve those issues, computational social science emerges due to\\nthe rapid advancements of computation technologies and the profound studies on\\nsocial science. With the aids of the advanced research techniques, various\\nkinds of data from diverse areas can be acquired nowadays, and they can help us\\nlook into social problems with a new eye. As a result, utilizing various data\\nto reveal issues derived from computational social science area has attracted\\nmore and more attentions. In this paper, to the best of our knowledge, we\\npresent a survey on data-driven computational social science for the first time\\nwhich primarily focuses on reviewing application domains involving human\\ndynamics. The state-of-the-art research on human dynamics is reviewed from\\nthree aspects: individuals, relationships, and collectives. Specifically, the\\nresearch methodologies used to address research challenges in aforementioned\\napplication domains are summarized. In addition, some important open challenges\\nwith respect to both emerging research topics and research methods are\\ndiscussed.', 'Computer science has experienced dramatic growth and diversification over the\\nlast twenty years. Towards a current understanding of the structure of this\\ndiscipline, we analyze a cohort of the computer science literature using the\\nDBLP database. For insight on the features of this cohort and the relationship\\nwithin its components, we constructed article level clusters based on either\\ndirect citations or co-citations, and reconciled them to major and minor\\nsubject categories in the Scopus All Science Journal Classification (ASJC). We\\ndescribed complementary insights from clustering by direct citation and\\nco-citation, and both point to the increase in computer science publications\\nand their scope. Our analysis shows cross-category clusters, some that interact\\nwith external fields, such as the biological sciences, while others remain\\ninward looking.', 'In the upcoming decades, the KM3NeT detectors will produce valuable data that\\ncan be used in various scientific contexts from astro- and particle physics to\\nenvironmental and Earth and Sea science. Based on the Open Science policy\\nestablished by the KM3NeT Collaboration, several efforts to offer science-ready\\ndata, foster common analysis approaches and publish open source software are\\ncurrently pursued. In this contribution, ongoing projects focusing on the\\nexchange of high-level data and simulation derivatives, production of particle\\nevent simulations and establishment of an integrated computing environment\\nsupporting an open-science focused workflow will be discussed.', \"Teaching color science to Electrical Engineering and Computer Science (EECS)\\nstudents is critical to preparing them for advanced topics such as graphics,\\nvisualization, imaging, Augmented/Virtual Reality. Color historically receive\\nlittle attention in EECS curriculum; students find it difficult to grasp basic\\nconcepts. This is because today's pedagogical approaches are nonintuitive and\\nlack rigor for teaching color science. We develop a set of interactive\\ntutorials that teach color science to EECS students. Each tutorial is backed up\\nby a mathematically rigorous narrative, but is presented in a form that invites\\nstudents to participate in developing each concept on their own through\\nvisualization tools. This paper describes the tutorial series we developed and\\ndiscusses the design decisions we made.\", 'Materials informatics, data-enabled investigation, is a \"fourth paradigm\" in\\nmaterials science research after the conventional empirical approach,\\ntheoretical science, and computational research. Materials informatics has two\\nessential ingredients: fingerprinting materials proprieties and the theory of\\nstatistical inference and learning. We have researched the organic\\nsemiconductor\\'s enigmas through the materials informatics approach. By applying\\ndiverse neural network topologies, logical axiom, and inferencing information\\nscience, we have developed data-driven procedures for novel organic\\nsemiconductor discovery for the semiconductor industry and knowledge extraction\\nfor the materials science community. We have reviewed and corresponded with\\nvarious algorithms for the neural network design topology for the materials\\ninformatics dataset.', \"An introduction to Extremal Optimization written for the Computer Simulation\\nColumn in ``Computing in Science and Engineering'' (CISE).\", 'Inductive inference is a recursion-theoretic theory of learning, first\\ndeveloped by E. M. Gold (1967). This paper surveys developments in\\nprobabilistic inductive inference. We mainly focus on finite inference of\\nrecursive functions, since this simple paradigm has produced the most\\ninteresting (and most complex) results.', 'The aim of this textbook is to bridge in regard of quantum computation what\\nproves to be a considerable threshold even to the usual science trained\\nreadership between the level of science popularization, and on the other hand,\\nthe presently available more encyclopedic textbooks. In this respect the\\npresent textbook is aimed to be a short, simple, rigorous and direct\\nintroduction, addressing itself only to quantum computation proper.', 'For certain generalized Thue-Morse words t, we compute the \"critical\\nexponent\", i.e., the supremum of the set of rational numbers that are exponents\\nof powers in t, and determine exactly the occurrences of powers realizing it.', 'Modal logics are widely used in computer science. The complexity of their\\nsatisfiability problems has been an active field of research since the 1970s.\\nWe prove that even very \"simple\" modal logics can be undecidable: We show that\\nthere is an undecidable modal logic that can be obtained by restricting the\\nallowed models with a first-order formula in which only universal quantifiers\\nappear.', 'Stock market prediction is the act of trying to determine the future value of\\na company stock or other financial instrument traded on a financial exchange.', 'Selfish routing on dynamic flows over time is used to model scenarios that\\nvary with time in which individual agents act in their best interest. In this\\npaper we provide a survey of a particular dynamic model, the deterministic\\nqueuing model, and discuss how the model can be adjusted and applied to\\ndifferent real-life scenarios. We then examine how these adjustments affect the\\ncomputability, optimality, and existence of selfish routings.', 'In this master thesis, I discuss how the theory of operator algebras, also\\ncalled operator theory, can be applied in quantum computer science.', 'Automatic verification deals with the validation by means of computers of\\ncorrectness certificates. The related tools, usually called proof assistants or\\ninteractive provers, provide an interactive environment for the creation of\\nformal certificates whose correctness can be assessed in a purely automatic\\nway. Such systems have applications both in mathematics, where certificates are\\nproofs of theorems, and in computer science, where certificates testify the\\ncorrectness of a given software with respect to its specification.', \"Engaging students in teaching foundational Computer Science concepts is vital\\nfor the student's continual success in more advanced topics in the field. An\\nidea of a series of Jupyter notebooks was conceived as a way of using Bloom's\\nTaxonomy to reinforce concepts taught in an introductory algorithms class. The\\nidea of the notebook is to keep the student's engaged in the lesson and in turn\\nmotivate them to persevere through the end of the course.\", 'Our objective in this note is to comment briefly on the newly emerging\\nliterature on computer-aided proofs in Social Choice Theory. We shall\\nspecifically comment on two papers, one by Tang and Lin (2009) and another by\\nGeist and Endriss (2011). We also provide statements and brief descriptions of\\nthe results discussed in this note.', 'We discuss quantum non-locality and contextuality, emphasising logical and\\nstructural aspects. We also show how the same mathematical structures arise in\\nvarious areas of classical computation.', 'We motivate why the science of learning to reject model predictions is\\ncentral to ML, and why human computation has a lead role in this effort.', 'We consider the problem whether termination of affine integer loops is\\ndecidable. Since Tiwari conjectured decidability in 2004, only special cases\\nhave been solved. We complement this work by proving decidability for the case\\nthat the update matrix is triangular.', 'This is a draft of a chapter on mathematical logic and foundations for an\\nupcoming handbook of computational proof assistants.', 'Gacs-Kucera Theorem, tightened by Barmpalias and Lewis-Pye, w.t.t.-reduces\\neach infinite sequence to a Kolmogorov--Martin-Lof random one and is broadly\\nused in various Math and CS areas. Its early proofs are somewhat cumbersome,\\nbut using some general concepts yields significant simplification illustrated\\nhere.', 'I provide a perspective on the development of quantum computing for data\\nscience, including a dive into state-of-the-art for both hardware and\\nalgorithms and the potential for quantum machine learning', 'Compact sets in constructive mathematics capture our intuition of what\\ncomputable subsets of the plane (or any other complete metric space) ought to\\nbe. A good representation of compact sets provides an efficient means of\\ncreating and displaying images with a computer. In this paper, I build upon\\nexisting work about complete metric spaces to define compact sets as the\\ncompletion of the space of finite sets under the Hausdorff metric. This\\ndefinition allowed me to quickly develop a computer verified theory of compact\\nsets. I applied this theory to compute provably correct plots of uniformly\\ncontinuous functions.', 'The field of computability and complexity was, where computer science sprung\\nfrom. Turing, Church, and Kleene all developed formalisms that demonstrated\\nwhat they held \"intuitively computable\". The times change however and today\\'s\\n(aspiring) computer scientists are less proficient in building automata or\\ncomposing functions and are much more native to the world of programming\\nlanguages. This article will try to introduce typical concepts of computability\\ntheory and complexity in a form more fitted for a modern developer. It is\\nmostly based on \\\\cite{jones}, but takes input from other sources to provide\\nexamples, additional information, etc.', 'In functional programming, monads are supposed to encapsulate computations,\\neffectfully producing the final result, but keeping to themselves the means of\\nacquiring it. For various reasons, we sometimes want to reveal the internals of\\na computation. To make that possible, in this paper we introduce monad\\ntransformers that add the ability to automatically accumulate observations\\nabout the course of execution as an effect. We discover that if we treat the\\nresulting trace as the actual result of the computation, we can find new\\nfunctionality in existing monads, notably when working with non-terminating\\ncomputations.', 'A brief, general-audience overview of the history of natural language\\nprocessing, focusing on data-driven approaches.Topics include \"Ambiguity and\\nlanguage analysis\", \"Firth things first\", \"A \\'C\\' change\", and \"The empiricists\\nstrike back\".', 'In this paper we propose an algorithm for the numerical solution of arbitrary\\ndifferential equations of fractional order. The algorithm is obtained by using\\nthe following decomposition of the differential equation into a system of\\ndifferential equation of integer order connected with inverse forms of\\nAbel-integral equations. The algorithm is used for solution of the linear and\\nnon-linear equations.', 'We show how solutions to many recursive arena equations can be computed in a\\nnatural way by allowing loops in arenas. We then equip arenas with winning\\nfunctions and total winning strategies. We present two natural winning\\nconditions compatible with the loop construction which respectively provide\\ninitial algebras and terminal coalgebras for a large class of continuous\\nfunctors. Finally, we introduce an intuitionistic sequent calculus, extended\\nwith syntactic constructions for least and greatest fixed points, and prove it\\nhas a sound and (in a certain weak sense) complete interpretation in our game\\nmodel.', 'Recursive domain equations have natural solutions. In particular there are\\ndomains defined by strictly positive induction. The class of countably based\\ndomains gives a computability theory for possibly non-countably based\\ntopological spaces. A $ qcb_{0} $ space is a topological space characterized by\\nits strong representability over domains. In this paper, we study strictly\\npositive inductive definitions for $ qcb_{0} $ spaces by means of domain\\nrepresentations, i.e. we show that there exists a canonical fixed point of\\nevery strictly positive operation on $qcb_{0} $ spaces.', \"We provide a mathematical theory and methodology for synthesising equational\\nlogics from algebraic metatheories. We illustrate our methodology by means of\\ntwo applications: a rational reconstruction of Birkhoff's Equational Logic and\\na new equational logic for reasoning about algebraic structure with\\nname-binding operators.\", 'The random-phase approximation (RPA) as an approach for computing the\\nelectronic correlation energy is reviewed. After a brief account of its basic\\nconcept and historical development, the paper is devoted to the theoretical\\nformulations of RPA, and its applications to realistic systems. With several\\nillustrating applications, we discuss the implications of RPA for computational\\nchemistry and materials science. The computational cost of RPA is also\\naddressed which is critical for its widespread use in future applications. In\\naddition, current correction schemes going beyond RPA and directions of further\\ndevelopment will be discussed.', \"This is a reflection on the author's experience in teaching logic at the\\ngraduate level in a computer science department. The main lesson is that model\\nbuilding and the process of modelling must be placed at the centre stage of\\nlogic teaching. Furthermore, effective use must be supported with adequate\\ntools. Finally, logic is the methodology underlying many applications, it is\\nhence paramount to pass on its principles, methods and concepts to computer\\nscience audiences.\", 'This volume contains the proceedings of the 14th International Conference on\\nQuantum Physics and Logic (QPL 2017), which was held July 3-7, 2017 at the LUX\\nCinema Nijmegen, the Netherlands, and was hosted by Radboud University. QPL is\\na conference that brings together researchers working on mathematical\\nfoundations of quantum physics, quantum computing, and related areas, with a\\nfocus on structural perspectives and the use of logical tools, ordered\\nalgebraic and category-theoretic structures, formal languages, semantical\\nmethods, and other computer science techniques applied to the study of physical\\nbehaviour in general. This conference also welcomes work that applies\\nstructures and methods inspired by quantum theory to other fields (including\\ncomputer science).', 'Multiple studies have shown that gender balance in the fields of Science,\\nTechnology, Engineering and Maths -- and in particular in ICT -- is still far\\nto be achieved. Several initiatives have been recently taken to increase the\\nwomen participation, but it is difficult, at present, to evaluate their impact\\nand their potential of changing the situation. This paper contributes to the\\ndiscussion by presenting a descriptive analysis of the gender balance in\\nComputer Science and Computer Engineering in Italian Universities.', \"From the philosopher's perspective, the interest in quantum computation stems\\nprimarily from the way that it combines fundamental concepts from two distinct\\nsciences: physics (especially quantum mechanics) and computer science, each\\nlong a subject of philosophical speculation and analysis in its own right.\\nQuantum computing combines both of these more traditional areas of inquiry into\\none wholly new (if not quite independent) science. There are philosophical\\nquestions that arise from this merger, and philosophical lessons to be learned.\\nOver the course of this chapter we discuss what I take to be some of the most\\nimportant.\", 'This volume consists of papers presented at the Sixteenth Conference on\\nTheoretical Aspects of Rationality and Knowledge (TARK) held at the University\\nof Liverpool, UK, from July 24 to 26, 2017.\\n  TARK conferences bring together researchers from a wide variety of fields,\\nincluding Computer Science (especially, Artificial Intelligence, Cryptography,\\nDistributed Computing), Economics (especially, Decision Theory, Game Theory,\\nSocial Choice Theory), Linguistics, Philosophy (especially, Philosophical\\nLogic), and Cognitive Psychology, in order to further understand the issues\\ninvolving reasoning about rationality and knowledge.', 'What does the cost of academic publishing look like to the common researcher\\ntoday? Our goal is to convey the current state of academic publishing,\\nspecifically in regards to the field of computer science and provide analysis\\nand data to be used as a basis for future studies. We will focus on author and\\nreader costs as they are the primary points of interaction within the\\npublishing world. In this work, we restrict our focus to only computer science\\nin order to make the data collection more feasible (the authors are computer\\nscientists) and hope future work can analyze and collect data across all\\nacademic fields.', 'Gender diversity in the tech sector is - not yet? - sufficient to create a\\nbalanced ratio of men and women. For many women, access to computer science is\\nhampered by socialization-related, social, cultural and structural obstacles.\\nThe so-called implicit gender bias has a great influence in this respect. The\\nlack of contact in areas of computer science makes it difficult to develop or\\nexpand potential interests. Female role models as well as more transparency of\\nthe job description should help women to promote their - possible - interest in\\nthe job description. However, gender diversity can also be promoted and\\nfostered through adapted measures by leaders.', 'Many fundamental questions in theoretical computer science are naturally\\nexpressed as special cases of the following problem: Let $G$ be a complex\\nreductive group, let $V$ be a $G$-module, and let $v,w$ be elements of $V$.\\nDetermine if $w$ is in the $G$-orbit closure of $v$. I explain the computer\\nscience problems, the questions in representation theory and algebraic geometry\\nthat they give rise to, and the new perspectives on old areas such as invariant\\ntheory that have arisen in light of these questions. I focus primarily on the\\ncomplexity of matrix multiplication.', 'This volume contains the proceedings of the 17th International Conference on\\nQuantum Physics and Logic (QPL 2020), which was held June 2-6, 2020. Quantum\\nPhysics and Logic is an annual conference that brings together researchers\\nworking on mathematical foundations of quantum physics, quantum computing, and\\nrelated areas, with a focus on structural perspectives and the use of logical\\ntools, ordered algebraic and category-theoretic structures, formal languages,\\nsemantical methods, and other computer science techniques applied to the study\\nof physical behavior in general. Work that applies structures and methods\\ninspired by quantum theory to other fields (including computer science) is also\\nwelcome.', 'Semantic technologies are evolving and being applied in several research\\nareas, including the education domain. This paper presents the outcomes of a\\nsystematic review carried out to provide an overview of the application of\\nsemantic technologies in the context of the Computer Science curriculum and\\ndiscuss the limitations in this field whilst offering insights for future\\nresearch. A total of 4,510 studies were reviewed, and 37 were analysed and\\nreported. As a result, while semantic technologies have been increasingly used\\nto develop Computer Science curricula, the alignment of ontologies and accurate\\ncurricula assessment appears to be the most significant limitations to the\\nwidespread adoption of such technologies.', 'This study presents an automated bibliometric analysis of 6569 research\\npapers published in thirteen Brazilian Computer Science Society (SBC)\\nconferences from 1999 to 2021. Our primary goal was to gather data to\\nunderstand the gender representation in publications in the field of Computer\\nScience. We applied a systematic assignment of gender to 23.573 listed papers\\nauthorships, finding that the gender gap for women is significant, with female\\nauthors being under-represented in all years of the study.', \"In this article we survey the main research topics of our group at the\\nUniversity of Essex. Our research interests lie at the intersection of\\ntheoretical computer science, artificial intelligence, and economic theory. In\\nparticular, we focus on the design and analysis of mechanisms for systems\\ninvolving multiple strategic agents, both from a theoretical and an applied\\nperspective. We present an overview of our group's activities, as well as its\\nmembers, and then discuss in detail past, present, and future work in\\nmulti-agent systems.\", 'A computer program LMTART for electronic structure calculations using full\\npotential linear muffin-tin orbital method is described', 'It is shown that the decision problem for the temporal logic with until and\\nsince connectives over real-numbers time is PSPACE-complete.', 'Are P and NP provably inseparable ? Take a look at some unorthodox, guilty\\nmentioned folklore and related unpublished results.', 'This paper proposes a computational method for obtaining the length of the\\ncycle that arises from the Fibonacci series taken mod m (some number) and mod p\\n(some prime number).', 'The following four classes of computational problems are equivalent: solving\\nmatrix games, solving linear programs, best $l^{\\\\infty}$ linear approximation,\\nbest $l^1$ linear approximation.', 'The algorithm checks the propositional formulas for patterns of\\nunsatisfiability.', 'This paper employs a powerful argument, called an algorithmic argument, to\\nprove lower bounds of the quantum query complexity of a multiple-block ordered\\nsearch problem in which, given a block number i, we are to find a location of a\\ntarget keyword in an ordered list of the i-th block. Apart from much studied\\npolynomial and adversary methods for quantum query complexity lower bounds, our\\nargument shows that the multiple-block ordered search needs a large number of\\nnonadaptive oracle queries on a black-box model of quantum computation that is\\nalso supplemented with advice. Our argument is also applied to the notions of\\ncomputational complexity theory: quantum truth-table reducibility and quantum\\ntruth-table autoreducibility.', \"I comment on GianCarlo Ghirardi's criticism of my claim that quantum\\ncomputation has no measurement problem.\", 'This paper examines several computer algorithms designed to assess mortality\\nand longevity risk.', 'Bridging cultures that have often been distant, Julia combines expertise from\\nthe diverse fields of computer science and computational science to create a\\nnew approach to numerical computing. Julia is designed to be easy and fast.\\nJulia questions notions generally held as \"laws of nature\" by practitioners of\\nnumerical computing:\\n  1. High-level dynamic programs have to be slow.\\n  2. One must prototype in one language and then rewrite in another language\\nfor speed or deployment, and\\n  3. There are parts of a system for the programmer, and other parts best left\\nuntouched as they are built by the experts.\\n  We introduce the Julia programming language and its design --- a dance\\nbetween specialization and abstraction. Specialization allows for custom\\ntreatment. Multiple dispatch, a technique from computer science, picks the\\nright algorithm for the right circumstance. Abstraction, what good computation\\nis really about, recognizes what remains the same after differences are\\nstripped away. Abstractions in mathematics are captured as code through another\\ntechnique from computer science, generic programming.\\n  Julia shows that one can have machine performance without sacrificing human\\nconvenience.', 'The aim of this short paper is to give a practical introduction to functional\\ninterpretation of proofs for computer scientists interested in synthesis.', 'With the relentless rise of computer power, there is a widespread expectation\\nthat computers can solve the most pressing problems of science, and even more\\nbesides. We explore the limits of computational modelling and conclude that, in\\nthe domains of science and engineering that are relatively simple and firmly\\ngrounded in theory, these methods are indeed powerful. Even so, the\\navailability of code, data and documentation, along with a range of techniques\\nfor validation, verification and uncertainty quantification, are essential for\\nbuilding trust in computer generated findings. When it comes to complex systems\\nin domains of science that are less firmly grounded in theory, notably biology\\nand medicine, to say nothing of the social sciences and humanities, computers\\ncan create the illusion of objectivity, not least because the rise of big data\\nand machine learning pose new challenges to reproducibility, while lacking true\\nexplanatory power. We also discuss important aspects of the natural world which\\ncannot be solved by digital means. In the long-term, renewed emphasis on\\nanalogue methods will be necessary to temper the excessive faith currently\\nplaced in digital computation.', 'Data science has arrived, and computational statistics is its engine. As the\\nscale and complexity of scientific and industrial data grow, the discipline of\\ncomputational statistics assumes an increasingly central role among the\\nstatistical sciences. An explosion in the range of real-world applications\\nmeans the development of more and more specialized computational methods, but\\nfive Core Challenges remain. We provide a high-level introduction to\\ncomputational statistics by focusing on its central challenges, present recent\\nmodel-specific advances and preach the ever-increasing role of non-sequential\\ncomputational paradigms such as multi-core, many-core and quantum computing.\\nData science is bringing major changes to computational statistics, and these\\nchanges will shape the trajectory of the discipline in the 21st century.', \"The measurable properties of the artifacts or objects in the computer,\\nmanagement, or finance disciplines are extrinsic, not inherent -- dependent on\\ntheir problem definitions and solution instantiations. Only after the\\ninstantiation can the solutions to the problem be measured. The processes of\\ndefinition, instantiation, and measurement are entangled, and they have complex\\nmutual influences. Meanwhile, the technology inertia brings instantiation bias\\n-- trapped into a subspace or even a point at a high-dimension solution space.\\nThese daunting challenges, which emerging computing aggravates, make metrology\\ncan not work for benchmark communities. It is pressing to establish independent\\nbenchmark science and engineering.\\n  This article presents a unifying benchmark definition, a conceptual\\nframework, and a traceable and supervised learning-based benchmarking\\nmethodology, laying the foundation for benchmark science and engineering. I\\nalso discuss BenchCouncil's plans for emerging and future computing. The\\nongoing projects include defining the challenges of intelligence, instinct,\\nquantum computers, Metaverse, planet-scale computers, and reformulating data\\ncenters, artificial intelligence for science, and CPU benchmark suites. Also,\\nBenchCouncil will collaborate with ComputerCouncil on open-source computer\\nsystems for planet-scale computing, AI for science systems, and Metaverse.\", \"Cognitive Computing (COC) aims to build highly cognitive machines with low\\ncomputational resources that respond in real-time. However, scholarly\\nliterature shows varying research areas and various interpretations of COC.\\nThis calls for a cohesive architecture that delineates the nature of COC. We\\nargue that if Herbert Simon considered the design science is the science of\\nartificial, cognitive systems are the products of cognitive science or 'the\\nnewest science of the artificial'. Therefore, building a conceptual basis for\\nCOC is an essential step into prospective cognitive computing-based systems.\\nThis paper proposes an architecture of COC through analyzing the literature on\\nCOC using a myriad of statistical analysis methods. Then, we compare the\\nstatistical analysis results with previous qualitative analysis results to\\nconfirm our findings. The study also comprehensively surveys the recent\\nresearch on COC to identify the state of the art and connect the advances in\\nvaried research disciplines in COC. The study found that there are three\\nunderlaying computing paradigms, Von-Neuman, Neuromorphic Engineering and\\nQuantum Computing, that comprehensively complement the structure of cognitive\\ncomputation. The research discuss possible applications and open research\\ndirections under the COC umbrella.\", 'The aim of this paper is to propose a strategy to implement the Minimal Model\\nProgram in modern computer algebra systems.', \"We present an overview of current academic curricula for Scientific\\nComputing, High-Performance Computing and Data Science. After a survey of\\ncurrent academic and non-academic programs across the globe, we focus on\\nCanadian programs and specifically on the education program of the SciNet HPC\\nConsortium, using its detailed enrollment and course statistics for the past\\nfour to five years. Not only do these data display a steady and rapid increase\\nin the demand for research-computing instruction, they also show a clear shift\\nfrom traditional (high performance) computing to data-oriented methods. It is\\nargued that this growing demand warrants specialized research computing\\ndegrees. The possible curricula of such degrees are described next, taking\\nexisting programs as an example, and adding SciNet's experiences of student\\ndesires as well as trends in advanced research computing.\", 'ESCAPE (European Science Cluster of Astronomy and Particle physics ESFRI\\nresearch infrastructures) is a project to set up a cluster of ESFRI (European\\nStrategy Forum on Research Infrastructures) facilities for astronomy,\\nastroparticle and particle physics to face the challenges emerging through the\\nmodern multi-disciplinary data driven science. One of the main goal of ESCAPE\\nis the building of ESAP (ESFRI Science Analysis Platform), a science platform\\nfor the analysis of open access data available through the EOSC (European Open\\nScience Cloud) environment. ESAP will allow EOSC researchers to identify and\\nstage existing data collections for analysis, share data, share and run\\nscientific workflows. For many of the concerned ESFRIs and RIs, the data scales\\ninvolved require significant computational resources (storage and compute) to\\nsupport processing and analysis. The EOSC-ESFRI science platform therefore must\\nimplement appropriate interfaces to an underlying HPC (High Performance\\nComputing) or HTC (High Throughput Computing) infrastructure to take advantage\\nof it. This poster describes the analysis done to identify the main\\nrequirements for the implementation of the interfaces enabling the ESAP data\\naccess and computation resources integration in HPC and HTC computation\\ninfrastructures in terms of authentication and authorization policies, data\\nmanagement, workflow deployment and run.', 'Machine Science, or Data-driven Research, is a new and interesting scientific\\nmethodology that uses advanced computational techniques to identify, retrieve,\\nclassify and analyse data in order to generate hypotheses and develop models.\\nIn this paper we describe three recent biomedical Machine Science studies, and\\nuse these to assess the current state of the art with specific emphasis on data\\nmining, data assessment, costs, limitations, skills and tool support.', 'This paper describes the development of iEnvironment, an open science\\nsoftware platform that supports monitoring and modeling of aspects of surface\\nwater. The platform supports science and engineering research, especially in\\nthe context of the creation, sharing, analysis and maintenance of big and open\\ndata. In this era of big data, iEnvironment facilitates access to open data\\nresources and research collaboration among science and research disciplines\\nsupported by computer scientists and software developers.', 'Many fields of science rely on software systems to answer different research\\nquestions. For valid results researchers need to trust the results scientific\\nsoftware produces, and consequently quality assurance is of utmost importance.\\nIn this paper we are investigating the impact of quality assurance in the\\ndomain of computational materials science (CMS). Based on our experience in\\nthis domain we formulate challenges for validation and verification of\\nscientific software and their results. Furthermore, we describe directions for\\nfuture research that can potentially help dealing with these challenges.', 'Citizen science changes the way scientific research is pursued. It opens up\\ndata collection and analysis to the general public, to the wisdom of crowds. In\\nthis emerging area, there is much research to be done to better understand how\\nwe can develop citizen science infrastructure and continue the democratization\\nof science. In creating such systems, there is much we can learn from\\nprinciples that have emerged out of computer-supported cooperative work (CSCW)\\nresearch. In this paper, I use a nine-step framework to highlight where CSCW\\nknowledge can contribute.', 'Metropolis Monte Carlo simulations of the eutectic NaK alloy are performed\\nusing the Second Moment Approximation (SMA) model potential across a wide range\\nof temperatures at constant pressure. The alloy structure and thermodynamics\\nare analyzed along with the atomic level structures using a variety of\\nstructure identification methods. Both enthalpy and density are followed along\\nan annealing process that reveals a clear melting point around 250 K. At lower\\ntemperatures, two thermodynamic branches are identified as crystalline and\\namorphous solids.', 'I draw attention to statistical, probabilistic, computer science aspects of\\nthe highly related topics of the Bell game and of a possible future Quantum\\nInternet.', \"Recently, a full-scale data processing workflow of the Square Kilometre Array\\n(SKA) Phase 1 was successfully executed on the world's fastest supercomputer\\nSummit, proving that scientists have the expertise, software tools and\\ncomputing resources to process the SKA data. The SKA-Summit experiment shows\\nthe importance of multidisciplinary cooperation between astronomy, computer\\nscience and others communities. The SKA science cannot be achieved without the\\njoint efforts of talents from multiple fields.\", 'GeoAI, or geospatial artificial intelligence, is an exciting new area that\\nleverages artificial intelligence (AI), geospatial big data, and massive\\ncomputing power to solve problems with high automation and intelligence. This\\npaper reviews the progress of AI in social science research, highlighting\\nimportant advancements in using GeoAI to fill critical data and knowledge gaps.\\nIt also discusses the importance of breaking down data silos, accelerating\\nconvergence among GeoAI research methods, as well as moving GeoAI beyond\\ngeospatial benefits.', 'It may seem surprising that, out of all areas of science, computer scientists\\nhave been slow to post electronic versions of papers on sites like arXiv.org.\\nInstead, computer scientists have tended to place papers on our individual home\\npages, but this loses the benefits of aggregation, namely notification and\\nbrowsing.\\n  But this is changing. More and more computer scientists are now using the\\narXiv. At the same time, there is ongoing discussion and controversy about how\\nprepublication affects peer review, especially for double-blind conferences.\\nThis discussion is often carried out with precious little evidence of how\\npopular prepublication is.\\n  We measure what percentage of papers in computer science are placed on the\\narXiv, by cross-referencing published papers in DBLP with e-prints on arXiv. We\\nfound:\\n  * Usage of arXiv.org has risen dramatically among the most selective\\nconferences in computer science. In 2017, fully 23% of papers had e-prints on\\narXiv, compared to only 1% ten years ago.\\n  * Areas of computer science vary widely in e-print prevalence. In theoretical\\ncomputer science and machine learning, over 60% of published papers are on\\narXiv, while other areas are essentially zero. In most areas, arXiv usage is\\nrising.\\n  * Many researchers use arXiv for posting preprints. Of the 2017 published\\npapers with arXiv e-prints, 56% were preprints that were posted before or\\nduring peer review.\\n  Our paper describes these results as well as policy implications for\\nresearchers and practitioners.', 'Over the last decade, computer science has made progress towards extracting\\nbody pose from single camera photographs or videos. This promises to enable\\nmovement science to detect disease, quantify movement performance, and take the\\nscience out of the lab into the real world. However, current pose tracking\\nalgorithms fall short of the needs of movement science; the types of movement\\ndata that matter are poorly estimated. For instance, the metrics currently used\\nfor evaluating pose tracking algorithms use noisy hand-labeled ground truth\\ndata and do not prioritize precision of relevant variables like\\nthree-dimensional position, velocity, acceleration, and forces which are\\ncrucial for movement science. Here, we introduce the scientific disciplines\\nthat use movement data, the types of data they need, and discuss the changes\\nneeded to make pose tracking truly transformative for movement science.', 'Data is a crucial raw material of this century, and the amount of data that\\nhas been created in materials science in recent years and is being created\\nevery new day is immense. Without a proper infrastructure that allows for\\ncollecting and sharing data (including the original data), the envisioned\\nsuccess of materials science and, in particular, Big-Data driven materials\\nscience will be hampered. For the field of computational materials science, the\\nNOMAD (Novel Materials Discovery) Center of Excellence (CoE) has changed the\\nscientific culture towards a comprehensive and FAIR data sharing, opening new\\navenues for mining Big-Data of materials science. Novel data-analytics concepts\\nand tools turn data into knowledge and help the prediction of new materials or\\nthe identification of new properties of already known materials.', 'The growth of the data science field requires better tools to understand such\\na fast-paced growing domain. Moreover, individuals from different backgrounds\\nbecame interested in following a career as data scientists. Therefore,\\nproviding a quantitative guide for individuals and organizations to understand\\nthe skills required in the job market would be crucial. This paper introduces a\\nframework to analyze the job market for data science-related jobs within the US\\nwhile providing an interface to access insights in this market. The proposed\\nframework includes three sub-modules allowing continuous data collection,\\ninformation extraction, and a web-based dashboard visualization to investigate\\nthe spatial and temporal distribution of data science-related jobs and skills.\\nThe result of this work shows important skills for the main branches of data\\nscience jobs and attempts to provide a skill-based definition of these data\\nscience branches. The current version of this application is deployed on the\\nweb and allows individuals and institutes to investigate skills required for\\ndata science positions through the industry lens.', 'This volume contains the proceedings of the Ninth Workshop on Fixed Points in\\nComputer Science which took place on the September 1st, 2013 in Torino, Italy\\nas a CSL-affiliated workshop. Past workshops have been held in Brno (1998,\\nMFCS/CSL workshop), Paris (2000, LC workshop), Florence (2001, PLI workshop),\\nCopenhagen (2002, LICS (FLoC) workshop), Warsaw (2003, ETAPS workshop), Coimbra\\n(2009, CSL workshop), Brno (2010, MFCS-CSL workshop), Tallinn (2012, CSL\\nworkshop). Fixed points play a fundamental role in several areas of computer\\nscience. They are used to justify (co)recursive definitions and associated\\nreasoning techniques. The construction and properties of fixed points have been\\ninvestigated in many different settings such as: design and implementation of\\nprogramming languages, logics, verification, databases. The aim of this\\nworkshop is to provide a forum for researchers to present their results to\\nthose members of the computer science and logic communities who study or apply\\nthe theory of fixed points.', 'Major breakthrough in quantum computation has recently been achieved using\\nquantum annealing to develop analog quantum computers instead of gate based\\ncomputers. After a short introduction to quantum computation, we retrace very\\nbriefly the history of these developments and discuss the Indian researches in\\nthis connection and provide some interesting documents (in the Figs.) obtained\\nfrom a chosen set of high impact papers (and also some recent news etc. blogs\\nappearing in the Internet). This note is also designed to supplement an earlier\\nnote by Bose (Science and Culture, 79, pp. 337-378, 2013).', \"Studies of scientists building models show that the development of scientific\\nmodels involves a great deal of subjectivity. However, science as experienced\\nin school settings typically emphasizes an overly objective and rationalistic\\nview. In this paper, we argue for focusing on the development of disciplined\\ninterpretation as an epistemic and representational practice that progressively\\ndeepens students' computational modeling in science by valuing, rather than\\ndeemphasizing, the subjective nature of the experience of modeling. We report\\nresults from a study in which fourth grade children engaged in computational\\nmodeling throughout the academic year. We present three salient themes that\\ncharacterize the development of students' disciplined interpretations in terms\\nof their development of computational modeling as a way of seeing and doing\\nscience.\", 'This volume contains the proceedings of the Eighth Workshop on Fixed Points\\nin Computer Science which took place on 24 March 2012 in Tallinn, Estonia as an\\nETAPS-affiliated workshop. Past workshops have been held in Brno (1998,\\nMFCS/CSL workshop), Paris (2000, LC workshop), Florence (2001, PLI workshop),\\nCopenhagen (2002, LICS (FLoC) workshop), Warsaw (2003, ETAPS workshop), Coimbra\\n(2009, CSL workshop), and Brno (2010, MFCS-CSL workshop).\\n  Fixed points play a fundamental role in several areas of computer science and\\nlogic by justifying induction and recursive definitions. The construction and\\nproperties of fixed points have been investigated in many different frameworks\\nsuch as: design and implementation of programming languages, program logics,\\nand databases. The aim of this workshop is to provide a forum for researchers\\nto present their results to those members of the computer science and logic\\ncommunities who study or apply the theory of fixed points.', 'We study a composition operation on monads, equivalently presented as large\\nequational theories. Specifically, we discuss the existence of tensors, which\\nare combinations of theories that impose mutual commutation of the operations\\nfrom the component theories. As such, they extend the sum of two theories,\\nwhich is just their unrestrained combination. Tensors of theories arise in\\nseveral contexts; in particular, in the semantics of programming languages, the\\nmonad transformer for global state is given by a tensor. We present two main\\nresults: we show that the tensor of two monads need not in general exist by\\npresenting two counterexamples, one of them involving finite powerset (i.e. the\\ntheory of join semilattices); this solves a somewhat long-standing open\\nproblem, and contrasts with recent results that had ruled out previously\\nexpected counterexamples. On the other hand, we show that tensors with bounded\\npowerset monads do exist from countable powerset upwards.', 'Knowledge-based biomedical data science (KBDS) involves the design and\\nimplementation of computer systems that act as if they knew about biomedicine.\\nSuch systems depend on formally represented knowledge in computer systems,\\noften in the form of knowledge graphs. Here we survey the progress in the last\\nyear in systems that use formally represented knowledge to address data science\\nproblems in both clinical and biological domains, as well as on approaches for\\ncreating knowledge graphs. Major themes include the relationships between\\nknowledge graphs and machine learning, the use of natural language processing,\\nand the expansion of knowledge-based approaches to novel domains, such as\\nChinese Traditional Medicine and biodiversity.', 'Probabilistic automata constitute a versatile and elegant model for\\nconcurrent probabilistic systems. They are equipped with a compositional theory\\nsupporting abstraction, enabled by weak probabilistic bisimulation serving as\\nthe reference notion for summarising the effect of abstraction. This paper\\nconsiders probabilistic automata augmented with costs. It extends the notions\\nof weak transitions in probabilistic automata in such a way that the costs\\nincurred along a weak transition are captured. This gives rise to\\ncost-preserving and cost-bounding variations of weak probabilistic\\nbisimilarity, for which we establish compositionality properties with respect\\nto parallel composition. Furthermore, polynomial-time decision algorithms are\\nproposed, that can be effectively used to compute reward-bounding abstractions\\nof Markov decision processes in a compositional manner.', 'We present a binary session type system using context-free session types to a\\nversion of the applied pi-calculus of Abadi et. al. where only base terms,\\nconstants and channels can be sent. Session types resemble process terms from\\nBPA and we use a version of bisimulation equivalence to characterize type\\nequivalence. We present a quotiented type system defined on type equivalence\\nclasses for which type equivalence is built into the type system. Both type\\nsystems satisfy general soundness properties; this is established by an appeal\\nto a generic session type system for psi-calculi.', 'Functor lifting along a fibration is used for several different purposes in\\ncomputer science. In the theory of coalgebras, it is used to define coinductive\\npredicates, such as simulation preorder and bisimilarity. Codensity lifting is\\na scheme to obtain a functor lifting along a fibration. It generalizes a few\\nprevious lifting schemes including the Kantorovich lifting. In this paper, we\\nseek a property of functor lifting called fiberedness. Hinted by a known result\\nfor Kantorovich lifting, we identify a sufficient condition for a codensity\\nlifting to be fibered. We see that this condition applies to many examples that\\nhave been studied. As an application, we derive some results on\\nbisimilarity-like notions.', 'This article discusses how to create an interactive virtual training program\\nat the intersection of neuroscience, robotics, and computer science for high\\nschool students. A four-day microseminar, titled Swarming Powered by\\nNeuroscience (SPN), was conducted virtually through a combination of\\npresentations and interactive computer game simulations, delivered by subject\\nmatter experts in neuroscience, mathematics, multi-agent swarm robotics, and\\neducation. The objective of this research was to determine if taking an\\ninterdisciplinary approach to high school education would enhance the students\\nlearning experiences in fields such as neuroscience, robotics, or computer\\nscience. This study found an improvement in student engagement for neuroscience\\nby 16.6%, while interest in robotics and computer science improved respectively\\nby 2.7% and 1.8%. The curriculum materials, developed for the SPN microseminar,\\ncan be used by high school teachers to further evaluate interdisciplinary\\ninstructions across life and physical sciences and computer science.', 'Artificial Intelligence (AI) is now entering every sub-field of science,\\ntechnology, engineering, arts, and management. Thanks to the hype and\\navailability of research funds, it is being adapted in many fields without much\\nthought. Computational Science and Engineering (CS&E) is one such sub-field. By\\nhighlighting some critical questions around the issues and challenges in\\nadapting Machine Learning (ML) for CS&E, most of which are often overlooked in\\njournal papers, this contribution hopes to offer some insights into the\\nadaptation of ML for applications in CS\\\\&E and related fields. This is a\\ngeneral-purpose article written for a general audience and researchers new to\\nthe fields of ML and/or CS\\\\&E. This work focuses only on the forward problems\\nin computational science and engineering. Some basic equations and MATLAB code\\nare also provided to help the reader understand the basics.', 'There has been growing interest within the computational science and\\nengineering (CSE) community in engaging with software engineering research --\\nthe systematic study of software systems and their development, operation, and\\nmaintenance -- to solve challenges in scientific software development.\\nHistorically, there has been little interaction between scientific computing\\nand the field, which has held back progress. With the ranks of scientific\\nsoftware teams expanding to include software engineering researchers and\\npractitioners, we can work to build bridges to software science and reap the\\nrewards of evidence-based practice in software development.', \"Science and technology journalists today face challenges in finding\\nnewsworthy leads due to increased workloads, reduced resources, and expanding\\nscientific publishing ecosystems. Given this context, we explore computational\\nmethods to aid these journalists' news discovery in terms of time-efficiency\\nand agency. In particular, we prototyped three computational information\\nsubsidies into an interactive tool that we used as a probe to better understand\\nhow such a tool may offer utility or more broadly shape the practices of\\nprofessional science journalists. Our findings highlight central considerations\\naround science journalists' agency, context, and responsibilities that such\\ntools can influence and could account for in design. Based on this, we suggest\\ndesign opportunities for greater and longer-term user agency; incorporating\\ncontextual, personal and collaborative notions of newsworthiness; and\\nleveraging flexible interfaces and generative models. Overall, our findings\\ncontribute a richer view of the sociotechnical system around computational news\\ndiscovery tools, and suggest ways to improve such tools to better support the\\npractices of science journalists.\", 'This note presents the first known class of termination orders for\\n3-polygraphs, together with an application.', 'We review some approaches and philosophies of causal inference coming from\\nsociology, economics, computer science, cognitive science, and statistics', \"This is part of a collection of discussion pieces on David Donoho's paper 50\\nYears of Data Science, appearing in Volume 26, Issue 4 of the Journal of\\nComputational and Graphical Statistics (2017).\", 'This article highlights synergies between quantum information science (QIS)\\nand operations research for QIS-curious operations researchers (and\\nvice-versa).', 'In this note it is shown that for a mono-energetic collection of Bosons, at a\\ncertain (non-zero) momentum or temperature, there is condensation while there\\nis another momentum or temperature at which there is infinite dilution and\\nbelow which the gas exhibits anomalous Fermionic behaviour.', 'Supportive attitudes can bring to a blossoming science, while neglect can\\nquickly make science absent from everyday life and provide a very primitive\\nview of the world. We compare one important Greek achievement, the computation\\nof the Earth meridian by Eratosthenes, to its later interpretation by the Roman\\nhistorian of science Pliny.', 'There is a perception that climate science can only be approached with\\ncomplex computer simulations. But working climate scientists often use simple\\nmodels to understand their simulations and make order-of-magnitude estimates.\\nThis article presents some of these simple models with the goal of making\\nclimate science more accessible and comprehensible.', 'In this paper we aim at providing a general reflection around the present and\\nfuture of social media metrics (or altmetrics) and how they could evolve into a\\nnew discipline focused on the study of the relationships and interactions\\nbetween science and social media, in what could be seen as the social media\\nstudies of science.', 'The Fourier transform operation is an important conceptual as well as\\ncomputational tool in the arsenal of every practitioner of physical and\\nmathematical sciences. We discuss some of its applications in optical science\\nand engineering, with the goal of providing a broad perspective on the intimate\\nrelation between the physical and mathematical concepts that are elegantly\\ninterwoven within the theory of Fourier transforms.', 'This volume contains the proceedings of the 7th Working Formal Methods\\nSymposium, which was held at the University of Bucharest, September 21-22,\\n2023.', \"The advent of experimental science facilities-instruments and observatories,\\nsuch as the Large Hadron Collider, the Laser Interferometer Gravitational Wave\\nObservatory, and the upcoming Large Synoptic Survey Telescope-has brought about\\nchallenging, large-scale computational and data processing requirements.\\nTraditionally, the computing infrastructure to support these facility's\\nrequirements were organized into separate infrastructure that supported their\\nhigh-throughput needs and those that supported their high-performance computing\\nneeds. We argue that to enable and accelerate scientific discovery at the scale\\nand sophistication that is now needed, this separation between high-performance\\ncomputing and high-throughput computing must be bridged and an integrated,\\nunified infrastructure provided. In this paper, we discuss several case studies\\nwhere such infrastructure has been implemented. These case studies span\\ndifferent science domains, software systems, and application requirements as\\nwell as levels of sustainability. A further aim of this paper is to provide a\\nbasis to determine the common characteristics and requirements of such\\ninfrastructure, as well as to begin a discussion of how best to support the\\ncomputing requirements of existing and future experimental science facilities.\", 'We introduce an affine generalization of counter automata, and analyze their\\nability as well as affine finite automata. Our contributions are as follows. We\\nshow that there is a language that can be recognized by exact realtime affine\\ncounter automata but by neither 1-way deterministic pushdown automata nor\\nrealtime deterministic k-counter automata. We also show that a certain promise\\nproblem, which is conjectured not to be solved by two-way quantum finite\\nautomata in polynomial time, can be solved by Las Vegas affine finite automata.\\nLastly, we show that how a counter helps for affine finite automata by showing\\nthat the language MANYTWINS, which is conjectured not to be recognized by\\naffine, quantum or classical finite state models in polynomial time, can be\\nrecognized by affine counter automata with one-sided bounded-error in realtime.', 'Discusses how CoRR was set up and some policy issues involved with setting up\\nsuch a repository.', \"We give a detailed treatment of the ``bit-model'' of computability and\\ncomplexity of real functions and subsets of R^n, and argue that this is a good\\nway to formalize many problems of scientific computation. In the introduction\\nwe also discuss the alternative Blum-Shub-Smale model. In the final section we\\ndiscuss the issue of whether physical systems could defeat the Church-Turing\\nThesis.\", 'To make arbitrarily accurate quantum computation possible, practical\\nrealization of quantum computers will require suppressing noise in quantum\\nmemory and gate operations to make it below a threshold value. A scheme based\\non realistic quantum computer models is described for suppressing noise in\\nquantum computation without the cost of stringent quantum computing resources.', \"We introduce a formal definition of Wolfram's notion of computational process\\nbased on cellular automata, a physics-like model of computation. There is a\\nnatural classification of these processes into decidable, intermediate and\\ncomplete. It is shown that in the context of standard finite injury priority\\narguments one cannot establish the existence of an intermediate computational\\nprocess.\", 'We propose a definition of quantum computable functions as mappings between\\nsuperpositions of natural numbers to probability distributions of natural\\nnumbers. Each function is obtained as a limit of an infinite computation of a\\nquantum Turing machine. The class of quantum computable functions is\\nrecursively enumerable, thus opening the door to a quantum computability theory\\nwhich may follow some of the classical developments.', 'Vagueness is something everyone is familiar with. In fact, most people think\\nthat vagueness is closely related to language and exists only there. However,\\nvagueness is a property of the physical world. Quantum computers harness\\nsuperposition and entanglement to perform their computational tasks. Both\\nsuperposition and entanglement are vague processes. Thus quantum computers,\\nwhich process exact data without \"exploiting\" vagueness, are actually vague\\ncomputers.', 'We introduce an axiomatization for the notion of computation. Based on the\\nidea of Brouwer choice sequences, we construct a model, denoted by $E$, which\\nsatisfies our axioms and $E \\\\models \\\\mathrm{ P \\\\neq NP}$. In other words,\\nregarding \"effective computability\" in Brouwer intuitionism viewpoint, we show\\n$\\\\mathrm{ P \\\\neq NP}$.', \"The concept of Science 2.0 was introduced almost a decade ago to describe the\\nnew generation of online-based tools for researchers allowing easier data\\nsharing, collaboration and publishing. Although technically sound, the concept\\nstill does not work as expected. Here we provide a systematic line of arguments\\nto modify the concept of Science 2.0, making it more consistent with the spirit\\nand traditions of science and Internet. Our first correction to the Science 2.0\\nparadigm concerns the open-access publication models charging fees to the\\nauthors. As discussed elsewhere, we show that the monopoly of such publishing\\nmodels increases biases and inequalities in the representation of scientific\\nideas based on the author's income. Our second correction concerns\\npost-publication comments online, which are all essentially non-anonymous in\\nthe current Science 2.0 paradigm. We conclude that scientific post-publication\\ndiscussions require special anonymization systems. We further analyze the\\nreasons of the failure of the current post-publication peer-review models and\\nsuggest what needs to be changed in Science 3.0 to convert Internet into a\\nlarge journal club.\", 'The rapid proliferation of online content producing and sharing technologies\\nresulted in an explosion of user-generated content (UGC), which now extends to\\nscientific data. Citizen science, in which ordinary people contribute\\ninformation for scientific research, epitomizes UGC. Citizen science projects\\nare typically open to everyone, engage diverse audiences, and challenge\\nordinary people to produce data of highest quality to be usable in science.\\nThis also makes citizen science a very exciting area to study both traditional\\nand innovative approaches to information quality management. With this paper we\\nposition citizen science as a leading information quality research frontier. We\\nalso show how citizen science opens a unique opportunity for the information\\nsystems community to contribute to a broad range of disciplines in natural and\\nsocial sciences and humanities.', 'Researchers may be tempted to attract attention through poetic titles for\\ntheir publications, but would this be mistaken in some fields? Whilst poetic\\ntitles are known to be common in medicine, it is not clear whether the practice\\nis widespread elsewhere. This article investigates the prevalence of poetic\\nexpressions in journal article titles 1996-2019 in 3.3 million articles from\\nall 27 Scopus broad fields. Expressions were identified by manually checking\\nall phrases with at least 5 words that occurred at least 25 times, finding 149\\nstock phrases, idioms, sayings, literary allusions, film names and song titles\\nor lyrics. The expressions found are most common in the social sciences and the\\nhumanities. They are also relatively common in medicine, but almost absent from\\nengineering and the natural and formal sciences. The differences may reflect\\nthe less hierarchical and more varied nature of the social sciences and\\nhumanities, where interesting titles may attract an audience. In engineering,\\nnatural science and formal science fields, authors should take extra care with\\npoetic expressions, in case their choice is judged inappropriate. This includes\\ninterdisciplinary research overlapping these areas. Conversely, reviewers of\\ninterdisciplinary research involving the social sciences should be more\\ntolerant of poetic license.', 'Software security can be ensured by specifying and verifying security\\nproperties of software using formal methods with strong theoretical bases. In\\nparticular, programs can be modeled in the framework of lambda-calculi, and\\ninteresting properties can be expressed formally by contextual equivalence\\n(a.k.a. observational equivalence). Furthermore, imperative features, which\\nexist in most real-life software, can be nicely expressed in the so-called\\ncomputational lambda-calculus. Contextual equivalence is difficult to prove\\ndirectly, but we can often use logical relations as a tool to establish it in\\nlambda-calculi. We have already defined logical relations for the computational\\nlambda-calculus in previous work. We devote this paper to the study of their\\ncompleteness w.r.t. contextual equivalence in the computational\\nlambda-calculus.', 'Cloud computing has permeated into the information technology industry in the\\nlast few years, and it is emerging nowadays in scientific environments. Science\\nuser communities are demanding a broad range of computing power to satisfy the\\nneeds of high-performance applications, such as local clusters,\\nhigh-performance computing systems, and computing grids. Different workloads\\nare needed from different computational models, and the cloud is already\\nconsidered as a promising paradigm. The scheduling and allocation of resources\\nis always a challenging matter in any form of computation and clouds are not an\\nexception. Science applications have unique features that differentiate their\\nworkloads, hence, their requirements have to be taken into consideration to be\\nfulfilled when building a Science Cloud. This paper will discuss what are the\\nmain scheduling and resource allocation challenges for any Infrastructure as a\\nService provider supporting scientific applications.', 'Over the last decade, the term spatial computing has grown to have two\\ndifferent, though not entirely unrelated, definitions. The first definition of\\nspatial computing stems from industry, where it refers primarily to new kinds\\nof augmented, virtual, mixed-reality, and natural user interface technologies.\\nA second definition coming out of academia takes a broader perspective that\\nincludes active research in geographic information science as well as the\\naforementioned novel UI technologies. Both senses reflect an ongoing shift\\ntoward increased interaction with computing interfaces and sensors embedded in\\nthe environment and how the use of these technologies influence how we behave\\nand make sense of and even change the world we live in. Regardless of the\\ndefinition, research in spatial computing is humming along nicely without the\\nneed to identify new research agendas or new labels for communities of\\nresearchers. However, as a field of research, it could be helpful to view\\nspatial data science as the glue that coheres spatial computing with\\nproblem-solving and learning in the real world into a more holistic discipline.', 'Nowadays, computer science (CS) has emerged as a dominant force in numerous\\nresearch areas both within and beyond its own discipline. However, despite its\\nsignificant impact on scholarly space, only a limited number of studies have\\nbeen conducted to analyze the research trends and relationships within computer\\nscience. In this study, we collected information on fields and subfields from\\nover 2,000 research articles published in the 2022 proceedings of the top\\nAssociation for Computing Machinery (ACM) conferences spanning various research\\nfields. Through a network approach, we investigated the interconnections\\nbetween CS fields and subfields to evaluate their interdisciplinarity and\\nmultidisciplinarity. Our findings indicate that computing methodologies and\\nprivacy and security stand out as the most interdisciplinary fields, while\\nhuman-centered computing exhibits the highest frequency among the papers.\\nFurthermore, we discovered that machine learning emerges as the most\\ninterdisciplinary and multidisciplinary subfield within computer science. These\\nresults offer valuable insights for universities seeking to foster\\ninterdisciplinary research opportunities for their students.', 'What is computable with limited resources? How can we verify the correctness\\nof computations? How to measure computational power with precision? Despite the\\nimmense scientific and engineering progress in computing, we still have only\\npartial answers to these questions. In order to make these problems more\\nprecise, we describe an abstract algebraic definition of classical computation,\\ngeneralizing traditional models to semigroups. The mathematical abstraction\\nalso allows the investigation of different computing paradigms (e.g. cellular\\nautomata, reversible computing) in the same framework. Here we summarize the\\nmain questions and recent results of the research of finite computation.', 'The proliferation of vast quantities of available datasets that are large and\\ncomplex in nature has challenged universities to keep up with the demand for\\ngraduates trained in both the statistical and the computational set of skills\\nrequired to effectively plan, acquire, manage, analyze, and communicate the\\nfindings of such data. To keep up with this demand, attracting students early\\non to data science as well as providing them a solid foray into the field\\nbecomes increasingly important. We present a case study of an introductory\\nundergraduate course in data science that is designed to address these needs.\\nOffered at Duke University, this course has no pre-requisites and serves a wide\\naudience of aspiring statistics and data science majors as well as humanities,\\nsocial sciences, and natural sciences students. We discuss the unique set of\\nchallenges posed by offering such a course and in light of these challenges, we\\npresent a detailed discussion into the pedagogical design elements, content,\\nstructure, computational infrastructure, and the assessment methodology of the\\ncourse. We also offer a repository containing all teaching materials that are\\nopen-source, along with supplemental materials and the R code for reproducing\\nthe figures found in the paper.', 'Computational aspects increasingly shape environmental sciences. Actually,\\ntransdisciplinary modelling of complex and uncertain environmental systems is\\nchallenging computational science (CS) and also the science-policy interface.\\nLarge spatial-scale problems falling within this category - i.e. wide-scale\\ntransdisciplinary modelling for environment (WSTMe) - often deal with factors\\n(a) for which deep-uncertainty may prevent usual statistical analysis of\\nmodelled quantities and need different ways for providing policy-making with\\nscience-based support. Here, practical recommendations are proposed for\\ntempering a peculiar - not infrequently underestimated - source of uncertainty.\\nSoftware errors in complex WSTMe may subtly affect the outcomes with possible\\nconsequences even on collective environmental decision-making. Semantic\\ntransparency in CS and free software are discussed as possible mitigations.', 'As one of the main subjects of investigation in data science, network science\\nhas been demonstrated a wide range of applications to real-world networks\\nanalysis and modeling. For example, the pervasive presence of structural or\\ntopological characteristics, such as the small-world phenomenon,\\nsmall-diameter, scale-free properties, or fat-tailed degree distribution were\\none of the underlying pillars fostering the study of complex networks. Relating\\nthese phenomena with other emergent properties in complex systems became a\\nsubject of central importance. By introducing new implications on the interface\\nbetween data science and complex systems science with the purpose of tackling\\nsome of these issues, in this article we present a model for a network game\\nplayed by complex networks in which nodes are computable systems. In\\nparticular, we present and discuss how some network topological properties and\\nsimple local communication rules are able to generate a phase transition with\\nrespect to the emergence of incompressible data.', 'Analysing patterns of engagement among citizen science participants can\\nprovide important insights into the organisation and practice of individual\\ncitizen science projects. In particular, methods from statistics and network\\nscience can be used to understand different types of user behaviour and user\\ninteractions to help the further implementation and organization of community\\nefforts. Using publicly available data from the iNaturalist community and their\\nyearly City Nature Challenges (CNC) from 2017-2020 as an example; we showcase\\ncomputational methods to explore the spatio-temporal evolution of this citizen\\nscience community that typically interacts in a hybrid offline-online way. In\\nparticular, we investigate the user types present in the community along with\\ntheir interactions, finding significant differences in usage-behavior on both\\nthe level of engagement and the types of community tasks/roles and how they\\ninteract with the network of contributors. We expect that these computational\\nanalysis strategies will be useful to gain further understanding of other\\ncitizen science communities and projects.', \"Parallel computation is widely employed in scientific researches, engineering\\nactivities and product development. Parallel program writing itself is not\\nalways a simple task depending on problems solved. Large-scale scientific\\ncomputing, huge data analyses and precise visualizations, for example, would\\nrequire parallel computations, and the parallel computing needs the\\nparallelization techniques. In this Chapter a parallel program generation\\nsupport is discussed, and a computer-assisted parallel program generation\\nsystem P-NCAS is introduced. Computer assisted problem solving is one of key\\nmethods to promote innovations in science and engineering, and contributes to\\nenrich our society and our life toward a programming-free environment in\\ncomputing science. Problem solving environments (PSE) research activities had\\nstarted to enhance the programming power in 1970's. The P-NCAS is one of the\\nPSEs; The PSE concept provides an integrated human-friendly computational\\nsoftware and hardware system to solve a target class of problems\", 'Scalable board games, including Five in a Row (or gomoku) and weiqi (or go),\\nare generalized so that they can be played on or by quantum computers. We adopt\\nthree principles for the generalization: the first two are to ensure that the\\ngames are compatible with quantum computer and the third is to ensure that the\\nstandard classical games are the special cases. We demonstrate how to construct\\nbasic quantum moves and use them to set up quantum games. There are three\\ndifferent schemes to play the quantized games: one quantum computer with\\nanother quantum computer (QwQ), two classical computer playing with each other\\non one quantum computer (CQC), and one classical computer with another\\nclassical computer(CwC). We illustrate these results with the games of Five in\\na Row and weiqi.', 'As multicore systems continue to gain ground in the High Performance\\nComputing world, linear algebra algorithms have to be reformulated or new\\nalgorithms have to be developed in order to take advantage of the architectural\\nfeatures on these new processors. Fine grain parallelism becomes a major\\nrequirement and introduces the necessity of loose synchronization in the\\nparallel execution of an operation. This paper presents an algorithm for the QR\\nfactorization where the operations can be represented as a sequence of small\\ntasks that operate on square blocks of data. These tasks can be dynamically\\nscheduled for execution based on the dependencies among them and on the\\navailability of computational resources. This may result in an out of order\\nexecution of the tasks which will completely hide the presence of intrinsically\\nsequential tasks in the factorization. Performance comparisons are presented\\nwith the LAPACK algorithm for QR factorization where parallelism can only be\\nexploited at the level of the BLAS operations.', 'A re-calibration is proposed for \"numerical analysis\" as it arises\\nspecifically within the broader, embracing field of modern computer science\\n(CS). This would facilitate research into theoretical and practicable models of\\nreal-number computation at the foundations of CS, and it would also advance the\\ninstructional objectives of the CS field. Our approach is premised on the key\\nobservation that the great \"watershed\" in numerical computation is much more\\nbetween finite- and infinite-dimensional numerical problems than it is between\\ndiscrete and continuous numerical problems. A revitalized discipline for\\nnumerical computation within modern CS can more accurately be defined as\\n\"numerical algorithmic science & engineering (NAS&E), or more compactly, as\\n\"numerical algorithmics,\" its focus being the algorithmic solution of numerical\\nproblems that are either discrete, or continuous over a space of finite\\ndimension, or a combination of the two. It is the counterpart within modern CS\\nof the numerical analysis discipline, whose primary focus is the algorithmic\\nsolution of continuous, infinite-dimensional numerical problems and their\\nfinite-dimensional approximates, and whose specialists today have largely been\\nrepatriated to departments of mathematics. Our detailed overview of NAS&E from\\nthe viewpoints of rationale, foundations, and organization is preceded by a\\nrecounting of the role played by numerical analysts in the evolution of\\nacademic departments of computer science, in order to provide background for\\nNAS&E and place the newly-emerging discipline within its larger historical\\ncontext.', \"The next decade will be an exciting time for computational physicists. After\\n50 years of being forced to use standardized commercial equipment, it will\\nfinally become relatively straightforward to adapt one's computing tools to\\none's own needs. The breakthrough that opens this new era is the now\\nwide-spread availability of programmable chips that allow virtually every\\ncomputational scientist to design his or her own special-purpose computer.\", \"De Vrijer has presented a proof of the finite developments theorem which, in\\naddition to showing that all developments are finite, gives an effective\\nreduction strategy computing longest developments as well as a simple formula\\ncomputing their length.\\n  We show that by applying a rather simple and intuitive principle of duality\\nto de Vrijer's approach one arrives at a proof that some developments are\\nfinite which in addition yields an effective reduction strategy computing\\nshortest developments as well as a simple formula computing their length. The\\nduality fails for general beta-reduction.\\n  Our results simplify previous work by Khasidashvili.\", 'This volume of the Electronic Proceedings in Theoretical Computer Science\\n(EPTCS) contains extended abstracts of talks to be presented at the Seventh\\nInternational Conference on Computability and Complexity in Analysis (CCA 2010)\\nthat will take place in Zhenjiang, China, June 21-25, 2010. This conference is\\nthe seventeenth event in the series of CCA annual meetings. The CCA conferences\\nare aimed at promoting the study and advancement of the theory of computability\\nand complexity over real-valued data and its application.', 'In order to find out the limiting speed of solving a specific problem using\\ncomputer, this essay provides a method based on information entropy. The\\nrelationship between the minimum computational complexity and information\\nentropy change is illustrated. A few examples are served as evidence of such\\nconnection. Meanwhile some basic rules of modeling problems are established.\\nFinally, the nature of solving problems with computer programs is disclosed to\\nsupport this theory and a redefinition of information entropy in this filed is\\nproposed. This will develop a new field of science.', \"Scientific computation is a discipline that combines numerical analysis,\\nphysical understanding, algorithm development, and structured programming.\\nSeveral yottacycles per year on the world's largest computers are spent\\nsimulating problems as diverse as weather prediction, the properties of\\nmaterial composites, the behavior of biomolecules in solution, and the quantum\\nnature of chemical compounds. This article is intended to review specfic\\nlanguages features and their use in computational science. We will review the\\nstrengths and weaknesses of different programming styles, with examples taken\\nfrom widely used scientific codes.\", \"As a part of our works on effective properties of probability distributions,\\nwe deal with the corresponding characteristic functions. A sequence of\\nprobability distributions is computable if and only if the corresponding\\nsequence of characteristic functions is computable. As for the onvergence\\nproblem, the effectivized Glivenko's theorem holds. Effectivizations of\\nBochner's theorem and de Moivre-Laplace central limit theorem are also proved.\", 'In this paper I discuss what, according to my long experience, every computer\\nscientist should know from logic. We concentrate on issues of modeling,\\ninterpretability and levels of abstraction. We discuss what the minimal toolbox\\nof logic tools should look like for a computer scientist who is involved in\\ndesigning and analyzing reliable systems. We shall conclude that many classical\\ntopics dear to logicians are less important than usually presented, and that\\nless-known ideas from logic may be more useful for the working computer\\nscientist.', 'Classify simple games into sixteen \"types\" in terms of the four conventional\\naxioms: monotonicity, properness, strongness, and nonweakness. Further classify\\nthem into sixty-four classes in terms of finiteness (existence of a finite\\ncarrier) and algorithmic computability. For each such class, we either show\\nthat it is empty or give an example of a game belonging to it. We observe that\\nif a type contains an infinite game, then it contains both computable ones and\\nnoncomputable ones. This strongly suggests that computability is logically, as\\nwell as conceptually, unrelated to the conventional axioms.', \"We extend categorical semantics of monadic programming to reversible\\ncomputing, by considering monoidal closed dagger categories: the dagger gives\\nreversibility, whereas closure gives higher-order expressivity. We demonstrate\\nthat Frobenius monads model the appropriate notion of coherence between the\\ndagger and closure by reinforcing Cayley's theorem; by proving that effectful\\ncomputations (Kleisli morphisms) are reversible precisely when the monad is\\nFrobenius; by characterizing the largest reversible subcategory of\\nEilenberg-Moore algebras; and by identifying the latter algebras as\\nmeasurements in our leading example of quantum computing. Strong Frobenius\\nmonads are characterized internally by Frobenius monoids.\", 'In this paper, we examine the state art of quantum computing and analyze its\\npotential effects in scientific computing and cybersecurity. Additionally, a\\nnon-technical description of the mechanics of the listed form of computing is\\nprovided to educate the reader for better understanding of the arguments\\nprovided. The purpose of this study is not only to increase awareness in this\\nnescient technology, but also serve as a general reference guide for any\\nindividual wishing to study other applications of quantum computing in areas\\nthat include finance, chemistry, and data science. Lastly, an educated argument\\nis provided in the discussion section that addresses the implications this form\\nof computing will have in the main areas examined.', 'The world faces two interlinked crises: climate change and loss of\\nbiodiversity. Forest restoration on degraded lands and surplus croplands can\\nplay a significant role both in sequestering carbon and re-establishing\\nbio-diversity. There is a considerable body of research and practice that\\naddresses forest restoration. However, there has been little work by computer\\nscientists to bring powerful computational techniques to bear on this important\\narea of work, perhaps due to a lack of awareness. In an attempt to bridge this\\ngap, we present our vision of how techniques from computer science, broadly\\nspeaking, can aid current practice in forest restoration.', 'Science communication forms the bridge between computer science researchers\\nand their target audience. Researchers who can effectively draw attention to\\ntheir research findings and communicate them comprehensibly not only help their\\ntarget audience to actually learn something, but also benefit themselves from\\nthe increased visibility of their work and person. However, the necessary\\nskills for good science communication must also be taught, and this has so far\\nbeen neglected in the field of software engineering education.\\n  We therefore designed and implemented a science communication seminar for\\nbachelor students of computer science curricula. Students take the position of\\na researcher who, shortly after publication, is faced with having to draw\\nattention to the paper and effectively communicate the contents of the paper to\\none or more target audiences. Based on this scenario, each student develops a\\ncommunication strategy for an already published software engineering research\\npaper and tests the resulting ideas with the other seminar participants.\\n  We explain our design decisions for the seminar, and combine our experiences\\nwith responses to a participant survey into lessons learned. With this\\nexperience report, we intend to motivate and enable other lecturers to offer a\\nsimilar seminar at their university. Collectively, university lecturers can\\nprepare the next generation of computer science researchers to not only be\\nexperts in their field, but also to communicate research findings more\\neffectively.', 'Relative to digital computation, analog computation has been neglected in the\\nphilosophical literature. To the extent that attention has been paid to analog\\ncomputation, it has been misunderstood. The received view -- that analog\\ncomputation has to do essentially with continuity -- is simply wrong, as shown\\nby careful attention to historical examples of discontinuous, discrete analog\\ncomputers. Instead of the received view, I develop an account of analog\\ncomputation in terms of a particular type of analog representation that allows\\nfor discontinuity. This account thus characterizes all types of analog\\ncomputation, whether continuous or discrete. Furthermore, the structure of this\\naccount can be generalized to other types of computation: analog computation\\nessentially involves analog representation, whereas digital computation\\nessentially involves digital representation. Besides being a necessary\\ncomponent of a complete philosophical understanding of computation in general,\\nunderstanding analog computation is important for computational explanation in\\ncontemporary neuroscience and cognitive science.', \"Modern data science research can involve massive computational\\nexperimentation; an ambitious PhD in computational fields may do experiments\\nconsuming several million CPU hours. Traditional computing practices, in which\\nresearchers use laptops or shared campus-resident resources, are inadequate for\\nexperiments at the massive scale and varied scope that we now see in data\\nscience. On the other hand, modern cloud computing promises seemingly unlimited\\ncomputational resources that can be custom configured, and seems to offer a\\npowerful new venue for ambitious data-driven science. Exploiting the cloud\\nfully, the amount of work that could be completed in a fixed amount of time can\\nexpand by several orders of magnitude.\\n  As potentially powerful as cloud-based experimentation may be in the\\nabstract, it has not yet become a standard option for researchers in many\\nacademic disciplines. The prospect of actually conducting massive computational\\nexperiments in today's cloud systems confronts the potential user with daunting\\nchallenges. Leading considerations include: (i) the seeming complexity of\\ntoday's cloud computing interface, (ii) the difficulty of executing an\\noverwhelmingly large number of jobs, and (iii) the difficulty of monitoring and\\ncombining a massive collection of separate results. Starting a massive\\nexperiment `bare-handed' seems therefore highly problematic and prone to rapid\\n`researcher burn out'.\\n  New software stacks are emerging that render massive cloud experiments\\nrelatively painless. Such stacks simplify experimentation by systematizing\\nexperiment definition, automating distribution and management of tasks, and\\nallowing easy harvesting of results and documentation. In this article, we\\ndiscuss several painless computing stacks that abstract away the difficulties\\nof massive experimentation, thereby allowing a proliferation of ambitious\\nexperiments for scientific discovery.\", 'Computational science is changing to be data intensive. Super-Computers must\\nbe balanced systems; not just CPU farms but also petascale IO and networking\\narrays. Anyone building CyberInfrastructure should allocate resources to\\nsupport a balanced Tier-1 through Tier-3 design.', 'Dusty plasmas play an important role in astrophysical environments such as\\nprotostellar clouds and ring systems. A computer simulation of such plasmas\\nwill be presented.', \"Review of the First International Conference on Unconventional Models of\\nComputation UMC'98, Auckland, New Zealand, 5-9 January, 1998\", 'The classical logical antinomy known as Richard-Berry paradox is combined\\nwith plausible assumptions about the size i.e. the descriptional complexity of\\nTuring machines formalizing certain sentences, to show that formalization of\\nlanguage leads to contradiction.', 'We introduce a new, substantially simplified version of the\\ntoggling-branching recurrence operation of Computability Logic, prove its\\nequivalence to Japaridze\\'s old, \"canonical\" version, and also prove that both\\nversions preserve the static property of their arguments.', 'We present an algorithm which computes the Landau constant up to any given\\nprecision.', 'We propose to study maximum flow problems for connectome graphs. We suggest a\\nfew computational problems: finding vertex pairs with maximal flow, finding new\\nedges which would increase the maximal flow. Initial computation results for\\nsome publicly available connectome graphs are described.', 'This article presents a survey of computability logic: its philosophy and\\nmotivations, main concepts and most significant results obtained so far. A\\ncontinuously updated online version of this article is maintained at\\nhttp://www.csc.villanova.edu/~japaridz/CL/ .', 'We provide an algorithm for computing the nucleolus for an instance of a\\nweighted voting game in pseudo-polynomial time. This resolves an open question\\nposed by Elkind. et.al. 2007.', 'We define a generalization of the Turing machine that computes on general\\nsets. Our main theorem states that the class of generalized Turing machine\\ncomputable functions and the class of Set Recursive functions coincide.', 'For the first time, the repeated wear phenomenon of high-frequency power\\nfailure on the data block area in intermittent computing file system is found.\\nA method to improve NVM wear in ICFS under high-frequency power failure\\nscenarios is proposed.', 'We compare the values associated with (traditional) community based proof\\nverification to those associated with computer proof verification. We propose\\nways that computer proofs might incorporate successful strategies from human\\nexperiences.', 'We give a quick survey of the various fixed point theorems in computability\\ntheory, partial combinatory algebra, and the theory of numberings, as well as\\ngeneralizations based on those. We also point out several open problems\\nconnected to these.', \"This is a critical review of the book 'A New Kind of Science' by Stephen\\nWolfram. We do not attempt a chapter-by-chapter evaluation, but instead focus\\non two areas: computational complexity and fundamental physics. In complexity,\\nwe address some of the questions Wolfram raises using standard techniques in\\ntheoretical computer science. In physics, we examine Wolfram's proposal for a\\ndeterministic model underlying quantum mechanics, with 'long-range threads' to\\nconnect entangled particles. We show that this proposal cannot be made\\ncompatible with both special relativity and Bell inequality violation.\", 'We give an explicit coinduction principle for recursively-defined stochastic\\nprocesses. The principle applies to any closed property, not just equality, and\\nworks even when solutions are not unique. The rule encapsulates low-level\\nanalytic arguments, allowing reasoning about such processes at a higher\\nalgebraic level. We illustrate the use of the rule in deriving properties of a\\nsimple coin-flip process.', 'We introduce a calculus for tuplices, which are expressions that generalize\\nmatrices and vectors. Tuplices have an underlying data type for quantities that\\nare taken from a zero-totalized field. We start with the core tuplix calculus\\nCTC for entries and tests, which are combined using conjunctive composition. We\\ndefine a standard model and prove that CTC is relatively complete with respect\\nto it. The core calculus is extended with operators for choice, information\\nhiding, scalar multiplication, clearing and encapsulation. We provide two\\nexamples of applications; one on incremental financial budgeting, and one on\\nmodular financial budget design.', 'We present a type theory with some proof-irrelevance built into the\\nconversion rule. We argue that this feature is useful when type theory is used\\nas the logical formalism underlying a theorem prover. We also show a close\\nrelation with the subset types of the theory of PVS. We show that in these\\ntheories, because of the additional extentionality, the axiom of choice implies\\nthe decidability of equality, that is, almost classical logic. Finally we\\ndescribe a simple set-theoretic semantics.', 'A foundation for closing the gap between biometrics in the narrower and the\\nbroader perspective is presented trough a conceptualization of biometric\\nsystems in both perspectives. A clear distinction between verification,\\nidentification and classification systems is made as well as shown that there\\nare additional classes of biometric systems. In the end a Unified Modeling\\nLanguage model is developed showing the connections between the two\\nperspectives.', 'This paper is concerned with the status of 1/0 and ways to deal with it.\\nThese matters are treated in the setting of Komori fields, also known as\\nnon-trivial cancellation meadows. Different viewpoints on the status of 1/0\\nexist in mathematics and theoretical computer science. We give a simple account\\nof how mathematicians deal with 1/0 in which a customary convention among\\nmathematicians plays a prominent part, and we make plausible that a convincing\\naccount, starting from the popular computer science viewpoint that 1/0 is\\nundefined, by means of some logic of partial functions is not attainable.', 'The researchers have drawn much attention about the birth weight of newborn\\nbabies in the last three decades. The birth weight is one of the vital roles in\\nthe babys health. So many researchers such as (2),(1) and (4) analyzed the\\nbirth weight of babies. The aim of this paper is to analyze the birth weight\\nand some other birth weight related variable, using singular value\\ndecomposition and multiple linear regression.', 'The capability to provide network service even under a significant network\\nsystem element disruption is the backbone for the survival of route optimize of\\nmobile network Technology in today s world. Keeping this view in mind, the\\npresent paper highlights a new method based on memetic algorithm.', 'The direct measurement of quality is difficult because there is no way we can\\nmeasure quality factors. For measuring these factors, we have to express them\\nin terms of metrics or models. Researchers have developed quality models that\\nattempt to measure quality in terms of attributes, characteristics and metrics.\\nIn this work we have proposed the methodology of controlled experimentation\\ncoupled with power of Logical Scoring of Preferences to evaluate global quality\\nof four object-oriented designs.', 'Crosstalk in VLSI interconnects is a major constrain in DSM and UDSM\\ntechnology. Among various strategies followed for its minimization, shield\\ninsertion between Aggressor and Victim is one of the prominent options. This\\npaper analyzes the extent of crosstalk in inductively coupled interconnects and\\nminimizes the same through distributed shield insertion. Comparison is drawn\\nbetween signal voltage and crosstalk voltage in three different conditions i.e.\\nprior to shield insertion, after shield insertion and after additional ground\\ntap insertion at shield terminal.', 'Designing and developing quality based computer game is always a challenging\\ntask for developers. In this paper I briefly discuss aero fighting war game\\nbased on simple 2D gaming concepts and developed in C & C++ programming\\nlanguages, using old bitmapping concepts. Going into the details of the game\\ndevelopment, I discuss the designed strategies, flow of game and implemented\\nprototype version of game, especially for beginners of game programming.', 'Process behaviour is often defined either in terms of the tests they satisfy,\\nor in terms of the logical properties they enjoy. Here we compare these two\\napproaches, using extensional testing in the style of DeNicola, Hennessy, and a\\nrecursive version of the property logic HML. We first characterise subsets of\\nthis property logic which can be captured by tests. Then we show that those\\nsubsets of the property logic capture precisely the power of tests.', 'Escalation is the fact that in a game (for instance an auction), the agents\\nplay forever. It is not necessary to consider complex examples to establish its\\nrationality. In particular, the $0,1$-game is an extremely simple infinite game\\nin which escalation arises naturally and rationally. In some sense, it can be\\nconsidered as the paradigm of escalation. Through an example of economic games,\\nwe show the benefit economics can take of coinduction.', 'A translation from Russian of the work of R.R. Kamalian \"Interval colorings\\nof complete bipartite graphs and trees\", Preprint of the Computing Centre of\\nthe Academy of Sciences of Armenia, Yerevan, 1989. (Was published by the\\ndecision of the Academic Council of the Computing Centre of the Academy of\\nSciences of Armenian SSR and Yerevan State University from 7.09.1989).', 'Steganography derives from the Greek word steganos, meaning covered or\\nsecret, and graphy (writing or drawing). Steganography is a technology where\\nmodern data compression, information theory, spread spectrum, and cryptography\\ntechnologies are brought together to satisfy the need for privacy on the\\nInternet. This paper is an attempt to analyse the various techniques used in\\nsteganography and to identify areas in which this technique can be applied, so\\nthat the human race can be benefited at large.', 'Similar to a tree grammar, a Horn theory can be used to describe an infinite\\nset of terms. In this paper, we present a class of Horn theories such that the\\nset of definable predicates is closed wrt. conjunction and such that the\\nsatisfiability of a predicate is decidable. This extends previous results on\\nHorn clauses with unary predicates.', 'We consider the individual points on a Martin-L\\\\\"of random path of Brownian\\nmotion. We show (1) that Khintchine\\'s law of the iterated logarithm holds at\\nalmost all points; and (2) there exist points (besides the trivial example of\\nthe origin) having effective dimension $<1$. The proof of (1) shows that for\\nalmost all times $t$, the path $f$ is Martin-L\\\\\"of random relative to $t$ and\\nso the effective dimension of $(t,f(t))$ is 2.', \"This tutorial deal with the Axiom of Choice and some of its applications to\\ntopics related to Computer Science. We will see that the Axiom of Choice is\\nequivalent to some well-known proof principles like Zorn's Lemma or Tuckey's\\nMaximality Principle. We try to touch upon some topics, which appear to be\\nimportant for developing mathematical structures within computer science.\", 'This paper explores the use of 2-categorical technology for describing and\\nreasoning about complex quantum procedures. We give syntactic definitions of a\\nfamily of complementary measurements, and of quantum key distribution, and show\\nthat they are equivalent. We then show abstractly that either structure gives a\\nsolution to the Mean King problem, which we also formulate 2-categorically.', \"We give a new order-theoretic characterization of a complete Heyting and\\nco-Heyting algebra $C$. This result provides an unexpected relationship with\\nthe field of Nash equilibria, being based on the so-called Veinott ordering\\nrelation on subcomplete sublattices of $C$, which is crucially used in Topkis'\\ntheorem for studying the order-theoretic stucture of Nash equilibria of\\nsupermodular games.\", \"A Petri net is structurally cyclic if every configuration is reachable from\\nitself in one or more steps. We show that structural cyclicity is decidable in\\ndeterministic polynomial time. For this, we adapt the Kosaraju's approach for\\nthe general reachability problem for Petri nets.\", 'In our previous work we reported on a linked-courses learning community for\\nunderrepresented groups in computer science, finding differences in attitudes\\nand resource utilization between students in the community and other\\nprogramming students. Here we present the first statistically significant\\ndifferences in pre- to post-quarter student attitudes between those in the\\nlearning community and others taking equivalent programming classes. We find\\nthat students in the learning community are less likely to feel isolated\\npost-quarter than other programming students. We also present results showing\\ndifferences in resource utilization by learning-community participants.', \"Topic models are a family of statistical-based algorithms to summarize,\\nexplore and index large collections of text documents. After a decade of\\nresearch led by computer scientists, topic models have spread to social science\\nas a new generation of data-driven social scientists have searched for tools to\\nexplore large collections of unstructured text. Recently, social scientists\\nhave contributed to topic model literature with developments in causal\\ninference and tools for handling the problem of multi-modality. In this paper,\\nI provide a literature review on the evolution of topic modeling including\\nextensions for document covariates, methods for evaluation and interpretation,\\nand advances in interactive visualizations along with each aspect's relevance\\nand application for social science research.\", 'Model checking is an automatic verification technique to verify hardware and\\nsoftware systems. However it suffers from state-space explosion problem. In\\nthis paper we address this problem in the context of cryptographic protocols by\\nproposing a security property-dependent heuristic. The heuristic weights the\\nstate space by exploiting the security formulae; the weights may then be used\\nto explore the state space when searching for attacks.', 'We study a natural hierarchy in first-order logic, namely the quantifier\\nstructure hierarchy, which gives a systematic classification of first-order\\nformulas based on structural quantifier resource. We define a variant of\\nEhrenfeucht-Fraisse games that characterizes quantifier classes and use it to\\nprove that this hierarchy is strict over finite structures, using strategy\\ncompositions. Moreover, we prove that this hierarchy is strict even over\\nordered finite structures, which is interesting in the context of descriptive\\ncomplexity.', 'Many have argued that statistics students need additional facility to express\\nstatistical computations. By introducing students to commonplace tools for data\\nmanagement, visualization, and reproducible analysis in data science and\\napplying these to real-world scenarios, we prepare them to think statistically.\\nIn an era of increasingly big data, it is imperative that students develop\\ndata-related capacities, beginning with the introductory course. We believe\\nthat the integration of these precursors to data science into our\\ncurricula-early and often-will help statisticians be part of the dialogue\\nregarding \"Big Data\" and \"Big Questions\".', 'We present a new type system combining refinement types and the\\nexpressiveness of intersection type discipline. The use of such features makes\\nit possible to derive more precise types than in the original refinement\\nsystem. We have been able to prove several interesting properties for our\\nsystem (including subject reduction) and developed an inference algorithm,\\nwhich we proved to be sound.', 'Artificial intelligence (AI) like deep learning, cloud AI computation has\\nbeen advancing at a rapid pace since 2014. There is no doubt that the\\nprosperity of AI is inseparable with the development of the Internet. However,\\nthere has been little attention to the link between AI and the internet. This\\npaper explores them with brain insights mainly from four views:1) How is the\\ngeneral relation between artificial intelligence and Internet of Things, cloud\\ncomputing, big data and Industrial Internet from the perspective of brain\\nscience. 2) Construction of a new AI system model with the Internet and brain\\nscience.', 'We study the multiagent epistemic logic CMAELCD with operators for common and\\ndistributed knowledge for all coalitions of agents. We introduce Hintikka\\nstructures for this logic and prove that satisfiability in such structures is\\nequivalent to satisfiability in standard models. Using this result, we design\\nan incremental tableau based decision procedure for testing satisfiability in\\nCMAELCD.', 'Consider a linear ordering equipped with a finite sequence of monadic\\npredicates. If the ordering contains an interval of order type \\\\omega or\\n-\\\\omega, and the monadic second-order theory of the combined structure is\\ndecidable, there exists a non-trivial expansion by a further monadic predicate\\nthat is still decidable.', 'The development of computer science has contributed greatly for increasing of\\nefficiency and effectively. Many areas are covered by computer science,\\nincluded education. The purpose of this research is to introduce jawi a type of\\nIndonesian letters. Jawis letter is one of the most popular letter in the past.\\nBut right now few people can read and understand it. Many documents in the past\\nwas written in Jawi. The writer develop or build the software using Pressman\\nmethod, and tools such as Microsoft Visual Basic, and Microsoft Access. This\\nsoftware can introduce Jawi then people can learn it easily.', 'There are several centrality measures that have been introduced and studied\\nfor real world networks. They account for the different vertex characteristics\\nthat permit them to be ranked in order of importance in the network.\\nBetweenness centrality is a measure of the influence of a vertex over the flow\\nof information between every pair of vertices under the assumption that\\ninformation primarily flows over the shortest path between them. In this paper\\nwe present betweenness centrality of some important classes of graphs.', 'How can elementary grade teachers integrate programming and computational\\nthinking with the science curriculum? To answer this question, we present\\nresults from a long-term, design-based, microgenetic study where 1) agent-based\\nprogramming using ViMAP was integrated with existing elementary science\\ncurricula and 2) lessons were taught by the classroom teacher. We present an\\ninvestigation of the co-development of children\\'s computational thinking and\\nscientific modeling and show that the integration of programming with\\nscientific modeling can be supported by the development of sociomathematical\\nnorms for designing \"mathematically good\" computational models.', 'The increasing data availability and imported analyzing tools from computer\\nscience and physical science have sharply changed traditional methodologies of\\nsocial sciences, leading to a new branch named computational socioeconomics\\nthat studies various phenomena in socioeconomic development by using\\nquantitative methods based on large-scale real-world data. Sited on recent\\npublications, this Perspective will introduce three representative methods: (i)\\nnatural data analyses, (ii) large-scale online experiments, and (iii)\\nintegration of big data and surveys. This Perspective ends up with in-depth\\ndiscussion on the limitations and challenges of the above-mentioned emerging\\nmethods.', 'We present infinite extensive strategy profiles with perfect information and\\nwe show that replacing finite by infinite changes the notions and the reasoning\\ntools. The presentation uses a formalism recently developed by logicians and\\ncomputer science theoreticians, called coinduction. This builds a bridge\\nbetween economic game theory and the most recent advance in theoretical\\ncomputer science and logic. The key result is that rational agents may have\\nstrategy leading to divergence .', 'In this paper we introduce a notion of counterfactual causality in the\\nHalpern and Pearl sense that is compositional with respect to the interleaving\\nof transition systems. The formal framework for reasoning on what caused the\\nviolation of a safety property is established in the context of labeled\\ntransition systems and Hennessy Milner logic. The compositionality results are\\ndevised for non-communicating systems.', 'CSIndexbr is a web-based system that provides meaningful,open,and transparent\\ndata about Brazilian scientific production in Computer Science. Currently, the\\nsystem collects full research papers published in the main track of selected\\nconferences. The papers are retrieved from DBLP. In this article, we describe\\nthe main features and resources provided by CSIndexbr. We also comment on how\\nother researchers can use the data provided by the system to analyze the\\nBrazilian production in Computer Science.', 'Categorical Query Language is an open-source query and data integration\\nscripting language that can be applied to common challenges in the field of\\ncomputational science. We discuss how the structure-preserving nature of CQL\\ndata migrations protect those who publicly share data from the\\nmisinterpretation of their data. Likewise, this feature of CQL migrations\\nallows those who draw from public data sources to be sure only data which meets\\ntheir specification will actually be transferred. We argue some open problems\\nin the field of data sharing in computational science are addressable by\\nworking within this paradigm of functorial data migration. We demonstrate these\\ntools by integrating data from the Open Quantum Materials Database with some\\nalternative materials databases.', \"We identify and highlight certain landmark results in Samson Abramsky's work\\nwhich we believe are fundamental to current developments and future trends. In\\nparticular, we focus on the use of (i) topological duality methods to solve\\nproblems in logic and computer science; (ii) category theory and, more\\nparticularly, free (and co-free) constructions; (iii) these tools to unify the\\n`power' and `structure' strands in computer science.\", 'The proceedings of the 6th International Workshop on Symbolic-Numeric Methods\\nfor Reasoning about CPS and IoT (SNR 2020) contains papers underlying talks\\npresented at the workshop. SNR focuses on the combination of symbolic and\\nnumeric methods for reasoning about Cyber-Physical Systems and the Internet of\\nThings to facilitate model identification, specification, verification, and\\ncontrol synthesis for these systems.', 'The undergraduate data science curriculum at the University of California,\\nBerkeley is anchored in five new courses that emphasize computational thinking,\\ninferential thinking, and working on real-world problems. We believe that\\ninterleaving these elements within our core courses is essential to preparing\\nstudents to engage in data-driven inquiry at the scale that contemporary\\nscientific and industrial applications demand. This new curriculum is already\\nreshaping the undergraduate experience at Berkeley, where these courses have\\nbecome some of the most popular on campus and have led to a surging interest in\\na new undergraduate major and minor program in data science.', 'Symbolic execution uses various algorithms (matching, (anti)unification),\\nwhose executions are parameters for proof object generation. This paper\\nproposes a generic method for generating proof objects for such parameters. We\\npresent in detail how our method works for the case of antiunification. The\\napproach is accompanied by an implementation prototype, including a proof\\nobject generator and a proof object checker. In order to investigate the size\\nof the proof objects, we generate and check proof objects for inputs inspired\\nfrom the K definitions of C and Java.', 'This document is an elementary introduction to string diagrams. It takes a\\ncomputer science perspective: rather than using category theory as a starting\\npoint, we build on intuitions from formal language theory, treating string\\ndiagrams as a syntax with its semantics. After the basic theory, pointers are\\nprovided to contemporary applications of string diagrams in various fields of\\nscience.', 'Symmetric powers are an important notion in mathematics, computer science and\\nphysics. In mathematics, they are used to build symmetric algebras, in computer\\nscience, to build free exponential modalities of linear logic and in physics,\\nFock spaces. We study symmetric powers through the lens of category theory. We\\nfocus here on the simpler case where nonnegative rational scalars are available\\nie. we study symmetric powers in symmetric monoidal $\\\\mathbb{Q}_{\\\\ge 0}$-linear\\ncategories. Among the developments, a main point is the introduction of the\\nnotion of binomial graded bimonoid and the associated string diagrams which\\ncharacterize symmetric powers in this setting.', \"While coursework provides undergraduate data science students with some\\nrelevant analytic skills, many are not given the rich experiences with data and\\ncomputing they need to be successful in the workplace. Additionally, students\\noften have limited exposure to team-based data science and the principles and\\ntools of collaboration that are encountered outside of school. In this paper,\\nwe describe the DSC-WAV program, an NSF-funded data science workforce\\ndevelopment project in which teams of undergraduate sophomores and juniors work\\nwith a local non-profit organization on a data-focused problem. To help\\nstudents develop a sense of agency and improve confidence in their technical\\nand non-technical data science skills, the project promoted a team-based\\napproach to data science, adopting several processes and tools intended to\\nfacilitate this collaboration. Evidence from the project evaluation, including\\nparticipant survey and interview data, is presented to document the degree to\\nwhich the project was successful in engaging students in team-based data\\nscience, and how the project changed the students' perceptions of their\\ntechnical and non-technical skills. We also examine opportunities for\\nimprovement and offer insight to other data science educators who may want to\\nimplement a similar team-based approach to data science projects at their own\\ninstitutions.\", \"Africa has a high student-to-teacher ratio which limits students' access to\\nteachers. Consequently, students struggle to get answers to their questions. In\\nthis work, we extended Kwame, our previous AI teaching assistant, adapted it\\nfor science education, and deployed it as a web app. Kwame for Science answers\\nquestions of students based on the Integrated Science subject of the West\\nAfrican Senior Secondary Certificate Examination (WASSCE). Kwame for Science is\\na Sentence-BERT-based question-answering web app that displays 3 paragraphs as\\nanswers along with a confidence score in response to science questions.\\nAdditionally, it displays the top 5 related past exam questions and their\\nanswers in addition to the 3 paragraphs. Our preliminary evaluation of the\\nKwame for Science with a 2.5-week real-world deployment showed a top 3 accuracy\\nof 87.5% (n=56) with 190 users across 11 countries. Kwame for Science will\\nenable the delivery of scalable, cost-effective, and quality remote education\\nto millions of people across Africa.\", 'The history of computer science and brain sciences are intertwined. In his\\nunfinished manuscript \"The Computer and the Brain,\" von Neumann debates whether\\nor not the brain can be thought of as a computing machine and identifies some\\nof the similarities and differences between natural and artificial computation.\\nTuring, in his 1950 article in Mind, argues that computing devices could\\nultimately emulate intelligence, leading to his proposed Turing test. Herbert\\nSimon predicted in 1957 that most psychological theories would take the form of\\na computer program. In 1976, David Marr proposed that the function of the\\nvisual system could be abstracted and studied at computational and algorithmic\\nlevels that did not depend on the underlying physical substrate.\\n  In December 2014, a two-day workshop supported by the Computing Community\\nConsortium (CCC) and the National Science Foundation\\'s Computer and Information\\nScience and Engineering Directorate (NSF CISE) was convened in Washington, DC,\\nwith the goal of bringing together computer scientists and brain researchers to\\nexplore these new opportunities and connections, and develop a new, modern\\ndialogue between the two research communities. Specifically, our objectives\\nwere: 1. To articulate a conceptual framework for research at the interface of\\nbrain sciences and computing and to identify key problems in this interface,\\npresented in a way that will attract both CISE and brain researchers into this\\nspace. 2. To inform and excite researchers within the CISE research community\\nabout brain research opportunities and to identify and explain strategic roles\\nthey can play in advancing this initiative. 3. To develop new connections,\\nconversations and collaborations between brain sciences and CISE researchers\\nthat will lead to highly relevant and competitive proposals, high-impact\\nresearch, and influential publications.', 'We report first principles density functional calculations of the Born\\neffective charges and electronic dielectric tensors for the relaxor PMN\\n(PbMg1/3Nb2/3O3). Visualization of the Born charge tensors as charge ellipsoids\\nhave provided microscopic insights on the factors governing piezoelectric\\nenhancements with polarization rotation. Several 15 and 30-atom ferroelectric\\nand antiferroelectric supercells of PMN involving 1:2 and 1:1 chemical ordering\\nhave been studied. A cascading set of ferroelectric phonon instabilities lead\\nto several low symmetry monoclinic structures. We find a ground state with a\\n15-atom unit cell with 1:2 chemical ordering along [111] with a monoclinic C2\\nstructure.', 'We discuss here our vision for an Open-Science platform for computational\\nMaterials Science. Such a platform needs to rely on three pillars, consisting\\nof 1) open data generation tools (including the simulation codes, the\\nscientific workflows and the infrastructure for automation and\\nprovenance-tracking); 2) an open integration platform where these tools\\ninteract in an easily accessible way and computations are coordinated by\\nautomated workflows; and 3) support for seamless code and data sharing through\\nportals that are FAIR-compliant and compatible with data-management plans. As a\\npractical implementation, we show how such a platform in a few examples and\\nfocusing in particular on the combination of the AiiDA infrastructure and the\\nMaterials Cloud web portal.', 'In the field of Computer Science, conference and workshop papers serve as\\nimportant contributions, carrying substantial weight in research assessment\\nprocesses, compared to other disciplines. However, a considerable number of\\nthese papers are not assigned a Digital Object Identifier (DOI), hence their\\ncitations are not reported in widely used citation datasets like OpenCitations\\nand Crossref, raising limitations to citation analysis. While the Microsoft\\nAcademic Graph (MAG) previously addressed this issue by providing substantial\\ncoverage, its discontinuation has created a void in available data. BIP! NDR\\naims to alleviate this issue and enhance the research assessment processes\\nwithin the field of Computer Science. To accomplish this, it leverages a\\nworkflow that identifies and retrieves Open Science papers lacking DOIs from\\nthe DBLP Corpus, and by performing text analysis, it extracts citation\\ninformation directly from their full text. The current version of the dataset\\ncontains more than 510K citations made by approximately 60K open access\\nComputer Science conference or workshop papers that, according to DBLP, do not\\nhave a DOI.', 'Political science, and social science in general, have traditionally been\\nusing computational methods to study areas such as voting behavior, policy\\nmaking, international conflict, and international development. More recently,\\nincreasingly available quantities of data are being combined with improved\\nalgorithms and affordable computational resources to predict, learn, and\\ndiscover new insights from data that is large in volume and variety. New\\ndevelopments in the areas of machine learning, deep learning, natural language\\nprocessing (NLP), and, more generally, artificial intelligence (AI) are opening\\nup new opportunities for testing theories and evaluating the impact of\\ninterventions and programs in a more dynamic and effective way. Applications\\nusing large volumes of structured and unstructured data are becoming common in\\ngovernment and industry, and increasingly also in social science research. This\\nchapter offers an introduction to such methods drawing examples from political\\nscience. Focusing on the areas where the strengths of the methods coincide with\\nchallenges in these fields, the chapter first presents an introduction to AI\\nand its core technology - machine learning, with its rapidly developing\\nsubfield of deep learning. The discussion of deep neural networks is\\nillustrated with the NLP tasks that are relevant to political science. The\\nlatest advances in deep learning methods for NLP are also reviewed, together\\nwith their potential for improving information extraction and pattern\\nrecognition from political science texts.', 'Through the 1990s to 2012 the internet changed the world of computing\\ndrastically. It started its journey with parallel computing after it advanced\\nto distributed computing and further to grid computing. And in present scenario\\nit creates a new world which is pronounced as a Cloud Computing [1]. These all\\nthree terms have different meanings. Cloud computing is based on backward\\ncomputing schemes like cluster computing, distributed computing, grid computing\\nand utility computing. The basic concept of cloud computing is virtualization.\\nIt provides virtual hardware and software resources to various requesting\\nprograms. This paper gives a detailed description about cluster computing, grid\\ncomputing and cloud computing and gives an insight of some implementations of\\nthe same. We try to list the inspirations for the advent of all these\\ntechnologies. We also account for some present scenario faults of grid\\ncomputing and also discuss new cloud computing projects which are being managed\\nby the Government of India for learning. The paper also reviews the existing\\nwork and covers (analytically), to some extent, some innovative ideas that can\\nbe implemented.', 'Emerging network scenarios require the development of solid large-scale\\nsituated systems. Unfortunately, the diffusion/aggregation computational\\nprocesses therein often introduce a source of complexity that hampers\\npredictability of the overall system behaviour. Computational fields have been\\nintroduced to help engineering such systems: they are spatially distributed\\ndata structures designed to adapt their shape to the topology of the underlying\\n(mobile) network and to the events occurring in it, with notable applications\\nto pervasive computing, sensor networks, and mobile robots. To assure\\nbehavioural correctness, namely, correspondence of micro-level specification\\n(single device behaviour) with macro-level behaviour (resulting global spatial\\npattern), we investigate the issue of self-stabilisation for computational\\nfields. We present a tiny, expressive, and type-sound calculus of computational\\nfields, and define sufficient conditions for self-stabilisation, defined as the\\nability to react to changes in the environment finding a new stable state in\\nfinite time. A type-based approach is used to provide a correct checking\\nprocedure for self-stabilisation.', \"Computational thinking has been a recent focus of education research within\\nthe sciences. However, there is a dearth of scholarly literature on how best to\\nteach and to assess this topic, especially in disciplinary science courses.\\nPhysics classes with computation integrated into the curriculum are a fitting\\nsetting for investigating computational thinking. In this paper, we lay the\\nfoundation for exploring computational thinking in introductory physics\\ncourses. First, we review relevant literature to synthesize a set of potential\\nlearning goals that students could engage in when working with computation. The\\ncomputational thinking framework that we have developed features 14 practices\\ncontained within 6 different categories. We use in-class video data as\\nexistence proofs of the computational thinking practices proposed in our\\nframework. In doing this work, we hope to provide ways for teachers to assess\\ntheir students' development of computational thinking, while also giving\\nphysics education researchers some guidance on how to study this topic in\\ngreater depth.\", 'The abstraction introduced by von Neumann correctly reflected the state of\\nthe art 70 years ago.\\n  Although it omitted data transmission time between components of the\\ncomputer, it served as an excellent base for classic computing for decades.\\n  Modern computer components and architectures, however, require to consider\\ntheir temporal behavior: data transmission time in contemporary systems may be\\nhigher than their processing time.\\n  Using the classic paradigm leaves some issues unexplained, from enormously\\nhigh power consumption to days-long training of artificial neural networks to\\nfailures of some cutting-edge supercomputer projects.\\n  The paper introduces the up to now missing timely behavior (a temporal logic)\\ninto computing, while keeps the solid computing science base.\\n  The careful analysis discovers that with considering the timely behavior of\\ncomponents and architectural principles, the mystic issues have a trivial\\nexplanation.\\n  Some classic design principles must be revised, and the temporal logic\\nenables us to design a more powerful and efficient computing.', 'Smartphones as one of information technology products have been affected\\nhigher education in various aspects. This article explains the useness of\\nsmartphones in facilitating online examination in information systems and\\ncomputer science students. The research objective to be achieved by the\\nresearchers through the research, are as follows: 1) Utilizing smartphone as a\\nmedia test online exam for green computing environment, 2) How to use social\\ninformation technologies in online test, and 3) Explore the facilities or\\nfeatures that could be used for the online exam implementation. The observation\\nwas conducted with 100 students as respondents. Researcher used google forms to\\ndisseminate questions for online examination. The findings of the research\\nshowed that most the college students used Android OS for their online\\nexamination. Social technology like google forms has rich features in\\nsupporting online examination for computer science students. The use of\\nsmartphones, google forms, and facebook can create an atmosphere of modern,\\ngreen computer science exams, efficient, and environmentally friendly.', 'Distributed software is becoming more and more dynamic to support\\napplications able to respond and adapt to the changes of their execution\\nenvironment. For instance, service-oriented computing (SOC) envisages\\napplications as services running over globally available computational\\nresources where discovery and binding between them is transparently performed\\nby a middleware. Asynchronous Relational Networks (ARNs) is a well-known formal\\norchestration model, based on hypergraphs, for the description of\\nservice-oriented software artefacts. Choreography and orchestration are the two\\nmain design principles for the development of distributed software. In this\\nwork, we propose Communicating Relational Networks (CRNs), which is a variant\\nof ARNs, but relies on choreographies for the characterisation of the\\ncommunicational aspects of a software artefact, and for making their automated\\nanalysis more efficient.', 'Modern science, technology, and politics are all permeated by data that comes\\nfrom people, measurements, or computational processes. While this data is often\\nincomplete, corrupt, or lacking in sufficient accuracy and precision, explicit\\nconsideration of uncertainty is rarely part of the computational and decision\\nmaking pipeline. The CCC Workshop on Quantification, Communication, and\\nInterpretation of Uncertainty in Simulation and Data Science explored this\\nproblem, identifying significant shortcomings in the ways we currently process,\\npresent, and interpret uncertain data. Specific recommendations on a research\\nagenda for the future were made in four areas: uncertainty quantification in\\nlarge-scale computational simulations, uncertainty quantification in data\\nscience, software support for uncertainty computation, and better integration\\nof uncertainty quantification and communication to stakeholders.', 'With huge design spaces for unique chemical and mechanical properties, we\\nremove a roadblock to computational design of {high-entropy alloys} using a\\nmetaheuristic hybrid Cuckoo Search (CS) for \"on-the-fly\" construction of\\nSuper-Cell Random APproximates (SCRAPs) having targeted atomic site and pair\\nprobabilities on arbitrary crystal lattices. Our hybrid-CS schema overcomes\\nlarge, discrete combinatorial optimization by ultrafast global solutions that\\nscale linearly in system size and strongly in parallel, e.g. a 4-element,\\n128-atom model [a $10^{73+}$ space] is found in seconds -- a reduction of\\n13,000+ over current strategies. With model-generation eliminated as a\\nbottleneck, computational alloy design can be performed that is currently\\nimpossible or impractical. We showcase the method for real alloys with varying\\nshort-range order. Being problem-agnostic, our hybrid-CS schema offers numerous\\napplications in diverse fields.', 'The search for new computational machines beyond the traditional von Neumann\\narchitecture has given rise to a modern area of nonlinear science --\\ndevelopment of unconventional computing -- requiring the efforts of\\nmathematicians, physicists and engineers. Many analogue physical systems\\nincluding nonlinear oscillator networks, lasers, and condensates were proposed\\nand realised to address hard computational problems from various areas of\\nsocial and physical sciences and technology. The analogue systems emulate spin\\nHamiltonians with continuous or discrete degrees of freedom to which actual\\noptimisation problems can be mapped. Understanding the underlying physical\\nprocess by which the system finds the ground state often leads to new classes\\nof system-inspired or quantum-inspired algorithms for hard optimisation.\\nTogether physical platforms and related algorithms can be combined to form a\\nhybrid architecture that may one day compete with conventional computing. In\\nthis Chapter, we review some of the systems and physically-inspired algorithms\\nthat show such promise.', \"This study evaluates the effectiveness of various large language models\\n(LLMs) in performing tasks common among undergraduate computer science\\nstudents. Although a number of research studies in the computing education\\ncommunity have explored the possibility of using LLMs for a variety of tasks,\\nthere is a lack of comprehensive research comparing different LLMs and\\nevaluating which LLMs are most effective for different tasks. Our research\\nsystematically assesses some of the publicly available LLMs such as Google\\nBard, ChatGPT, GitHub Copilot Chat, and Microsoft Copilot across diverse tasks\\ncommonly encountered by undergraduate computer science students. These tasks\\ninclude code generation, explanation, project ideation, content generation,\\nclass assignments, and email composition. Evaluation for these tasks was\\ncarried out by junior and senior students in computer science, and provides\\ninsights into the models' strengths and limitations. This study aims to guide\\nstudents in selecting suitable LLMs for any specific task and offers valuable\\ninsights on how LLMs can be used constructively by students and instructors.\", \"Quantum computing is a good way to justify difficult physics experiments. But\\nuntil quantum computers are built, do computer scientists need to know anything\\nabout quantum information? In fact, quantum computing is not merely a recipe\\nfor new computing devices, but a new way of looking at the world that has been\\nastonishingly intellectually productive. In this article, I'll talk about where\\nquantum computing came from, what it is, and what we can learn from it.\", 'We study computable topological spaces and semicomputable and computable sets\\nin these spaces. In particular, we investigate conditions under which\\nsemicomputable sets are computable. We prove that a semicomputable compact\\nmanifold $M$ is computable if its boundary $\\\\partial M$ is computable. We also\\nshow how this result combined with certain construction which compactifies a\\nsemicomputable set leads to the conclusion that some noncompact semicomputable\\nmanifolds in computable metric spaces are computable.', 'Virtualization promises significant benefits in security, efficiency,\\ndependability, and cost. Achieving these benefits depends upon the reliability\\nof the underlying virtual machine monitors (hypervisors). This paper describes\\nan ongoing project to develop and verify MinVisor, a simple but functional\\nType-I x86 hypervisor, proving protection properties at the assembly level\\nusing ACL2. Originally based on an existing research hypervisor, MinVisor\\nprovides protection of its own memory from a malicious guest. Our long-term\\ngoal is to fully verify MinVisor, providing a vehicle to investigate the\\nmodeling and verification of hypervisors at the implementation level, and also\\na basis for further systems research. Functional segments of the MinVisor C\\ncode base are translated into Y86 assembly, and verified with respect to the\\nY86 model. The inductive assertions (also known as \"compositional cutpoints\")\\nmethodology is used to prove the correctness of the code. The proof of the code\\nthat sets up the nested page tables is described. We compare this project to\\nrelated efforts in systems code verification and outline some useful steps\\nforward.', 'The Lax Logical Framework, LLFP, was introduced, by a team including the last\\ntwo authors, to provide a conceptual framework for integrating different proof\\ndevelopment tools, thus allowing for external evidence and for postponing,\\ndelegating, or factoring-out side conditions. In particular, LLFP allows for\\nreducing the number of times a proof-irrelevant check is performed. In this\\npaper we give a shallow, actually definitional, implementation of LLFP in Coq,\\ni.e. we use Coq both as host framework and oracle for LLFP. This illuminates\\nthe principles underpinning the mechanism of Lock-types and also suggests how\\nto possibly extend Coq with the features of LLFP. The derived proof editor is\\nthen put to use for developing case-studies on an emerging paradigm, both at\\nlogical and implementation level, which we call fast and loose reasoning\\nfollowing Danielsson et alii [6]. This paradigm trades off efficiency for\\ncorrectness and amounts to postponing, or running in parallel, tedious or\\ncomputationally demanding checks, until we are really sure that the intended\\ngoal can be achieved. Typical examples are branch-prediction in CPUs and\\noptimistic concurrency control.', 'The rapid growth of technology and computer science, which has led to a surge\\nin demand for skilled professionals in this field. The skill set required for\\ncomputer science jobs has evolved rapidly, creating challenges for those\\nalready in the workforce who need to adapt their skills quickly to meet\\nindustry demands. To stay ahead of the curve, it is essential to understand the\\nhottest skills needed in the field. The article introduces a new method for\\nanalyzing job advertisements using social network analysis to identify the most\\ncritical skills required by employers in the market. In this research, to form\\nthe communication network of skills, first 5763 skills were collected from the\\nLinkedIn social network, then the relationship between skills was collected and\\nsearched in 7777 computer science job advertisements, and finally, the balanced\\ncommunication network of skills was formed. The study analyzes the formed\\ncommunication network of skills in the computer science job market and\\nidentifies four distinct communities of skills: Generalists, Infrastructure and\\nSecurity, Software Development, and Embedded Systems. The findings reveal that\\nemployers value both hard and soft skills, such as programming languages and\\nteamwork. Communication skills were found to be the most important skill in the\\nlabor market. Additionally, certain skills were highlighted based on their\\ncentrality indices, including communication, English, SQL, Git, and business\\nskills, among others. The study provides valuable insights into the current\\nstate of the computer science job market and can help guide individuals and\\norganizations in making informed decisions about skills acquisition and hiring\\npractices.', 'Today, cloud computing is an emerging way of computing in computer science.\\nCloud computing is a set of resources and services that are offered by the\\nnetwork or internet. Cloud computing extends various computing techniques like\\ngrid computing, distributed computing. Today cloud computing is used in both\\nindustrial field and academic field. Cloud facilitates its users by providing\\nvirtual resources via internet. As the field of cloud computing is spreading\\nthe new techniques are developing. This increase in cloud computing environment\\nalso increases security challenges for cloud developers. Users of cloud save\\ntheir data in the cloud hence the lack of security in cloud can lose the users\\ntrust. In this paper we will discuss some of the cloud security issues in\\nvarious aspects like multi-tenancy, elasticity, availability etc. The paper\\nalso discuss existing security techniques and approaches for a secure cloud.\\nThis paper will enable researchers and professionals to know about different\\nsecurity threats and models and tools proposed.', 'Cloud Computing is an Internet based computing, whereby shared resources,\\nsoftware and information, are provided to computers and devices on demand, like\\nthe electricity grid. Currently, IaaS (Infrastructure as a Service), PaaS\\n(Platform as a Service) and SaaS (Software as a Service) are used as a business\\nmodel for Cloud Computing. Nowadays, the adoption and deployment of Cloud\\nComputing is increasing in various domains, forcing researchers to conduct\\nresearch in the area of Cloud Computing globally. Setting up the research\\nenvironment is critical for the researchers in the developing countries to\\nevaluate the research outputs. Currently, modeling, simulation technology and\\naccess of resources from various university data centers has become a useful\\nand powerful tool in cloud computing research. Several cloud simulators have\\nbeen specifically developed by various universities to carry out Cloud\\nComputing research, including CloudSim, SPECI, Green Cloud and Future Systems\\n(the Indiana University machines India, Bravo, Delta, Echo and Foxtrot)\\nsupports leading edge data science research and a broad range of\\ncomputing-enabled education as well as integration of ideas from cloud and HPC\\nsystems. In this paper, the features, suitability, adaptability and the\\nlearning curve of the existing Cloud Computing simulators and Future Systems\\nare reviewed and analyzed.', 'The novel coronavirus pandemic continues to ravage communities across the US.\\nOpinion surveys identified importance of political ideology in shaping\\nperceptions of the pandemic and compliance with preventive measures. Here, we\\nuse social media data to study complexity of polarization. We analyze a large\\ndataset of tweets related to the pandemic collected between January and May of\\n2020, and develop methods to classify the ideological alignment of users along\\nthe moderacy (hardline vs moderate), political (liberal vs conservative) and\\nscience (anti-science vs pro-science) dimensions. While polarization along the\\nscience and political dimensions are correlated, politically moderate users are\\nmore likely to be aligned with the pro-science views, and politically hardline\\nusers with anti-science views. Contrary to expectations, we do not find that\\npolarization grows over time; instead, we see increasing activity by moderate\\npro-science users. We also show that anti-science conservatives tend to tweet\\nfrom the Southern US, while anti-science moderates from the Western states. Our\\nfindings shed light on the multi-dimensional nature of polarization, and the\\nfeasibility of tracking polarized opinions about the pandemic across time and\\nspace through social media data.', 'Citizen Science is research undertaken by professional scientists and members\\nof the public collaboratively. Despite numerous benefits of citizen science for\\nboth the advancement of science and the community of the citizen scientists,\\nthere is still no comprehensive knowledge of patterns of contributions, and the\\ndemography of contributors to citizen science projects. In this paper we\\nprovide a first overview of spatiotemporal and gender distribution of citizen\\nscience workforce by analyzing 54 million classifications contributed by more\\nthan 340 thousand citizen science volunteers from 198 countries to one of the\\nlargest citizen science platforms, Zooniverse. First we report on the uneven\\ngeographical distribution of the citizen scientist and model the variations\\namong countries based on the socio-economic conditions as well as the level of\\nresearch investment in each country. Analyzing the temporal features of\\ncontributions, we report on high \"burstiness\" of participation instances as\\nwell as the leisurely nature of participation suggested by the time of the day\\nthat the citizen scientists were the most active. Finally, we discuss the\\ngender imbalance among citizen scientists (about 30% female) and compare it\\nwith other collaborative projects as well as the gender distribution in more\\nformal scientific activities. Citizen science projects need further attention\\nfrom outside of the academic community, and our findings can help attract the\\nattention of public and private stakeholders, as well as to inform the design\\nof the platforms and science policy making processes.', 'Axiomatic approach has demonstrated its power in mathematics. The main goal\\nof this preprint is to show that axiomatic methods are also very efficient for\\ncomputer science. It is possible to apply these methods to many problems in\\ncomputer science. Here the main modes of computer functioning and program\\nexecution are described, formalized, and studied in an axiomatic context. The\\nemphasis is on three principal modes: computation, decision, and acceptation.\\nNow the prevalent mode for computers is computation. Problems of artificial\\nintelligence involve decision mode, while communication functions of computer\\ndemand accepting mode. The main goal of this preprint is to study properties of\\nthese modes and relations between them. These problems are closely related to\\nsuch fundamental concepts of computer science and technology as computability,\\ndecidability, and acceptability. In other words, we are concerned with the\\nquestion what computers and software systems can do working in this or that\\nmode. Consequently, results of this preprint allow one to achieve higher\\nunderstanding of computations and in such a way, to find some basic properties\\nof computers and their applications. Classes of algorithms, which model\\ndifferent kinds of computers and software, are compared with respect to their\\ncomputing, accepting or deciding power. Operations with algorithms and machines\\nare introduced. Examples show how to apply axiomatic results to different\\nclasses of algorithms and machines in order to enhance their performance.', 'Social computing encompasses the mechanisms through which people interact\\nwith computational systems: crowdsourcing systems, ranking and recommendation\\nsystems, online prediction markets, citizen science projects, and\\ncollaboratively edited wikis, to name a few. These systems share the common\\nfeature that humans are active participants, making choices that determine the\\ninput to, and therefore the output of, the system. The output of these systems\\ncan be viewed as a joint computation between machine and human, and can be\\nricher than what either could produce alone. The term social computing is often\\nused as a synonym for several related areas, such as \"human computation\" and\\nsubsets of \"collective intelligence\"; we use it in its broadest sense to\\nencompass all of these things.\\n  Social computing is blossoming into a rich research area of its own, with\\ncontributions from diverse disciplines including computer science, economics,\\nand other social sciences. Yet a broad mathematical foundation for social\\ncomputing is yet to be established, with a plethora of under-explored\\nopportunities for mathematical research to impact social computing.\\n  As in other fields, there is great potential for mathematical work to\\ninfluence and shape the future of social computing. However, we are far from\\nhaving the systematic and principled understanding of the advantages,\\nlimitations, and potentials of social computing required to match the impact on\\napplications that has occurred in other fields. In June 2015, we brought\\ntogether roughly 25 experts in related fields to discuss the promise and\\nchallenges of establishing mathematical foundations for social computing. This\\ndocument captures several of the key ideas discussed.', 'We study observation-based strategies for two-player turn-based games on\\ngraphs with omega-regular objectives. An observation-based strategy relies on\\nimperfect information about the history of a play, namely, on the past sequence\\nof observations. Such games occur in the synthesis of a controller that does\\nnot see the private state of the plant. Our main results are twofold. First, we\\ngive a fixed-point algorithm for computing the set of states from which a\\nplayer can win with a deterministic observation-based strategy for any\\nomega-regular objective. The fixed point is computed in the lattice of\\nantichains of state sets. This algorithm has the advantages of being directed\\nby the objective and of avoiding an explicit subset construction on the game\\ngraph. Second, we give an algorithm for computing the set of states from which\\na player can win with probability 1 with a randomized observation-based\\nstrategy for a Buechi objective. This set is of interest because in the absence\\nof perfect information, randomized strategies are more powerful than\\ndeterministic ones. We show that our algorithms are optimal by proving matching\\nlower bounds.', \"In this survey, we explore Andrei Nikolayevich Kolmogorov's seminal work in\\njust one of his many facets: its influence Computer Science especially his\\nviewpoint of what herein we call 'Algorithmic Theory of Informatics.'\\n  Can a computer file 'reduce' its 'size' if we add to it new symbols? Do\\nequations of state like second Newton law in Physics exist in Computer Science?\\nCan Leibniz' principle of identification by indistinguishability be formalized?\\n  In the computer, there are no coordinates, no distances, and no dimensions;\\nmost of traditional mathematical approaches do not work. The computer processes\\nfinite binary sequences i.e. the sequences of 0 and 1. A natural question\\narises: Should we continue today, as we have done for many years, to approach\\nComputer Science problems by using classical mathematical apparatus such as\\n'mathematical modeling'? The first who drew attention to this question and gave\\ninsightful answers to it was Kolmogorov in 1960s. Kolmogorov's empirical\\npostulate about existence of a program that translates 'a natural number into\\nits binary record and the record into the number' formulated in 1958 represents\\na hint of Kolmogorov's approach to Computer Science.\\n  Following his ideas, we interpret Kolmogorov algorithm, Kolmogorov machine,\\nand Kolmogorov complexity in the context of modern information technologies\\nshowing that they essentially represent fundamental elements of Algorithmic\\nTheory of Informatics, Kolmogorov Programmable Technology, and new Komputer\\nMathematics i.e. Mathematics of computers.\", 'This Master thesis examines issues of interoperability and integration\\nbetween the Classic Information Science (CIS) and Quantum Information Science\\n(QIS). It provides a short introduction to the Extensible Markup Language (XML)\\nand proceeds to describe the development steps that have lead to a prototype\\nXML specification for quantum computing (QIS-XML). QIS-XML is a proposed\\nframework, based on the widely used standard (XML) to describe, visualize,\\nexchange and process quantum gates and quantum circuits. It also provides a\\npotential approach to a generic programming language for quantum computers\\nthrough the concept of XML driven compilers. Examples are provided for the\\ndescription of commonly used quantum gates and circuits, accompanied with tools\\nto visualize them in standard web browsers. An algorithmic example is also\\npresented, performing a simple addition operation with quantum circuits and\\nrunning the program on a quantum computer simulator. Overall, this initial\\neffort demonstrates how XML technologies could be at the core of the\\narchitecture for describing and programming quantum computers. By leveraging a\\nwidely accepted standard, QIS-XML also builds a bridge between classic and\\nquantum IT, which could foster the acceptance of QIS by the ICT community and\\nfacilitate the understanding of quantum technology by IT experts. This would\\nsupport the consolidation of Classic Information Science and Quantum\\nInformation Science into a Complete Information Science, a challenge that could\\nbe referred to as the \"Information Science Grand Unification Challenge\".', 'The last decade saw the emergence of systematic large-scale replication\\nprojects in the social and behavioral sciences, (Camerer et al., 2016, 2018;\\nEbersole et al., 2016; Klein et al., 2014, 2018; Collaboration, 2015). These\\nprojects were driven by theoretical and conceptual concerns about a high\\nfraction of \"false positives\" in the scientific publications (Ioannidis, 2005)\\n(and a high prevalence of \"questionable research practices\" (Simmons, Nelson,\\nand Simonsohn, 2011). Concerns about the credibility of research findings are\\nnot unique to the behavioral and social sciences; within Computer Science,\\nArtificial Intelligence (AI) and Machine Learning (ML) are areas of particular\\nconcern (Lucic et al., 2018; Freire, Bonnet, and Shasha, 2012; Gundersen and\\nKjensmo, 2018; Henderson et al., 2018). Given the pioneering role of the\\nbehavioral and social sciences in the promotion of novel methodologies to\\nimprove the credibility of research, it is a promising approach to analyze the\\nlessons learned from this field and adjust strategies for Computer Science, AI\\nand ML In this paper, we review approaches used in the behavioral and social\\nsciences and in the DARPA SCORE project. We particularly focus on the role of\\nhuman forecasting of replication outcomes, and how forecasting can leverage the\\ninformation gained from relatively labor and resource-intensive replications.\\nWe will discuss opportunities and challenges of using these approaches to\\nmonitor and improve the credibility of research areas in Computer Science, AI,\\nand ML.', 'Limit computable functions can be characterized by Turing jumps on the input\\nside or limits on the output side. As a monad of this pair of adjoint\\noperations we obtain a problem that characterizes the low functions and dually\\nto this another problem that characterizes the functions that are computable\\nrelative to the halting problem. Correspondingly, these two classes are the\\nlargest classes of functions that can be pre or post composed to limit\\ncomputable functions without leaving the class of limit computable functions.\\nWe transfer these observations to the lattice of represented spaces where it\\nleads to a formal Galois connection. We also formulate a version of this result\\nfor computable metric spaces. Limit computability and computability relative to\\nthe halting problem are notions that coincide for points and sequences, but\\neven restricted to continuous functions the former class is strictly larger\\nthan the latter. On computable metric spaces we can characterize the functions\\nthat are computable relative to the halting problem as those functions that are\\nlimit computable with a modulus of continuity that is computable relative to\\nthe halting problem. As a consequence of this result we obtain, for instance,\\nthat Lipschitz continuous functions that are limit computable are automatically\\ncomputable relative to the halting problem. We also discuss 1-generic points as\\nthe canonical points of continuity of limit computable functions, and we prove\\nthat restricted to these points limit computable functions are computable\\nrelative to the halting problem. Finally, we demonstrate how these results can\\nbe applied in computable analysis.', 'Interest in computer science is growing. As a result, computer science (CS)\\nand related departments are experiencing an explosive increase in undergraduate\\nenrollments and unprecedented demand from other disciplines for learning\\ncomputing. According to the 2014 CRA Taulbee Survey, the number of\\nundergraduates declaring a computing major at Ph.D. granting departments in the\\nUS has increased 60% from 2011-2014 and the number of degrees granted has\\nincreased by 34% from 2008-2013. However, this growth is not limited to higher\\neducation. New York City, San Francisco and Oakland public schools will soon be\\noffering computer science to all students at all schools from preschool to 12th\\ngrade, although it will be an elective for high school students. This\\nunprecedented demand means that CS departments are likely to teach not only\\nmore students in the coming decades, but more diverse students, with more\\nvaried backgrounds, motivations, preparations, and abilities.\\n  This growth is an unparalleled opportunity to expand the reach of computing\\neducation. However, this growth is also a unique research challenge, as we know\\nvery little about how best to teach our current students, let alone the\\nstudents soon to arrive. The burgeoning field of Computing Education Research\\n(CER) is positioned to address this challenge by answering research questions\\nsuch as, how should we teach computer science, from programming to advanced\\nprinciples, to a broader and more diverse audience? We argue that computer\\nscience departments should lead the way in establishing CER as a foundational\\nresearch area of computer science, discovering the best ways to teach CS, and\\ninventing the best technologies with which to teach it. This white paper\\nprovides a snapshot of the current state of CER and makes actionable\\nrecommendations for academic leaders to grow CER as a successful research area\\nin their departments.', 'The four authors present their speculations about the future developments of\\nmathematical logic in the twenty-first century. The areas of recursion theory,\\nproof theory and logic for computer science, model theory, and set theory are\\ndiscussed independently.', 'We define the notion of a partially additive Kleene algebra, which is a\\nKleene algebra where the + operation need only be partially defined. These\\nstructures formalize a number of examples that cannot be handled directly by\\nKleene algebras. We relate partially additive Kleene algebras to existing\\nalgebraic structures, by exhibiting categorical connections with Kleene\\nalgebras, partially additive categories, and closed semirings.', 'This article presents an implementation of a graphical software with various\\nalgorithms in Operations research, like minimum path, minimum tree, chinese\\npostman problem and travelling salesman.', 'We give a new simple proof of the decidability of the First Order Theory of\\n(omega^omega^i,+) and the Monadic Second Order Theory of (omega^i,<), improving\\nthe complexity in both cases. Our algorithm is based on tree automata and a new\\nrepresentation of (sets of) ordinals by (infinite) trees.', 'We give a brief overview of the main characteristics of diagrammatic\\nreasoning, analyze a case of human reasoning in a mastermind game, and explain\\nwhy hybrid representation systems (HRS) are particularly attractive and\\npromising for Artificial General Intelligence and Computer Science in general.', \"We consider the $n\\\\times n$ game of Phutball. It is shown that, given an\\narbitrary position of stones on the board, it is a PSPACE-hard problem to\\ndetermine whether the specified player can win the game, regardless of the\\nopponent's choices made during the game.\", 'In order to better understand reasoning involved in analyzing infinite games\\nin extensive form, we performed experiments in the proof assistant Coq that are\\nreported here.', 'For movements of the viscous continuous flow in generalized Couette cell the\\ndynamic system describing the central limiting variety is received.', 'This paper presents simulation-based relations for probabilistic game\\nstructures. The first relation is called probabilistic alternating simulation,\\nand the second called probabilistic alternating forward simulation, following\\nthe naming convention of Segala and Lynch. We study these relations with\\nrespect to the preservation of properties specified in probabilistic\\nalternating-time temporal logic.', 'This paper discusses the important process of knowledge and its management,\\nand differences between tacit and explicit knowledge and understanding the\\nculture as a key issue for the successful implementation of knowledge\\nmanagement, in addition to, this paper is concerned with the four-stage model\\nfor the evolution of information technology (IT) support for knowledge\\nmanagement in law firms.', 'This paper surveys main and recent studies on temporal logics in a broad\\nsense by presenting various logic systems, dealing with various time\\nstructures, and discussing important features, such as decidability (or\\nundecidability) results, expressiveness and proof systems.', 'The following random process on $\\\\Z^4$ is studied. At first visit to a site,\\nthe two first coordinates perform a (2-dimensional) simple random walk step. At\\nfurther visits, it is the last two coordinates which perform a simple random\\nwalk step. We prove that this process is almost surely transient. The lower\\ndimensional versions are discussed and various generalizations and related\\nquestions are proposed.', 'In this paper, we discuss a well-known self-referential paradox in\\nfoundational game theory, the Brandenburger - Keisler paradox. We approach the\\nparadox from two different perspectives: non-well-founded set theory and\\nparaconsistent logic. We show that the paradox persists in both frameworks for\\ncategory theoretical reasons, but, with different properties.', 'In this paper we introduce public announcement logic in different geometric\\nframeworks. First, we consider topological models, and then extend our\\ndiscussion to a more expressive model, namely, subset space models.\\nFurthermore, we prove the completeness of public announcement logic in those\\nframeworks. Moreover, we apply our results to different issues: announcement\\nstabilization, backward induction and persistence.', 'The feeling of good or bad luck occurs whenever there is an emotion contrast\\nbetween an event and an easily accessible counterfactual alternative. This\\nstudy suggests that cognitive simplicity plays a key role in the human ability\\nto experience good and bad luck after the occurrence of an event.', 'The theme of the paper is the use of commutative Frobenius algebras in\\nbraided strict monoidal categories in the study of varieties of circuits and\\ncommunicating systems which occur in Computer Science, including circuits in\\nwhich the wires are tangled. We indicate also some possible novel geometric\\ninterest in such algebras.', 'In recent work, the author and others have studied compositional algebras of\\nPetri nets. Here we consider mathematical aspects of the pure linking algebras\\nthat underly them. We characterise composition of nets without places as the\\ncomposition of spans over appropriate categories of relations, and study the\\nunderlying algebraic structures.', 'We show that the counting sequence for permutations avoiding both of the\\n(classical) patterns 1243 and 2134 has the algebraic generating function\\nsupplied by Vaclav Kotesovec for sequence A164651 in The On-Line Encyclopedia\\nof Integer Sequences.', 'We show that permutations avoiding both of the (classical) patterns 4321 and\\n3241 have the algebraic generating function conjectured by Vladimir Kruchinin.', 'We propose a semantics for permutation equivalence in higher-order rewriting.\\nThis semantics takes place in cartesian closed 2-categories, and is proved\\nsound and complete.', 'We study conditions relating to the impossibility of agreeing to disagree in\\nmodels of interactive KD45 belief (in contrast to models of S5 knowledge, which\\nare used in nearly all the agreements literature). We show that even when the\\ntruth axiom is not assumed it turns out that players will find it impossible to\\nagree to disagree under fairly broad conditions.', 'We characterise piecewise Boolean domains, that is, those domains that arise\\nas Boolean subalgebras of a piecewise Boolean algebra. This leads to equivalent\\ndescriptions of the category of piecewise Boolean algebras: either as piecewise\\nBoolean domains equipped with an orientation, or as full structure sheaves on\\npiecewise Boolean domains.', 'It is an unfortunate convention of science that research should pretend to be\\nreproducible; our top tips will help you mitigate this fussy conventionality,\\nenabling you to enthusiastically showcase your irreproducible work.', 'The satisfiability and finite satisfiability problems for the two-variable\\nguarded fragment of first-order logic with counting quantifiers, a database,\\nand path-functional dependencies are both ExpTime-complete.', 'I overview the work of the Tbilisi school on intuitionistic modal logics of\\nwell-founded/scattered structures and its connections with contemporary\\ntheoretical computer science. Fixed-point theorems and their consequences are\\nof particular interest.', \"In 1998 Zielonka simplified the proofs of memoryless determinacy of infinite\\nparity games. In 2018 Haddad simplified some proofs of memoryless determinacy\\nof finite parity games. This article adapts Haddad's technique for infinite\\nparity games. Two proofs are given, a shorter one and a more constructive one.\\nNone of them uses Zielonka's traps and attractors.\", 'We introduce parallelism into the basic algebra of games to model concurrent\\ngame algebraically. Parallelism is treated as a new kind of game operation. The\\nresulted algebra of concurrent games can be used widely to reason the parallel\\nsystems.', 'We define vertex cover algebras for weighted simplicial multicomplexes and\\nprove basics properties of them. Also, we describe these algebras for\\nmulticomplexes which have only one maximal facet and we prove that they are\\nfinitely generated.', 'A new class of languages of infinite words is introduced, called the\\nmax-regular languages, extending the class of $\\\\omega$-regular languages. The\\nclass has two equivalent descriptions: in terms of automata (a type of\\ndeterministic counter automaton), and in terms of logic (weak monadic\\nsecond-order logic with a bounding quantifier). Effective translations between\\nthe logic and automata are given.', 'We discuss what hampers the rate of scientific progress in our exponentially\\ngrowing world. The rapid increase in technologies leaves the growth of research\\nresult metrics far behind. The reason for this lies in the education of\\nastronomers lacking basic computer science aspects crucially important in the\\ndata intensive science era.', 'We use the technique of \"classical realizability\" to build new models of ZF +\\nDC in which R is not well ordered. This gives new relative consistency results,\\nprobably not obtainable by forcing. This gives also a new method to get\\nprograms from proofs of arithmetical formulas with dependent choice.', 'The main thrust of the article is to provide interesting example, useful for\\nstudents of using bitwise operations in the programming languages C ++ and\\nJava. As an example, we describe an algorithm for obtaining a Latin square of\\narbitrary order. We will outline some techniques for the use of bitwise\\noperations.', 'We present the Theorem Prover Museum, and initiative to conserve -- and make\\npublicly available -- the sources and source-related artefacts of automated\\nreasoning systems. Theorem provers have been at the forefront of Artificial\\nIntelligence, stretching the limits of computation, and incubating many\\ninnovations we take for granted today. Without the systems themselves as\\npreserved cultural artefacts, future historians will have difficulties to study\\nthe history of science and engineering in our discipline.', 'The article considers strategies of coalitions that are based on intelligence\\ninformation about moves of some of the other agents. The main technical result\\nis a sound and complete logical system that describes the interplay between\\ncoalition power modality with intelligence and distributed knowledge modality\\nin games with imperfect information.', 'We discuss how mathematical semantics has evolved, and suggest some new\\ndirections for future work. As an example, we discuss some recent work on\\nencapsulating model comparison games as comonads, in the context of finite\\nmodel theory.', 'Discounting future costs and rewards is a common practice in accounting, game\\ntheory, and machine learning. In spite of this, existing logics for reasoning\\nabout strategies with cost and resource constraints do not account for\\ndiscounting. The paper proposes a sound and complete logical system for\\nreasoning about budget-constrained strategic abilities that incorporates\\ndiscounting into its semantics.', 'This article highlights some of the basic concepts of bioinformatics and data\\nmining. The major research areas of bioinformatics are highlighted. The\\napplication of data mining in the domain of bioinformatics is explained. It\\nalso highlights some of the current challenges and opportunities of data mining\\nin bioinformatics.', 'We present a refinement of the Calculus of Inductive Constructions in which\\none can easily define a notion of relational parametricity. It provides a new\\nway to automate proofs in an interactive theorem prover like Coq.', 'In this paper we study the concept of intuitionistic neutrosophic set of\\nBhowmik and Pal. We have introduced this concept in soft sets and defined\\nintuitionistic neutrosophic soft set. Some definitions and operations have been\\nintroduced on intuitionistic neutrosophic soft set. Some properties of this\\nconcept have been established.', 'Suppose that m is a positive integer, not a perfect square. We present a\\nformula solution to the 2-variable Frobenius problem in Z[\\\\sqrt m] of the\\n\"first kind\" ([3]).', 'Some relations among Pythagorean triples are established. The main tool is a\\nfundamental characterization of the Pythagorean triples through a chatetus\\nwhich allows to determine relationships with Pythagorean triples having the\\nsame chatetus raised to an integer power.', 'In this paper, we give a brief overview of the rule-based programming system,\\ncalled P$\\\\rho$Log and illustrate its capabilities.', 'Methodology adapted from data science sparked the field of materials\\ninformatics, and materials databases are at the heart of it. Applying\\nartificial intelligence to these databases will allow the prediction of\\nproperties of complex organic crystals.', 'Abstract. Matching logic cannot handle concurrency. We introduce concurrent\\nmatching logic (CML) to reason about fault-free partial correctness of\\nshared-memory concurrent programs. We also present a soundness proof for\\nconcurrent matching logic (CML) in terms of operational semantics. Under\\ncertain assumptions, the assertion of CSL can be transformed into the assertion\\nof CML. Hence, CSL can be seen as an instance of CML.', 'We prove that the category of c-spaces with continuous maps is not cartesian\\nclosed. As a corollary the category of locally finitary compact spaces with\\ncontinuous maps is also not cartesian closed.', 'In this perspective I give my answer to the question of how quantum computing\\nwill impact on data-intensive applications in engineering and science. I focus\\non quantum Monte Carlo integration as a likely source of (relatively) near-term\\nquantum advantage, but also discuss some other ideas that have garnered\\nwide-spread interest.', 'We propose a definition of computable manifold by introducing computability\\nas a structure that we impose to a given topological manifold, just in the same\\nway as differentiability or piecewise linearity are defined for smooth and PL\\nmanifolds respectively. Using the framework of computable topology and Type-2\\ntheory of effectivity, we develop computable versions of all the basic concepts\\nneeded to define manifolds, like computable atlases and (computably) compatible\\ncomputable atlases. We prove that given a computable atlas $\\\\Phi$ defined on a\\nset $M$, we can construct a computable topological space $(M, \\\\tau_\\\\Phi,\\n\\\\beta_\\\\Phi, \\\\nu_\\\\Phi)$, where $\\\\tau_\\\\Phi$ is the topology on $M$ induced by\\n$\\\\Phi$ and that the equivalence class of this computable space characterizes\\nthe computable structure determined by $\\\\Phi$. The concept of computable\\nsubmanifold is also investigated. We show that any compact computable manifold\\nwhich satisfies a computable version of the $T_2$-separation axiom, can be\\nembedded as a computable submanifold of some euclidean space $\\\\mathbb{R}^{q}$,\\nwith a computable embedding, where $\\\\mathbb{R}^{q}$ is equipped with its usual\\ntopology and some canonical computable encoding of all open rational balls.', 'We describe the design of Comlex Syntax, a computational lexicon providing\\ndetailed syntactic information for approximately 38,000 English headwords. We\\nconsider the types of errors which arise in creating such a lexicon, and how\\nsuch errors can be measured and controlled.', \"Shor's and Grover's famous quantum algorithms for factoring and searching\\nshow that quantum computers can solve certain computational problems\\nsignificantly faster than any classical computer. We discuss here what quantum\\ncomputers_cannot_ do, and specifically how to prove limits on their\\ncomputational power. We cover the main known techniques for proving lower\\nbounds, and exemplify and compare the methods.\", 'This paper contains the most important aspects of computing grids. Grid\\ncomputing allows high performance distributed systems to act as a single\\ncomputer. An overview of grids structure and techniques is given in order to\\nunderstand the way grids work.', 'The Turing machine (TM) and the Church thesis have formalized the concept of\\ncomputable number, this allowed to display non-computable numbers. This paper\\ndefines the concept of number \"approachable\" by a TM and shows that some (if\\nnot all) known non-computable numbers are approachable by TMs. Then an example\\nof a number not approachable by a TM is given.', 'Computational content encoded into constructive type theory proofs can be\\nused to make computing experiments over concrete data structures. In this\\npaper, we explore this possibility when working in Coq with chain complexes of\\ninfinite type (that is to say, generated by infinite sets) as a part of the\\nformalization of a hierarchy of homological algebra structures.', 'Computability logic is a formal theory of computability. The earlier article\\n\"Introduction to cirquent calculus and abstract resource semantics\" by\\nJaparidze proved soundness and completeness for the basic fragment CL5 of\\ncomputability logic. The present article extends that result to the more\\nexpressive cirquent calculus system CL6, which is a conservative extension of\\nboth CL5 and classical propositional logic.', 'We describe the Turing Machine, list some of its many influences on the\\ntheory of computation and complexity of computations, and illustrate its\\nimportance.', 'A coherent mathematical overview of computation and its generalisations is\\ndescribed. This conceptual framework is sufficient to comfortably host a wide\\nrange of contemporary thinking on embodied computation and its models.', 'The notion of g-frames for Hilbert spaces was introduced and studied by\\nWenchang Sun [16] as a generalization of the notion of frames. In this paper,\\nwe define computable g-frames in computable Hilbert spaces and obtain\\ncomputable versions of some of their characterizations and related results.', \"In this paper we have proposed an algorithm for computing prime implicates of\\na modal formula in $\\\\mathbf{K}$ using resolution method suggested in\\n\\\\cite{Enjalbert}. The algorithm suggested in this paper takes polynomial times\\nexponential time ,i.e, $O(n^{2k}\\\\times 2^{n})$ to compute prime implicates\\nwhereas Binevenu's algorithm \\\\cite{Bienvenu} takes doubly exponential time to\\ncompute prime implicates. We have also proved its correctness.\", 'One possible escape from the Gibbard-Satterthwaite theorem is computational\\ncomplexity. For example, it is NP-hard to compute if the STV rule can be\\nmanipulated. However, there is increasing concern that such results may not re\\nect the difficulty of manipulation in practice. In this tutorial, I survey\\nrecent results in this area.', 'We describe a case of an interplay between human and computer proving which\\nplayed a role in the discovery of an interesting mathematical result. The\\nunusual feature of the use of computers here was that a computer generated but\\nhuman readable proof was read, understood, generalized and abstracted by\\nmathematicians to obtain the key lemma in an interesting mathematical result.', 'We study the complexity of computing the Shapley value in games with\\nexternalities. We focus on two representations based on marginal contribution\\nnets (embedded MC-nets and weighted MC-nets). Our results show that while\\nweighted MC-nets are more concise than embedded MC-nets, they have slightly\\nworse computational properties when it comes to computing the Shapley value.', 'Classical and quantum information are very different. Together they can\\nperform feats that neither could achieve alone, such as quantum computing,\\nquantum cryptography and quantum teleportation. Some of the applications range\\nfrom helping to preventing spies from reading private communications. Among the\\ntools that will facilitate their implementation, we note quantum purification\\nand quantum error correction. Although some of these ideas are still beyond the\\ngrasp of current technology, quantum cryptography has been implemented and the\\nprospects are encouraging for small-scale prototypes of quantum computation\\ndevices before the end of the millennium.', 'The study deals with the parallelization of finite element based\\nNavier-Stokes codes using domain decomposition and state-ofart sparse direct\\nsolvers. There has been significant improvement in the performance of sparse\\ndirect solvers. Parallel sparse direct solvers are not found to exhibit good\\nscalability. Hence, the parallelization of sparse direct solvers is done using\\ndomain decomposition techniques. A highly efficient sparse direct solver\\nPARDISO is used in this study. The scalability of both Newton and modified\\nNewton algorithms are tested.', 'This paper discusses some generic approach for developing grid-based\\nframework for enabling establishment of workflows comprising existing software\\nin computational sciences areas. We highlight the main requirements addressed\\nthe developing of such framework. Some strategies for enabling interoperability\\nbetween convenient computation software in the grid environment has been shown.\\nThe UML based instruments of graphical description of workflows for the\\ndeveloping system has been suggested.', 'Among the various features of amino acids, the hydrophobic property has most\\nvisible impact on stability of a sequence folding. This is mentioned in many\\nprotein folding related work, in this paper we more elaborately discuss the\\ncomputational impact of the well defined hydrophobic aspect in determining\\nstability, approach with the help of a developed free energy computing\\nalgorithm covering various aspects preprocessing of an amino acid sequence,\\ngenerating the folding and calculating free energy. Later discussing its use in\\nprotein structure related research work.', 'Different points of view on the nature and content of the assertiveness are\\nfollowed in this paper. The main purpose is to study the assertive profile of\\nBulgarian students in computer science and computer engineering by analyzing\\nthe components of assertiveness. Research was performed using testing methods.\\nIt was found that the level of expressivity of this personal quality among\\nsubjects were above-average level.', 'The present paper deals with the possibility of creation of the quantum\\ncomputer in which the role of q-bits is played by quasi-particles. In such a\\ncomputer, the elementary computation block should represent a cluster created\\non the basis of the paramagnetic molecules. The latter form heterogeneous spin\\nstates in the cluster owing to the presence of interelectron correlations.', 'Computational science and engineering (CSE) has been misunderstood to advance\\nwith the construction of enormous computers. To the contrary, the historical\\nrecord demonstrates that innovations in CSE come from improvements to the\\nmathematics embodied by computer programs. Whether scientists and engineers\\nbecome inventors who make these breakthroughs depends on circumstances and the\\ninterdisciplinary extent of their educations. The USA currently has the largest\\nCSE professorate, but the data suggest this prominence is ephemeral.', \"Quadrics in the Grassmannian of lines in 3-space form a 19-dimensional\\nprojective space. We study the subvariety of coisotropic hypersurfaces.\\nFollowing Gel'fand, Kapranov and Zelevinsky, it decomposes into Chow forms of\\nplane conics, Chow forms of pairs of lines, and Hurwitz forms of quadric\\nsurfaces. We compute the ideals of these loci.\", 'This paper investigates how high school students approach computing through\\nan introductory computer science course situated in the Logic Programming (LP)\\nparadigm. This study shows how novice students operate within the LP paradigm\\nwhile engaging in foundational computing concepts and skills, and presents a\\ncase for LP as a viable paradigm choice for introductory CS courses.', 'The Wivace 2013 Electronic Proceedings in Theoretical Computer Science\\n(EPTCS) contain some selected long and short articles accepted for the\\npresentation at Wivace 2013 - Italian Workshop on Artificial Life and\\nEvolutionary Computation, which was held at the University of Milan-Bicocca,\\nMilan, on the 1st and 2nd of July, 2013.', 'Computing equilibria of games is a central task in computer science. A large\\nnumber of results are known for \\\\emph{Nash equilibrium} (NE). However, these\\ncan be adopted only when coalitions are not an issue. When instead agents can\\nform coalitions, NE is inadequate and an appropriate solution concept is\\n\\\\emph{strong Nash equilibrium} (SNE). Few computational results are known about\\nSNE. In this paper, we first study the problem of verifying whether a strategy\\nprofile is an SNE, showing that the problem is in $\\\\mathcal{P}$. We then design\\na spatial branch--and--bound algorithm to find an SNE, and we experimentally\\nevaluate the algorithm.', \"We introduce the notion of being Weihrauch-complete for layerwise\\ncomputability and provide several natural examples related to complex\\noscillations, the law of the iterated logarithm and Birkhoff's theorem. We also\\nconsider hitting time operators, which share the Weihrauch degree of the former\\nexamples but fail to be layerwise computable.\", 'It has been shown that a functional interpretation of proofs in mathematical\\nanalysis can be given by the product of selection functions, a mode of\\nrecursion that has an intuitive reading in terms of the computation of optimal\\nstrategies in sequential games. We argue that this result has genuine practical\\nvalue by interpreting some well-known theorems of mathematics and demonstrating\\nthat the product gives these theorems a natural computational interpretation\\nthat can be clearly understood in game theoretic terms.', 'Due to the fast development of the Cloud Computing technologies, the rapid\\nincrease of cloud services are became very remarkable. The fact of integration\\nof these services with many of the modern enterprises cannot be ignored.\\nMicrosoft, Google, Amazon, SalesForce.com and the other leading IT companies\\nare entered the field of developing these services. This paper presents a\\ncomprehensive survey of current cloud services, which are divided into eleven\\ncategories. Also the most famous providers for these services are listed.\\nFinally, the Deployment Models of Cloud Computing are mentioned and briefly\\ndiscussed.', 'How do massive stars explode? Progress toward the answer is driven by\\nincreases in compute power. Petascale supercomputers are enabling detailed\\nthree-dimensional simulations of core-collapse supernovae. These are\\nelucidating the role of fluid instabilities, turbulence, and magnetic field\\namplification in supernova engines.', \"We present our approach to teaching functional programming to First Year\\nComputer Science students at Middlesex University through projects in robotics.\\nA holistic approach is taken to the curriculum, emphasising the connections\\nbetween different subject areas. A key part of the students' learning is\\nthrough practical projects that draw upon and integrate the taught material. To\\nsupport these, we developed the Middlesex Robotic plaTfOrm (MIRTO), an\\nopen-source platform built using Raspberry Pi, Arduino, HUB-ee wheels and\\nrunning Racket (a LISP dialect). In this paper we present the motivations for\\nour choices and explain how a number of concepts of functional programming may\\nbe employed when programming robotic applications. We present some students'\\nwork with robotics projects: we consider the use of robotics projects to have\\nbeen a success, both for their value in reinforcing students' understanding of\\nprogramming concepts and for their value in motivating the students.\", \"In recent years, citizen science has grown in popularity due to a number of\\nreasons, including the emphasis on informal learning and creativity potential\\nassociated with these initiatives. Citizen science projects address research\\nquestions from various domains, ranging from Ecology to Astronomy. Due to the\\nadvancement of communication technologies, which makes outreach and engagement\\nof wider communities easier, scientists are keen to turn their own research\\ninto citizen science projects. However, the development, deployment and\\nmanagement of these projects remains challenging. One of the most important\\nchallenges is building the project itself. There is no single tool or\\nframework, which guides the step-by-step development of the project, since\\nevery project has specific characteristics, such as geographical constraints or\\nvolunteers' mode of participation. Therefore, in this article, we present a\\nseries of conceptual frameworks for categorisation, decision and deployment,\\nwhich guide a citizen science project creator in every step of creating a new\\nproject starting from the research question to project deployment. The\\nframeworks are designed with consideration to the properties of already\\nexisting citizen science projects and could be easily extended to include other\\ndimensions, which are not currently perceived.\", 'What are the landmark papers in scientific disciplines? On whose shoulders\\ndoes research in these fields stand? Which papers are indispensable for\\nscientific progress? These are typical questions which are not only of interest\\nfor researchers (who frequently know the answers - or guess to know them), but\\nalso for the interested general public. Citation counts can be used to identify\\nvery useful papers, since they reflect the wisdom of the crowd; in this case,\\nthe scientists using the published results for their own research. In this\\nstudy, we identified with recently developed methods for the program CRExplorer\\nlandmark publications in nearly all Web of Science subject categories (WoSSCs).\\nThese are publications which belong more frequently than other publications\\nacross the citing years to the top-per mill in their subject category. The\\nresults for three subject categories \"Information Science and Library Science\",\\n\"Computer Science, Information Systems\", and \"Computer Science, Software\\nEngineering\" are exemplarily discussed in more detail. The results for the\\nother WoSSCs can be found online at http://crexplorer.net.', 'This paper is devoted to studying a type of contact problems modeled by\\nhemivariational inequalities with small periodic coefficients appearing in\\nPDEs, and the PDEs we considered are linear, second order and uniformly\\nelliptic. Under the assumptions, it is proved that the original problem can be\\nhomogenized, and the solution weakly converges. We derive an\\n$O(\\\\epsilon^{1/2})$ estimation which is pivotal in building the computational\\nframework. We also show that Robin problems--- a special case of contact\\nproblems, it leads to an $O(\\\\epsilon)$ estimation in $L^2$ norm. Our\\ncomputational framework is based on finite element methods, and the numerical\\nanalysis is given, together with experiments to convince the estimation.', \"The TriRhenaTech alliance presents the accepted papers of the 'Upper-Rhine\\nArtificial Intelligence Symposium' held on October 27th 2021 in Kaiserslautern,\\nGermany. Topics of the conference are applications of Artificial Intellgence in\\nlife sciences, intelligent systems, industry 4.0, mobility and others. The\\nTriRhenaTech alliance is a network of universities in the Upper-Rhine\\nTrinational Metropolitan Region comprising of the German universities of\\napplied sciences in Furtwangen, Kaiserslautern, Karlsruhe, Offenburg and Trier,\\nthe Baden-Wuerttemberg Cooperative State University Loerrach, the French\\nuniversity network Alsace Tech (comprised of 14 'grandes \\\\'ecoles' in the\\nfields of engineering, architecture and management) and the University of\\nApplied Sciences and Arts Northwestern Switzerland. The alliance's common goal\\nis to reinforce the transfer of knowledge, research, and technology, as well as\\nthe cross-border mobility of students.\", 'Recently, graph neural networks have become a hot topic in machine learning\\ncommunity. This paper presents a Scopus based bibliometric overview of the GNNs\\nresearch since 2004, when GNN papers were first published. The study aims to\\nevaluate GNN research trend, both quantitatively and qualitatively. We provide\\nthe trend of research, distribution of subjects, active and influential authors\\nand institutions, sources of publications, most cited documents, and hot\\ntopics. Our investigations reveal that the most frequent subject categories in\\nthis field are computer science, engineering, telecommunications, linguistics,\\noperations research and management science, information science and library\\nscience, business and economics, automation and control systems, robotics, and\\nsocial sciences. In addition, the most active source of GNN publications is\\nLecture Notes in Computer Science. The most prolific or impactful institutions\\nare found in the United States, China, and Canada. We also provide must read\\npapers and future directions. Finally, the application of graph convolutional\\nnetworks and attention mechanism are now among hot topics of GNN research.', \"Large Language Models (LLMs) have exhibited remarkable capabilities in\\nunderstanding and interacting with natural language across various sectors.\\nHowever, their effectiveness is limited in specialized areas requiring high\\naccuracy, such as plant science, due to a lack of specific expertise in these\\nfields. This paper introduces PLLaMa, an open-source language model that\\nevolved from LLaMa-2. It's enhanced with a comprehensive database, comprising\\nmore than 1.5 million scholarly articles in plant science. This development\\nsignificantly enriches PLLaMa with extensive knowledge and proficiency in plant\\nand agricultural sciences. Our initial tests, involving specific datasets\\nrelated to plants and agriculture, show that PLLaMa substantially improves its\\nunderstanding of plant science-related topics. Moreover, we have formed an\\ninternational panel of professionals, including plant scientists, agricultural\\nengineers, and plant breeders. This team plays a crucial role in verifying the\\naccuracy of PLLaMa's responses to various academic inquiries, ensuring its\\neffective and reliable application in the field. To support further research\\nand development, we have made the model's checkpoints and source codes\\naccessible to the scientific community. These resources are available for\\ndownload at \\\\url{https://github.com/Xianjun-Yang/PLLaMa}.\", 'Combining different data sets with information on grant and fellowship\\napplications submitted to two renowned funding agencies, we are able to compare\\ntheir funding decisions (award and rejection) with scientometric performance\\nindicators across two fields of science (life sciences and social sciences).\\nThe data sets involve 671 applications in social sciences and 668 applications\\nin life sciences. In both fields, awarded applicants perform on average better\\nthan all rejected applicants. If only the most preeminent rejected applicants\\nare considered in both fields, they score better than the awardees on citation\\nimpact. With regard to productivity we find differences between the fields:\\nWhile the awardees in life sciences outperform on average the most preeminent\\nrejected applicants, the situation is reversed in social sciences.', 'From its inception, a large part of the motivation for Cognitive Science has\\nbeen the need for an interdisciplinary journal for the study of minds and\\nintelligent systems. One threat to the interdisciplinarity of Cognitive\\nScience, both the field and journal, is that it may become, or already be, too\\ndominated by psychologists. In 2005, psychology was a keyword for 51% of\\nsubmissions, followed distantly by linguistics (17%), artificial intelligence\\n(13%), neuroscience (10%), computer science (9%), and philosophy (8%). The\\nInstitute for Scientific Information (ISI) gathers data not only on how\\nindividual articles cite one another, but also on macroscopic citation patterns\\namong journals. Journals or sets of journals can be considered as proxies for\\nfields. As fields become established, they often create journals. By studying\\nthe patterns of citations among journals that cite and are cited by Cognitive\\nScience, we can better: 1) appreciate the scholarly ecology surrounding the\\njournal and the journals role within this ecology, 2) establish competitor and\\nalternate journals, and 3) determine the natural clustering of fields related\\nto cognitive science.', 'Data Science---Today, everybody and everything produces data. People produce\\nlarge amounts of data in social networks and in commercial transactions.\\nMedical, corporate, and government databases continue to grow. Sensors continue\\nto get cheaper and are increasingly connected, creating an Internet of Things,\\nand generating even more data. In every discipline, large, diverse, and rich\\ndata sets are emerging, from astrophysics, to the life sciences, to the\\nbehavioral sciences, to finance and commerce, to the humanities and to the\\narts. In every discipline people want to organize, analyze, optimize and\\nunderstand their data to answer questions and to deepen insights. The science\\nthat is transforming this ocean of data into a sea of knowledge is called data\\nscience. This lecture will discuss how data science has changed the way in\\nwhich one of the most visible challenges to public health is handled, the 2014\\nEbola outbreak in West Africa.', \"Implementation of cognitive apprenticeship in an introductory physics lab\\ngroup problem solving exercise may be mitigated by epistemic views toward\\nphysics of non-physics science majors. Quantitative pre-post data of the Force\\nConcept Inventory (FCI) and Colorado Learning Attitudes About Science Survey\\n(CLASS) of 39 students of a first-semester algebra-based introductory physics\\ncourse, while describing typical results for a traditional-format course\\noverall (g = +0.14), suggest differences in epistemic views between health\\nscience majors and life science majors which may correlate with differences in\\npre-post conceptual understanding. Audiovisual data of student lab groups\\nworking on a context-rich problem and students' written reflections described\\neach group's typical dynamics and invoked epistemic games. We examined the\\neffects of framework-based orientation (favored by biology majors) and\\nperformance-based orientation (favored by computer science, chemistry, and\\nhealth science majors) on pre-post attitude survey performance. We also\\ninvestigated possible correlations of these orientations with individual\\nquantitative survey results, and with qualitative audiovisual data of lab\\ngroups' choice of epistemic games.\", 'Normalization of citation scores using reference sets based on Web-of-Science\\nSubject Categories (WCs) has become an established (\"best\") practice in\\nevaluative bibliometrics. For example, the Times Higher Education World\\nUniversity Rankings are, among other things, based on this operationalization.\\nHowever, WCs were developed decades ago for the purpose of information\\nretrieval and evolved incrementally with the database; the classification is\\nmachine-based and partially manually corrected. Using the WC \"information\\nscience & library science\" and the WCs attributed to journals in the field of\\n\"science and technology studies,\" we show that WCs do not provide sufficient\\nanalytical clarity to carry bibliometric normalization in evaluation practices\\nbecause of \"indexer effects.\" Can the compliance with \"best practices\" be\\nreplaced with an ambition to develop \"best possible practices\"? New research\\nquestions can then be envisaged.', \"Throughout history, everyday people have contributed to science through a\\nmyriad of volunteer activities. This early participation required training and\\noften involved mentorship from scientists or senior citizen scientists (or, as\\nthey were often called, gentleman scientists). During this learning process,\\nparticipants learned how they and their data would be used both to advance\\nscience, and in some cases, advance the careers of professional collaborators.\\nModern, online citizen science, allows participation with just a few clicks,\\nand people may participate without understanding what they are contributing to.\\nToo often, they happily see what they are doing as the privilege of painting\\nTom Sawyer's fence without realizing they are actually being used as merely a\\nmeans to a scientific end. This paper discusses the ethical dilemmas that\\nplague modern citizen science, including: the issues of informed consent, such\\nas not requiring logins; the issues of coercion inherent in mandatory classroom\\nassignments requiring data submission; and the issues of using people merely as\\na means to an end that are inherent in technonationalism, and projects that do\\nnot provide utility to the users beyond the knowledge they helped. This work is\\ntested within the context of astronomy citizen science.\", 'While data science has emerged as a contentious new scientific field,\\nenormous debates and discussions have been made on it why we need data science\\nand what makes it as a science. In reviewing hundreds of pieces of literature\\nwhich include data science in their titles, we find that the majority of the\\ndiscussions essentially concern statistics, data mining, machine learning, big\\ndata, or broadly data analytics, and only a limited number of new data-driven\\nchallenges and directions have been explored. In this paper, we explore the\\nintrinsic challenges and directions inspired by comprehensively exploring the\\ncomplexities and intelligence embedded in data science problems. We focus on\\nthe research and innovation challenges inspired by the nature of data science\\nproblems as complex systems, and the methodologies for handling such systems.', 'The advent of ChatGPT by OpenAI has prompted extensive discourse on its\\npotential implications for science and higher education. While the impact on\\neducation has been a primary focus, there is limited empirical research on the\\neffects of large language models (LLMs) and LLM-based chatbots on science and\\nscientific practice. To investigate this further, we conducted a Delphi study\\ninvolving 72 experts specialising in research and AI. The study focused on\\napplications and limitations of LLMs, their effects on the science system,\\nethical and legal considerations, and the required competencies for their\\neffective use. Our findings highlight the transformative potential of LLMs in\\nscience, particularly in administrative, creative, and analytical tasks.\\nHowever, risks related to bias, misinformation, and quality assurance need to\\nbe addressed through proactive regulation and science education. This research\\ncontributes to informed discussions on the impact of generative AI in science\\nand helps identify areas for future action.', 'The advancement of natural language processing has paved the way for\\nautomated scoring systems in various languages, such as German (e.g., German\\nBERT [G-BERT]). Automatically scoring written responses to science questions in\\nGerman is a complex task and challenging for standard G-BERT as they lack\\ncontextual knowledge in the science domain and may be unaligned with student\\nwriting styles. This paper developed a contextualized German Science Education\\nBERT (G-SciEdBERT), an innovative large language model tailored for scoring\\nGerman-written responses to science tasks. Using G-BERT, we pre-trained\\nG-SciEdBERT on a corpus of 50K German written science responses with 5M tokens\\nto the Programme for International Student Assessment (PISA) 2015. We\\nfine-tuned G-SciEdBERT on 59 assessment items and examined the scoring\\naccuracy. We then compared its performance with G-BERT. Our findings reveal a\\nsubstantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a\\n10% increase of quadratic weighted kappa compared to G-BERT (mean accuracy\\ndifference = 0.096, SD = 0.024). These insights underline the significance of\\nspecialized language models like G-SciEdBERT, which is trained to enhance the\\naccuracy of automated scoring, offering a substantial contribution to the field\\nof AI in education.', 'We study the computational model of polygraphs. For that, we consider\\npolygraphic programs, a subclass of these objects, as a formal description of\\nfirst-order functional programs. We explain their semantics and prove that they\\nform a Turing-complete computational model. Their algebraic structure is used\\nby analysis tools, called polygraphic interpretations, for complexity analysis.\\nIn particular, we delineate a subclass of polygraphic programs that compute\\nexactly the functions that are Turing-computable in polynomial time.', 'Secure sum computation of private data inputs is an important component of\\nSecure Multi party Computation (SMC).In this paper we provide a protocol to\\ncompute the sum of individual data inputs with zero probability of data\\nleakage. In our proposed protocol we break input of each party into number of\\nsegments and change the arrangement of the parties such that in each round of\\nthe computation the neighbors are changed. In this protocol it becomes\\nimpossible for semi honest parties to know the private data of some other\\nparty.', 'Reversibility is a key issue in the interface between computation and\\nphysics, and of growing importance as miniaturization progresses towards its\\nphysical limits. Most foundational work on reversible computing to date has\\nfocussed on simulations of low-level machine models. By contrast, we develop a\\nmore structural approach. We show how high-level functional programs can be\\nmapped compositionally (i.e. in a syntax-directed fashion) into a simple kind\\nof automata which are immediately seen to be reversible. The size of the\\nautomaton is linear in the size of the functional term. In mathematical terms,\\nwe are building a concrete model of functional computation. This construction\\nstems directly from ideas arising in Geometry of Interaction and Linear\\nLogic---but can be understood without any knowledge of these topics. In fact,\\nit serves as an excellent introduction to them. At the same time, an\\ninteresting logical delineation between reversible and irreversible forms of\\ncomputation emerges from our analysis.', \"Quantum cybernetics and its connections to complex quantum systems science is\\naddressed from the perspective of complex quantum computing systems. In this\\nway, the notion of an autonomous quantum computing system is introduced in\\nregards to quantum artificial intelligence, and applied to quantum artificial\\nneural networks, considered as autonomous quantum computing systems, which\\nleads to a quantum connectionist framework within quantum cybernetics for\\ncomplex quantum computing systems. Several examples of quantum feedforward\\nneural networks are addressed in regards to Boolean functions' computation,\\nmultilayer quantum computation dynamics, entanglement and quantum\\ncomplementarity. The examples provide a framework for a reflection on the role\\nof quantum artificial neural networks as a general framework for addressing\\ncomplex quantum systems that perform network-based quantum computation,\\npossible consequences are drawn regarding quantum technologies, as well as\\nfundamental research in complex quantum systems science and quantum biology.\", \"Those working on policy, digital ethics and governance often refer to issues\\nin `computer science', that includes, but is not limited to, common subfields\\nof Artificial Intelligence (AI), Computer Science (CS) Computer Security\\n(InfoSec), Computer Vision (CV), Human Computer Interaction (HCI), Information\\nSystems, (IS), Machine Learning (ML), Natural Language Processing (NLP) and\\nSystems Architecture. Within this framework, this paper is a preliminary\\nexploration of two hypotheses, namely 1) Each community has differing inclusion\\nof minoritised groups (using women as our test case); and 2) Even where women\\nexist in a community, they are not published representatively. Using data from\\n20,000 research records, totalling 503,318 names, preliminary data supported\\nour hypothesis. We argue that ACM has an ethical duty of care to its community\\nto increase these ratios, and to hold individual computing communities to\\naccount in order to do so, by providing incentives and a regular reporting\\nsystem, in order to uphold its own Code.\", 'Advances in high-throughput simulation (HTS) software enabled computational\\ndatabases and big data to become common resources in materials science.\\nHowever, while computational power is increasingly larger, software packages\\norchestrating complex workflows in heterogeneous environments are scarce. This\\npaper introduces mkite, a Python package for performing HTS in distributed\\ncomputing environments. The mkite toolkit is built with the server-client\\npattern, decoupling production databases from client runners. When used in\\ncombination with message brokers, mkite enables any available client to perform\\ncalculations without prior hardware specification on the server side.\\nFurthermore, the software enables the creation of complex workflows with\\nmultiple inputs and branches, facilitating the exploration of combinatorial\\nchemical spaces. Software design principles are discussed in detail,\\nhighlighting the usefulness of decoupling simulations and data management tasks\\nto diversify simulation environments. To exemplify how mkite handles simulation\\nworkflows of combinatorial systems, case studies on zeolite synthesis and\\nsurface catalyst discovery are provided. Finally, key differences with other\\natomistic simulation workflows are outlined. The mkite suite can enable HTS in\\ndistributed computing environments, simplifying workflows with heterogeneous\\nhardware and software, and helping deployment of calculations at scale.', \"In this chapter we will argue that studying such multi-scale multi-science\\nsystems gives rise to inherently hybrid models containing many different\\nalgorithms best serviced by different types of computing environments (ranging\\nfrom massively parallel computers, via large-scale special purpose machines to\\nclusters of PC's) whose total integrated computing capacity can easily reach\\nthe PFlop/s scale. Such hybrid models, in combination with the by now\\ninherently distributed nature of the data on which the models `feed' suggest a\\ndistributed computing model, where parts of the multi-scale multi-science model\\nare executed on the most suitable computing environment, and/or where the\\ncomputations are carried out close to the required data (i.e. bring the\\ncomputations to the data instead of the other way around). We presents an\\nestimate for the compute requirements to simulate the Galaxy as a typical\\nexample of a multi-scale multi-physics application, requiring distributed\\nPetaflop/s computational power.\", 'Computability logic is a formal theory of computational tasks and resources.\\nIts formulas represent interactive computational problems, logical operators\\nstand for operations on computational problems, and validity of a formula is\\nunderstood as being a scheme of problems that always have algorithmic\\nsolutions. A comprehensive online source on the subject is available at\\nhttp://www.cis.upenn.edu/~giorgi/cl.html . The earlier article \"Propositional\\ncomputability logic I\" proved soundness and completeness for the (in a sense)\\nminimal nontrivial fragment CL1 of computability logic. The present paper\\nextends that result to the significantly more expressive propositional system\\nCL2. What makes CL2 more expressive than CL1 is the presence of two sorts of\\natoms in its language: elementary atoms, representing elementary computational\\nproblems (i.e. predicates), and general atoms, representing arbitrary\\ncomputational problems. CL2 conservatively extends CL1, with the latter being\\nnothing but the general-atom-free fragment of the former.', 'The special theme of DCM 2009, co-located with ICALP 2009, concerned\\nComputational Models From Nature, with a particular emphasis on computational\\nmodels derived from physics and biology. The intention was to bring together\\ndifferent approaches - in a community with a strong foundational background as\\nproffered by the ICALP attendees - to create inspirational cross-boundary\\nexchanges, and to lead to innovative further research. Specifically DCM 2009\\nsought contributions in quantum computation and information, probabilistic\\nmodels, chemical, biological and bio-inspired ones, including spatial models,\\ngrowth models and models of self-assembly. Contributions putting to the test\\nlogical or algorithmic aspects of computing (e.g., continuous computing with\\ndynamical systems, or solid state computing models) were also very much\\nwelcomed.', 'In June 1965, Sister Mary Kenneth Keller, BVM, received the first US PhD in\\nComputer Science, and this paper outlines her life and accomplishments. As a\\nscholar, she has the distinction of being an early advocate of\\nlearning-by-example in artificial intelligence. Her main scholarly contribution\\nwas in shaping computer science education in high schools and small colleges.\\nShe was an evangelist for viewing the computer as a symbol manipulator, for\\nproviding computer literacy to everyone, and for the use of computers in\\nservice to humanity. She was far ahead of her time in working to ensure a place\\nfor women in technology and in eliminating barriers preventing their\\nparticipation, such as poor access to education and daycare. She was a strong\\nand spirited woman, a visionary in seeing how computers would revolutionize our\\nlives. A condensation of this paper appeared as, ``The Legacy of Mary Kenneth\\nKeller, First U.S. Ph.D. in Computer Science,\" Jennifer Head and Dianne P.\\nO\\'Leary, IEEE Annals of the History of Computing 45(1):55--63, January-March\\n2023.', \"What makes quantum information science a science? These notes explore the\\nidea that quantum information science may offer a powerful approach to the\\nstudy of complex quantum systems. We discuss how to quantify complexity in\\nquantum systems, and argue that there are two qualitatively different types of\\ncomplex quantum system. We also explore ways of understanding complex quantum\\ndynamics by quantifying the strength of a quantum dynamical operation as a\\nphysical resource. This is the text for a talk at the ``Sixth International\\nConference on Quantum Communication, Measurement and Computing'', held at MIT,\\nJuly 2002. Viewgraphs for the talk may be found at http://www.qinfo.org/talks/.\", 'We provide a solution for elementary science test using instructional\\nmaterials. We posit that there is a hidden structure that explains the\\ncorrectness of an answer given the question and instructional materials and\\npresent a unified max-margin framework that learns to find these hidden\\nstructures (given a corpus of question-answer pairs and instructional\\nmaterials), and uses what it learns to answer novel elementary science\\nquestions. Our evaluation shows that our framework outperforms several strong\\nbaselines.', \"Developers in data science and other domains frequently use computational\\nnotebooks to create exploratory analyses and prototype models. However, they\\noften struggle to incorporate existing software engineering tooling into these\\nnotebook-based workflows, leading to fragile development processes. We\\nintroduce Assembl\\\\'{e}, a new development environment for collaborative data\\nscience projects, in which promising code fragments of data science pipelines\\ncan be contributed as pull requests to an upstream repository entirely from\\nwithin JupyterLab, abstracting away low-level version control tool usage. We\\ndescribe the design and implementation of Assembl\\\\'{e} and report on a user\\nstudy of 23 data scientists.\", 'The objective of this short report is to reconsider the subject of\\nbioinformatics as just being a tool of experimental biological science. To do\\nthat, we introduce three examples to show how bioinformatics could be\\nconsidered as an experimental science. These examples show how the development\\nof theoretical biological models generates experimentally verifiable computer\\nhypotheses, which necessarily must be validated by experiments in vitro or in\\nvivo.', 'Service composition has become commonplace nowadays, in large part due to the\\nincreased complexity of software and supporting networks. Composition can be of\\nmany types, for instance sequential, prioritising, non-deterministic. However,\\na fundamental feature of the services to be composed consists in their\\ndependencies with respect to each other. In this paper we propose a theory of\\nservice dependency, modelled around a dependency operator in the Action Systems\\nformalism. We analyze its properties, composition behaviour, and refinement\\nconditions with accompanying examples.', 'Scientometric techniques have been remarkably successful at mapping science\\nbut they face important difficulties when mapping research for societal\\nproblems possibly because they they are derived only from scientific documents\\nand thus do not rely on non-academic expert knowledge. Here we aim to explore\\nhow ontologies can be used in science mapping, thus enriching current\\nalgorithmic techniques with systematic domain expert knowledge. This study\\nintroduces the methodology behind the construction of an ontology and tests\\npotential uses in science mapping. We use obesity as a topic of case study.', 'A strong link between complexity theory and literature is possible, i.e.\\nfeasible, under one proviso, namely that total novels be considered. However,\\nneither in literature at large nor in complexity science has been literature\\nseriously taken into consideration. This paper argues that a total novel is\\nmost conspicuous example of a complex system. The argument is supported by a\\nclear characterization of what a total novel is and entails. Science and\\nliterature can be thus complemented and developed, hand in hand.', 'This article sets out our perspective on how to begin the journey of\\ndecolonising computational fields, such as data and cognitive sciences. We see\\nthis struggle as requiring two basic steps: a) realisation that the present-day\\nsystem has inherited, and still enacts, hostile, conservative, and oppressive\\nbehaviours and principles towards women of colour (WoC); and b) rejection of\\nthe idea that centering individual people is a solution to system-level\\nproblems. The longer we ignore these two steps, the more \"our\" academic system\\nmaintains its toxic structure, excludes, and harms Black women and other\\nminoritised groups. This also keeps the door open to discredited pseudoscience,\\nlike eugenics and physiognomy. We propose that grappling with our fields\\'\\nhistories and heritage holds the key to avoiding mistakes of the past. For\\nexample, initiatives such as \"diversity boards\" can still be harmful because\\nthey superficially appear reformatory but nonetheless center whiteness and\\nmaintain the status quo. Building on the shoulders of many WoC\\'s work, who have\\nbeen paving the way, we hope to advance the dialogue required to build both a\\ngrass-roots and a top-down re-imagining of computational sciences -- including\\nbut not limited to psychology, neuroscience, cognitive science, computer\\nscience, data science, statistics, machine learning, and artificial\\nintelligence. We aspire for these fields to progress away from their stagnant,\\nsexist, and racist shared past into carving and maintaining an ecosystem where\\nboth a diverse demographics of researchers and scientific ideas that critically\\nchallenge the status quo are welcomed.', 'Machine learning (ML) algorithms are gaining increased importance in many\\nacademic and industrial applications, and such algorithms are, accordingly,\\nbecoming common components in computer science curricula. Learning ML is\\nchallenging not only due to its complex mathematical and algorithmic aspects,\\nbut also due to a) the complexity of using correctly these algorithms in the\\ncontext of real-life situations and b) the understanding of related social and\\nethical issues. Cognitive biases are phenomena of the human brain that may\\ncause erroneous perceptions and irrational decision-making processes. As such,\\nthey have been researched thoroughly in the context of cognitive psychology and\\ndecision making; they do, however, have important implications for computer\\nscience education as well. One well-known cognitive bias, first described by\\nKahneman and Tversky, is the base rate neglect bias, according to which humans\\nfail to consider the base rate of the underlying phenomena when evaluating\\nconditional probabilities. In this paper, we explore the expression of the base\\nrate neglect bias in ML education. Specifically, we show that about one third\\nof students in an Introduction to ML course, from varied backgrounds (computer\\nscience students and teachers, data science, engineering, social science and\\ndigital humanities), fail to correctly evaluate ML algorithm performance due to\\nthe base rate neglect bias. This failure rate should alert educators and\\npromote the development of new pedagogical methods for teaching ML algorithm\\nperformance.', \"Understanding the relationship between the composition of a research team and\\nthe potential impact of their research papers is crucial as it can steer the\\ndevelopment of new science policies for improving the research enterprise.\\nNumerous studies assess how the characteristics and diversity of research teams\\ncan influence their performance across several dimensions: ethnicity,\\ninternationality, size, and others. In this paper, we explore the impact of\\ndiversity in terms of the authors' expertise. To this purpose, we retrieved\\n114K papers in the field of Computer Science and analysed how the diversity of\\nresearch fields within a research team relates to the number of citations their\\npapers received in the upcoming 5 years. The results show that two different\\nmetrics we defined, reflecting the diversity of expertise, are significantly\\nassociated with the number of citations. This suggests that, at least in\\nComputer Science, diversity of expertise is key to scientific impact.\", 'We present a survey on analog models of computations. Analog can be\\nunderstood both as computing by analogy, or as working on the continuum. We\\nconsider both approaches, often intertwined, with a point of view mostly\\noriented by computation theory.', 'We study the computational complexity of computing or approximating a\\nquasi-proper equilibrium for a given finite extensive form game of perfect\\nrecall. We show that the task of computing a symbolic quasi-proper equilibrium\\nis $\\\\mathrm{PPAD}$-complete for two-player games. For the case of zero-sum\\ngames we obtain a polynomial time algorithm based on Linear Programming. For\\ngeneral $n$-player games we show that computing an approximation of a\\nquasi-proper equilibrium is $\\\\mathrm{FIXP}_a$-complete.', 'As quantum computers mature, the applicability in practice becomes more\\nimportant. Many uses of quantum computers will be hybrid, with classical\\ncomputers still playing an important role in operating and using the quantum\\ncomputer. The term hybrid is however diffuse and multi-interpretable. In this\\nwork we define two classes of hybrid quantum-classical computing: vertical and\\nhorizontal. The first is application-agnostic and concerns using quantum\\ncomputers. The second is application-specific and concerns running an\\nalgorithm. For both, we give a further subdivision in different types of hybrid\\nquantum-classical computing and we coin terms for them.', 'We consider quantifier-free spatial logics, designed for qualitative spatial\\nrepresentation and reasoning in AI, and extend them with the means to represent\\ntopological connectedness of regions and restrict the number of their connected\\ncomponents. We investigate the computational complexity of these logics and\\nshow that the connectedness constraints can increase complexity from NP to\\nPSpace, ExpTime and, if component counting is allowed, to NExpTime.', 'We present a both simple and comprehensive graphical calculus for quantum\\ncomputing. In particular, we axiomatize the notion of an environment, which\\ntogether with the earlier introduced axiomatic notion of classical structure\\nenables us to define classical channels, quantum measurements and classical\\ncontrol. If we moreover adjoin the earlier introduced axiomatic notion of\\ncomplementarity, we obtain sufficient structural power for constructive\\nrepresentation and correctness derivation of typical quantum informatic\\nprotocols.', \"Dependently typed programs contain an excessive amount of static terms which\\nare necessary to please the type checker but irrelevant for computation. To\\nseparate static and dynamic code, several static analyses and type systems have\\nbeen put forward. We consider Pfenning's type theory with irrelevant\\nquantification which is compatible with a type-based notion of equality that\\nrespects eta-laws. We extend Pfenning's theory to universes and large\\neliminations and develop its meta-theory. Subject reduction, normalization and\\nconsistency are obtained by a Kripke model over the typed equality judgement.\\nFinally, a type-directed equality algorithm is described whose completeness is\\nproven by a second Kripke model.\", 'Category Theory is a well-known powerful mathematical modeling language with\\na wide area of applications in mathematics and computer science, including\\nespecially the semantical foundations of topics in software science and\\ndevelopment. Categorical methods are already well established for the\\nsemantical foundation of type theory (cartesian closed categories), data type\\nspecification frameworks (institutions) and graph transformation (adhesive high\\nlevel replacement categories).\\n  It is the intention of the ACCAT Workshops on Applied and Computational\\nCategory Theory to bring together leading researchers in these areas with those\\nin software science and development in order to transfer categorical concepts\\nand theories in both directions. The workshops aims to represent a forum for\\nresearchers and practitioners who are interested in an exchange of ideas,\\nnotions, and techniques for different applications of category theory.\\n  The seventh ACCAT workshop on Applied and Computational Category Theory 2012\\nwas held in Tallinn, Estonia on the 1st of April 2012 as a satellite event of\\nETAPS 2012. This issue contains the full version of one of the invited talks as\\nwell as the submitted papers, which cover a wide range of applications of\\ncategory theory, from model-driven engineering over transition systems in\\nstochastic processes to transformations in M-adhesive categories.', 'A large-scale, up-to-date analysis of Computer Science literature (11.8M\\npapers through 2019) reveals that, if trends from the last 50 years continue,\\nparity between the number of male and female authors will not be reached in\\nthis century. In contrast, parity is projected to be reached within two to\\nthree decades or may have already been reached in other fields of study like\\nMedicine or Sociology. Our analysis of collaboration trends in Computer Science\\nreveals shifts in the size of the collaboration gap between authors of\\ndifferent perceived genders. The gap is persistent but shrinking, corresponding\\nto a slow increase in the rate of cross-gender collaborations over time.\\nTogether, these trends describe a persistent gender gap in the authorship of\\nComputer Science literature that may not close without systematic intervention.', \"The hardware/software boundary in modern heterogeneous multicore computers is\\nincreasingly complex, and diverse across different platforms. A single memory\\naccess by a core or DMA engine traverses multiple hardware translation and\\ncaching steps, and the destination memory cell or register often appears at\\ndifferent physical addresses for different cores. Interrupts pass through a\\ncomplex topology of interrupt controllers and remappers before delivery to one\\nor more cores, each with specific constraints on their configurations. System\\nsoftware must not only correctly understand the specific hardware at hand, but\\nalso configure it appropriately at runtime. We propose a formal model of\\naddress spaces and resources in a system that allows us to express and verify\\ninvariants of the system's runtime configuration, and illustrate (and motivate)\\nit with several real platforms we have encountered in the process of OS\\nimplementation.\", 'Generating big data pervades much of physics. But some problems, which we\\ncall extreme data problems, are too large to be treated within big data\\nscience. The nonequilibrium quantum many-body problem on a lattice is just such\\na problem, where the Hilbert space grows exponentially with system size and\\nrapidly becomes too large to fit on any computer (and can be effectively\\nthought of as an infinite-sized data set). Nevertheless, much progress has been\\nmade with computational methods on this problem, which serve as a paradigm for\\nhow one can approach and attack extreme data problems. In addition, viewing\\nthese physics problems from a computer-science perspective leads to new\\napproaches that can be tried to solve them more accurately and for longer\\ntimes. We review a number of these different ideas here.', \"Denoising filters, such as bilateral, guided, and total variation filters,\\napplied to images on general graphs may require repeated application if noise\\nis not small enough. We formulate two acceleration techniques of the resulted\\niterations: conjugate gradient method and Nesterov's acceleration. We\\nnumerically show efficiency of the accelerated nonlinear filters for image\\ndenoising and demonstrate 2-12 times speed-up, i.e., the acceleration\\ntechniques reduce the number of iterations required to reach a given peak\\nsignal-to-noise ratio (PSNR) by the above indicated factor of 2-12.\", 'Inhibition is one of the core concepts in Cognitive Psychology. The idea of\\ninhibitory mechanisms actively weakening representations in the human mind has\\ninspired a great number of studies in various research domains. In contrast,\\nComputer Science only recently has begun to consider inhibition as a second\\nbasic processing quality beside activation. Here, we review psychological\\nresearch on inhibition in memory and link the gained insights with the current\\nefforts in Computer Science of incorporating inhibitory principles for\\noptimizing information retrieval in Personal Information Management. Four\\ncommon aspects guide this review in both domains: 1. The purpose of inhibition\\nto increase processing efficiency. 2. Its relation to activation. 3. Its links\\nto contexts. 4. Its temporariness. In summary, the concept of inhibition has\\nbeen used by Computer Science for enhancing software in various ways already.\\nYet, we also identify areas for promising future developments of inhibitory\\nmechanisms, particularly context inhibition.', 'How should software engineering be adapted for Computational Science (CS)? If\\nwe understood that, then we could better support software sustainability,\\nverifiability, reproducibility, comprehension, and usability for CS community.\\nFor example, improving the maintainability of the CS code could lead to: (a)\\nfaster adaptation of scientific project simulations to new and efficient\\nhardware (multi-core and heterogeneous systems); (b) better support for larger\\nteams to co-ordinate (through integration with interdisciplinary teams); and\\n(c) an extended capability to model complex phenomena.\\n  In order to better understand computational science, this paper uses\\nquantitative evidence (from 59 CS projects in Github) to check 13 published\\nbeliefs about CS. These beliefs reflect on (a) the nature of scientific\\nchallenges; (b) the implications of limitations of computer hardware; and (c)\\nthe cultural environment of scientific software development. What we found was,\\nusing this new data from Github, only a minority of those older beliefs can be\\nendorsed. More than half of the pre-existing beliefs are dubious, which leads\\nus to conclude that the nature of CS software development is changing.\\n  Further, going forward, this has implications for (1) what kinds of tools we\\nwould propose to better support computational science and (2) research\\ndirections for both communities.', 'High school science classrooms across the United States are answering calls\\nto make computation a part of science learning. The problem is that there is\\nlittle known about the barriers to learning that computation might bring to a\\nscience classroom or about how to help students overcome these challenges. This\\ncase study explores these challenges from the perspectives of students in a\\nhigh school physics classroom with a newly revamped, computation-integrated\\ncurriculum. Focusing mainly on interviews to center the perspectives of\\nstudents, we found that computation is a double-edged sword: It can make\\nscience learning more authentic for students who are familiar with it, but it\\ncan also generate frustration and an aversion towards physics for students who\\nare not.', 'Replication crises have shaken the scientific landscape during the last\\ndecade. As potential solutions, open science practices were heavily discussed\\nand have been implemented with varying success in different disciplines. We\\nargue that computational-x disciplines such as computational social science,\\nare also susceptible for the symptoms of the crises, but in terms of\\nreproducibility. We expand the binary definition of reproducibility into a tier\\nsystem which allows increasing levels of reproducibility based on external\\nverfiability to counteract the practice of open-washing. We provide solutions\\nfor barriers in Computational Social Science that hinder researchers from\\nobtaining the highest level of reproducibility, including the use of alternate\\ndata sources and considering reproducibility proactively.', \"Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and\\ndominate the current discourse. Their transformative capabilities have led to a\\nparadigm shift in how we interact with and utilize (text-based) information.\\nEach day, new possibilities to leverage the capabilities of these models\\nemerge. This paper presents findings on the performance of different large\\nlanguage models in a university of applied sciences' undergraduate computer\\nscience degree program. Our primary objective is to assess the effectiveness of\\nthese models within the curriculum by employing them as educational aids. By\\nprompting the models with lecture material, exercise tasks, and past exams, we\\naim to evaluate their proficiency across different computer science domains. We\\nshowcase the strong performance of current large language models while\\nhighlighting limitations and constraints within the context of such a degree\\nprogram. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10\\ntested modules, BingAI achieved 68.4%, and LLaMa, in the 65 billion parameter\\nvariant, 20%. Despite these convincing results, even GPT-4.0 would not pass the\\ndegree program - due to limitations in mathematical calculations.\", 'Mental health conversational agents (a.k.a. chatbots) are widely studied for\\ntheir potential to offer accessible support to those experiencing mental health\\nchallenges. Previous surveys on the topic primarily consider papers published\\nin either computer science or medicine, leading to a divide in understanding\\nand hindering the sharing of beneficial knowledge between both domains. To\\nbridge this gap, we conduct a comprehensive literature review using the PRISMA\\nframework, reviewing 534 papers published in both computer science and\\nmedicine. Our systematic review reveals 136 key papers on building mental\\nhealth-related conversational agents with diverse characteristics of modeling\\nand experimental design techniques. We find that computer science papers focus\\non LLM techniques and evaluating response quality using automated metrics with\\nlittle attention to the application while medical papers use rule-based\\nconversational agents and outcome metrics to measure the health outcomes of\\nparticipants. Based on our findings on transparency, ethics, and cultural\\nheterogeneity in this review, we provide a few recommendations to help bridge\\nthe disciplinary divide and enable the cross-disciplinary development of mental\\nhealth conversational agents.', \"Computational science has seen in the last decades a spectacular rise in the\\nscope, breadth, and depth of its efforts. Notwithstanding this prevalence and\\nimpact, it is often still performed using the renaissance model of individual\\nartisans gathered in a workshop, under the guidance of an established\\npractitioner. Great benefits could follow instead from adopting concepts and\\ntools coming from computer science to manage, preserve, and share these\\ncomputational efforts. We illustrate here our paradigm sustaining such vision,\\nbased around the four pillars of Automation, Data, Environment, and Sharing. We\\nthen discuss its implementation in the open-source AiiDA platform\\n(http://www.aiida.net), that has been tuned first to the demands of\\ncomputational materials science. AiiDA's design is based on directed acyclic\\ngraphs to track the provenance of data and calculations, and ensure\\npreservation and searchability. Remote computational resources are managed\\ntransparently, and automation is coupled with data storage to ensure\\nreproducibility. Last, complex sequences of calculations can be encoded into\\nscientific workflows. We believe that AiiDA's design and its sharing\\ncapabilities will encourage the creation of social ecosystems to disseminate\\ncodes, data, and scientific workflows.\", \"This document introduces the Generalized Moving Peaks Benchmark (GMPB), a\\ntool for generating continuous dynamic optimization problem instances that is\\nused for the CEC 2024 Competition on Dynamic Optimization. GMPB is adept at\\ngenerating landscapes with a broad spectrum of characteristics, offering\\neverything from unimodal to highly multimodal landscapes and ranging from\\nsymmetric to highly asymmetric configurations. The landscapes also vary in\\ntexture, from smooth to highly irregular surfaces, encompassing diverse degrees\\nof variable interaction and conditioning. This document delves into the\\nintricacies of GMPB, detailing the myriad ways in which its parameters can be\\ntuned to produce these diverse landscape characteristics. GMPB's MATLAB\\nimplementation is available on the EDOLAB Platform.\", 'We use erasers-like basic operations on words to construct a set that is both\\nBorel and above Delta^0_omega, built as a set V^\\\\omega where V is a language of\\nfinite words accepted by a pushdown automaton. In particular, this gives a\\nfirst example of an omega-power of a context free language which is a Borel set\\nof infinite rank.', \"This Master of Science in Computer and Information Sciences (MICS) is an\\ninternational accredited master program that has been initiated in 2004 and\\nstarted in September 2005. MICS is a research-oriented academic study of 4\\nsemesters and a continuation of the Bachelor towards the PhD. It is completely\\ntaught in English, supported by lecturers coming from more than ten different\\ncountries. This report compass a description of its underlying architecture,\\ndescribes some implementation details and gives a presentation of diverse\\nexperiences and results. As the program has been designed and implemented right\\nafter the creation of the University, the significance of the program is\\nmoreover a self-discovery of the computer science department, which has finally\\nled to the creation of the today's research institutes and research axes.\", 'We propose an explanatory and computational theory of transformative\\ndiscoveries in science. The theory is derived from a recurring theme found in a\\ndiverse range of scientific change, scientific discovery, and knowledge\\ndiffusion theories in philosophy of science, sociology of science, social\\nnetwork analysis, and information science. The theory extends the concept of\\nstructural holes from social networks to a broader range of associative\\nnetworks found in science studies, especially including networks that reflect\\nunderlying intellectual structures such as co-citation networks and\\ncollaboration networks. The central premise is that connecting otherwise\\ndisparate patches of knowledge is a valuable mechanism of creative thinking in\\ngeneral and transformative scientific discovery in particular.', 'Quantitative social science is not only about regression analysis or, in\\ngeneral, data inference. Computer simulations of social mechanisms have a\\n60-year long history. They have been used for many different purposes -- to\\ntest scenarios, test the consistency of descriptive theories (proof-of-concept\\nmodels), explore emergent phenomena, forecast, etc. In this essay, we sketch\\nthese historical developments, the role of mechanistic models in the social\\nsciences, and the influences from the natural and formal sciences. We argue\\nthat mechanistic computational models form a common ground for social and\\nnatural sciences and look forward to possible future information flow across\\nthe social-natural divide.', 'We describe an introductory data science course, entitled Introduction to\\nData Science, offered at the University of Illinois at Urbana-Champaign. The\\ncourse introduced general programming concepts by using the Python programming\\nlanguage with an emphasis on data preparation, processing, and presentation.\\nThe course had no prerequisites, and students were not expected to have any\\nprogramming experience. This introductory course was designed to cover a wide\\nrange of topics, from the nature of data, to storage, to visualization, to\\nprobability and statistical analysis, to cloud and high performance computing,\\nwithout becoming overly focused on any one subject. We conclude this article\\nwith a discussion of lessons learned and our plans to develop new data science\\ncourses.', 'Currently it is widely accepted that the language of science is mathematics.\\nThis book explores an alternative idea where the future of science is based on\\nthe language of algorithms and programs. How such a language can actually be\\nimplemented in the sciences is outlined in some detail. We start by\\nconstructing a simple formal system where statements are represented as\\nprograms and inference is based on computability as opposed to the classical\\nnotion of truth value assignments.\\n  The focus is on theories where the intrinsic properties and dynamic state of\\nreal world objects can be defined in terms of information and subject to laws\\nbased on simple deterministic rules and finite state arithmetic. Such models,\\nit is argued, not only offer alternative simulation tools, as opposed to those\\nbased on discrete approximations of conventional continuum theories, but in\\nthemselves can be regarded as a language that describes the physical laws at a\\nfundamental level. This book does not examine any specific application in\\ndetail but rather attempts to lay down a foundation for the validation of such\\ntheories by employing the inference scheme based on computability logic.', 'In the last few years, contributions of the general public in scientific\\nprojects has increased due to the advancement of communication and computing\\ntechnologies. Internet played an important role in connecting scientists and\\nvolunteers who are interested in participating in their scientific projects.\\nHowever, despite potential benefits, only a limited number of crowdsourcing\\nbased large-scale science (citizen science) projects have been deployed due to\\nthe complexity involved in setting them up and running them. In this paper, we\\npresent CitizenGrid - an online middleware platform which addresses security\\nand deployment complexity issues by making use of cloud computing and\\nvirtualisation technologies. CitizenGrid incentivises scientists to make their\\nsmall-to-medium scale applications available as citizen science projects by: 1)\\nproviding a directory of projects through a web-based portal that makes\\napplications easy to discover; 2) providing flexibility to participate in,\\nmonitor, and control multiple citizen science projects from a common interface;\\n3) supporting diverse categories of citizen science projects. The paper\\ndescribes the design, development and evaluation of CitizenGrid and its use\\ncases.', 'Co-authorship in publications within a discipline uncovers interesting\\nproperties of the analysed field. We represent collaboration in academic papers\\nof computer science in terms of differently grained networks, including those\\nsub-networks that emerge from conference and journal co-authorship only. We\\ntake advantage of the network science paraphernalia to take a picture of\\ncomputer science collaboration including all papers published in the field\\nsince 1936. We investigate typical bibliometric properties like scientific\\nproductivity of authors and collaboration level in papers, as well as\\nlarge-scale network properties like reachability and average separation\\ndistance among scholars, distribution of the number of scholar collaborators,\\nnetwork resilience and dependence on star collaborators, network clustering,\\nand network assortativity by number of collaborators.', 'We review design and development decisions and their impact for the open\\nsource code Nmag from a software engineering in computational science point of\\nview. We summarise lessons learned and recommendations for future computational\\nscience projects. Key lessons include that encapsulating the simulation\\nfunctionality in a library of a general purpose language, here Python, provides\\ngreat flexibility in using the software. The choice of Python for the top-level\\nuser interface was very well received by users from the science and engineering\\ncommunity. The from-source installation in which required external libraries\\nand dependencies are compiled from a tarball was remarkably robust. In places,\\nthe code is a lot more ambitious than necessary, which introduces unnecessary\\ncomplexity and reduces main- tainability. Tests distributed with the package\\nare useful, although more unit tests and continuous integration would have been\\ndesirable. The detailed documentation, together with a tutorial for the usage\\nof the system, was perceived as one of its main strengths by the community.', \"Community detection is a major issue in network analysis. This paper combines\\na socio-historical approach with an experimental reconstruction of programs to\\ninvestigate the early automation of clique detection algorithms, which remains\\none of the unsolved NP-complete problems today. The research led by the\\narchaeologist Jean-Claude Gardin from the 1950s on non-numerical information\\nand graph analysis is retraced to demonstrate the early contributions of social\\nsciences and humanities. The limited recognition and reception of Gardin's\\ninnovative computer application to the humanities are addressed through two\\nfactors, in addition to the effects of historiography and bibliographies on the\\nrecording, discoverability, and reuse of scientific productions: 1) funding\\npolicies, evidenced by the transfer of research effort on graph applications\\nfrom temporary interdisciplinary spaces to disciplinary organizations related\\nto the then-emerging field of computer science; and 2) the erratic careers of\\nalgorithms, in which efficiency, flaws, corrections, and authors' status, were\\ndetermining factors.\", 'Epistemic logics of intensional groups lift the assumption that membership in\\na group of agents is common knowledge. Instead of being represented directly as\\na set of agents, intensional groups are represented by a property that may\\nchange its extension from world to world. Several authors have considered\\nversions of the intensional group framework where group-specifying properties\\nare articulated using structured terms of a language, such as the language of\\nBoolean algebras or of description logic. In this paper we formulate a general\\nsemantic framework for epistemic logics of structured intensional groups,\\ndevelop the basic theory leading to completeness-via-canonicity results, and\\nshow that several frameworks presented in the literature correspond to special\\ncases of the general framework.', 'In response to public scrutiny of data-driven algorithms, the field of data\\nscience has adopted ethics training and principles. Although ethics can help\\ndata scientists reflect on certain normative aspects of their work, such\\nefforts are ill-equipped to generate a data science that avoids social harms\\nand promotes social justice. In this article, I argue that data science must\\nembrace a political orientation. Data scientists must recognize themselves as\\npolitical actors engaged in normative constructions of society and evaluate\\ntheir work according to its downstream impacts on people\\'s lives. I first\\narticulate why data scientists must recognize themselves as political actors.\\nIn this section, I respond to three arguments that data scientists commonly\\ninvoke when challenged to take political positions regarding their work. In\\nconfronting these arguments, I describe why attempting to remain apolitical is\\nitself a political stance--a fundamentally conservative one--and why data\\nscience\\'s attempts to promote \"social good\" dangerously rely on unarticulated\\nand incrementalist political assumptions. I then propose a framework for how\\ndata science can evolve toward a deliberative and rigorous politics of social\\njustice. I conceptualize the process of developing a politically engaged data\\nscience as a sequence of four stages. Pursuing these new approaches will\\nempower data scientists with new methods for thoughtfully and rigorously\\ncontributing to social justice.', \"Online Citizen Science platforms are good examples of socio-technical systems\\nwhere technology-enabled interactions occur between scientists and the general\\npublic (volunteers). Citizen Science platforms usually host multiple Citizen\\nScience projects, and allow volunteers to choose the ones to participate in.\\nRecent work in the area has demonstrated a positive feedback loop between\\nparticipation and learning and creativity in Citizen Science projects, which is\\none of the motivating factors both for scientists and the volunteers. This\\nemphasises the importance of creating successful Citizen Science platforms,\\nwhich support this feedback process, and enable enhanced learning and\\ncreativity to occur through knowledge sharing and diverse participation. In\\nthis paper, we discuss how scientists' and volunteers' motivation and\\nparticipation influence the design of Citizen Science platforms. We present our\\nsummary as guidelines for designing these platforms as user-inspired\\nsocio-technical systems. We also present the case-studies on popular Citizen\\nScience platforms, including our own CitizenGrid platform, developed as part of\\nthe CCL EU project, as well as Zooniverse, World Community Grid, CrowdCrafting\\nand EpiCollect+ to see how closely these platforms follow our proposed\\nguidelines and how these may be further improved to incorporate the creativity\\nenabled by the collective knowledge sharing.\", 'Developing a physical theory of computation is an open challenging task at\\nthe interface of non-equilibrium thermodynamics and computer science. An\\nimportant part of this task requires the examination of thermodynamic\\nquantities in formal models of computers which execute computational tasks,\\nsuch as automata or word-RAM models executing algorithms. This implies dealing\\nwith a number of difficulties such as stochastic halting times, unidirectional\\n(possibly deterministic) transitions, and restricted initial conditions. Here,\\nwe present a framework which tackles all such difficulties by extending\\nmartingale theory of nonequilibrium thermodynamics to non-stationary Markovian\\ndynamics with broken local detailed balance and absolute irreversibility. In\\ndoing so, we derive universal fluctuation relations and second-law-like\\ninequalities that provide both lower and upper bounds for the intrinsic\\ndissipation (mismatch cost) associated with any computation performed over\\narbitrary stochastic times, so long as it is implemented with a periodic\\nprocess. We then provide universal equalities and inequalities for the\\nacceptance probability of words of a given length by a computer in terms of\\nthermodynamic quantities, and outline connections between computer science and\\nstochastic resetting. We illustrate most of our results with exhaustive\\nnumerical simulations of fundamental models of computation from theoretical\\ncomputer science, namely deterministic finite automata processing bit strings.\\nOur results, while motivated from the computational context, are applicable far\\nbeyond it.', 'The creation of popular science web videos on the Internet has increased in\\nrecent years. The diversity of formats, genres, and producers makes it\\ndifficult to formulate a universal definition of science web videos since not\\nevery producer considers him- or herself to be a science communicator in an\\ninstitutional sense, and professionalism and success on video platforms no\\nlonger depend exclusively on technical excellence or production costs.\\nEntertainment, content quality, and authenticity have become the keys to\\ncommunity building and success. The democratization of science video production\\nallows a new variety of genres, styles, and forms. This article provides a\\nfirst overview of the typologies and characteristics of popular science web\\nvideos. To avoid a misleading identification of science web videos with\\ninstitutionally produced videos, we steer clear of the term science\\ncommunication video, since many of the actual producers are not even familiar\\nwith the academic discussion on science communication, and since the subject\\nmatter does not depend on political or educational strategies. A content\\nanalysis of 200 videos from 100 online video channels was conducted. Several\\nfactors such as narrative strategies, video editing techniques, and design\\ntendencies with regard to cinematography, the number of shots, the kind of\\nmontage used, and even the spread use of sound design and special FX point to\\nan increasing professionalism among science communicators independent of\\ninstitutional or personal commitments: in general, it can be said that supposed\\namateurs are creating the visual language of science video communication. This\\nstudy represents an important step in understanding the essence of current\\npopular science web videos and provides an evidence-based definition as a\\nhelpful cornerstone for further studies on science communication within this\\nkind of new media.', 'The advent of increasingly large and complex datasets has fundamentally\\naltered the way that scientists conduct astronomy research. The need to work\\nclosely to the data has motivated the creation of online science platforms,\\nwhich include a suite of software tools and services, therefore going beyond\\ndata storage and data access. We present two example applications of Jupyter as\\na part of astrophysical science platforms for professional researchers and\\nstudents. First, the Astro Data Lab is developed and operated by NOIRLab with a\\nmission to serve the astronomy community with now over 1500 registered users.\\nSecond, the Dark Energy Spectroscopic Instrument science platform serves its\\ngeographically distributed team comprising about 900 collaborators from over 90\\ninstitutions. We describe the main uses of Jupyter and the interfaces that\\nneeded to be created to embed it within science platform ecosystems. We use\\nthese examples to illustrate the broader concept of empowering researchers and\\nproviding them with access to not only large datasets but also cutting-edge\\nsoftware, tools, and data services without requiring any local installation,\\nwhich can be relevant for a wide range of disciplines. Future advances may\\ninvolve science platform networks, and tools for simultaneously developing\\nJupyter notebooks to facilitate collaborations.', 'The growing amount of data and advances in data science have created a need\\nfor a new kind of cloud platform that provides users with flexibility, strong\\nsecurity, and the ability to couple with supercomputers and edge devices\\nthrough high-performance networks. We have built such a nation-wide cloud\\nplatform, called \"mdx\" to meet this need. The mdx platform\\'s virtualization\\nservice, jointly operated by 9 national universities and 2 national research\\ninstitutes in Japan, launched in 2021, and more features are in development.\\nCurrently mdx is used by researchers in a wide variety of domains, including\\nmaterials informatics, geo-spatial information science, life science,\\nastronomical science, economics, social science, and computer science. This\\npaper provides an the overview of the mdx platform, details the motivation for\\nits development, reports its current status, and outlines its future plans.', 'Researchers point to four potential issues related to the popularisation of\\nquantum science and technology. These include a lack of explaining underlying\\nquantum concepts of quantum 2.0 technology, framing quantum science and\\ntechnology as spooky and enigmatic, framing quantum technology narrowly in\\nterms of public good and having a strong focus on quantum computing. To date,\\nno research has yet assessed whether these potential issues are actually\\npresent in popular communication about quantum science. In this content\\nanalysis, we have examined the presence of these potential issues in 501 TEDx\\ntalks with quantum science and technology content. Results show that while most\\nexperts (70%) explained at least one underlying quantum concept (superposition,\\nentanglement or contextuality) of quantum 2.0 technology, only 28% of the\\nnon-experts did so. Secondly, the spooky/enigmatic frame was present in about a\\nquarter of the talks. Thirdly, a narrow public good frame was found,\\npredominantly by highlighting the benefits of quantum science and technology\\n(found in over 6 times more talks than risks). Finally, the main focus was on\\nquantum computing at the expense of other quantum technologies. In conclusion,\\nthe proposed frames are indeed found in TEDx talks, there is indeed a focus on\\nquantum computing, but at least experts explain underlying quantum concepts\\noften.', 'To analyse large numbers of texts, social science researchers are\\nincreasingly confronting the challenge of text classification. When manual\\nlabeling is not possible and researchers have to find automatized ways to\\nclassify texts, computer science provides a useful toolbox of machine-learning\\nmethods whose performance remains understudied in the social sciences. In this\\narticle, we compare the performance of the most widely used text classifiers by\\napplying them to a typical research scenario in social science research: a\\nrelatively small labeled dataset with infrequent occurrence of categories of\\ninterest, which is a part of a large unlabeled dataset. As an example case, we\\nlook at Twitter communication regarding climate change, a topic of increasing\\nscholarly interest in interdisciplinary social science research. Using a novel\\ndataset including 5,750 tweets from various international organizations\\nregarding the highly ambiguous concept of climate change, we evaluate the\\nperformance of methods in automatically classifying tweets based on whether\\nthey are about climate change or not. In this context, we highlight two main\\nfindings. First, supervised machine-learning methods perform better than\\nstate-of-the-art lexicons, in particular as class balance increases. Second,\\ntraditional machine-learning methods, such as logistic regression and random\\nforest, perform similarly to sophisticated deep-learning methods, whilst\\nrequiring much less training time and computational resources. The results have\\nimportant implications for the analysis of short texts in social science\\nresearch.', 'We define formally decohered quantum computers (using density matrices), and\\npresent a simulation of them by a probabalistic classical Turing Machine. We\\nstudy the slowdown of the simulation for two cases: (1) sequential quantum\\ncomputers, or quantum Turing machines(QTM), and (2) parallel quantum computers,\\nor quantum circuits. This paper shows that the computational power of decohered\\nquantum computers depends strongly on the amount of parallelism in the\\ncomputation.\\n  The expected slowdown of the simulation of a QTM is polynomial in time and\\nspace of the quantum computation, for any non zero decoherence rate. This means\\nthat a QTM subjected to any amount of noise is worthless. For decohered quantum\\ncircuits, the situation is more subtle and depends on the decoherence rate,\\neta. We find that our simulation is efficient for circuits with decoherence\\nrate higher than some constant, but exponential for general circuits with\\ndecoherence rate lower than some other constant. Using computer experiments, we\\nshow that the transition from exponential cost to polynomial cost happens in a\\nshort range of decoherence rates, and exhibit the phase transitions in various\\nquantum circuits.', 'Construction of explicit quantum circuits follows the notion of the \"standard\\ncircuit model\" introduced in the solid and profound analysis of elementary\\ngates providing quantum computation. Nevertheless the model is not always\\noptimal (e.g. concerning the number of computational steps) and it neglects\\nphysical systems which cannot follow the \"standard circuit model\" analysis. We\\npropose a computational scheme which overcomes the notion of the transposition\\nfrom classical circuits providing a computation scheme with the least possible\\nnumber of Hamiltonians in order to minimize the physical resources needed to\\nperform quantum computation and to succeed a minimization of the computational\\nprocedure (minimizing the number of computational steps needed to perform an\\narbitrary unitary transformation). It is a general scheme of construction,\\nindependent of the specific system used for the implementation of the quantum\\ncomputer. The open problem of controllability in Lie groups is directly related\\nand rises to prominence in an effort to perform universal quantum computation.', 'We consider a setting where a verifier with limited computation power\\ndelegates a resource intensive computation task---which requires a $T\\\\times S$\\ncomputation tableau---to two provers where the provers are rational in that\\neach prover maximizes their own payoff---taking into account losses incurred by\\nthe cost of computation. We design a mechanism called the Minimal Refereed\\nMechanism (MRM) such that if the verifier has $O(\\\\log S + \\\\log T)$ time and\\n$O(\\\\log S + \\\\log T)$ space computation power, then both provers will provide a\\nhonest result without the verifier putting any effort to verify the results.\\nThe amount of computation required for the provers (and thus the cost) is a\\nmultiplicative $\\\\log S$-factor more than the computation itself, making this\\nschema efficient especially for low-space computations.', 'The relationship between brains and computers is often taken to be merely\\nmetaphorical. However, genuine computational systems can be implemented in\\nvirtually any media; thus, one can take seriously the view that brains\\nliterally compute. But without empirical criteria for what makes a physical\\nsystem genuinely a computational one, computation remains a matter of\\nperspective, especially for natural systems (e.g., brains) that were not\\nexplicitly designed and engineered to be computers. Considerations from real\\nexamples of physical computers-both analog and digital, contemporary and\\nhistorical-make clear what those empirical criteria must be. Finally, applying\\nthose criteria to the brain shows how we can view the brain as a computer\\n(probably an analog one at that), which, in turn, illuminates how that claim is\\nboth informative and falsifiable.', 'While the modern science is characterized by an exponential growth in\\nscientific literature, the increase in publication volume clearly does not\\nreflect the expansion of the cognitive boundaries of science. Nevertheless,\\nmost of the metrics for assessing the vitality of science or for making funding\\nand policy decisions are based on productivity. Similarly, the increasing level\\nof knowledge production by large science teams, whose results often enjoy\\ngreater visibility, does not necessarily mean that \"big science\" leads to\\ncognitive expansion. Here we present a novel, big-data method to quantify the\\nextents of cognitive domains of different bodies of scientific literature\\nindependently from publication volume, and apply it to 20 million articles\\npublished over 60-130 years in physics, astronomy, and biomedicine. The method\\nis based on the lexical diversity of titles of fixed quotas of research\\narticles. Owing to large size of quotas, the method overcomes the inherent\\nstochasticity of article titles to achieve <1% precision. We show that the\\nperiods of cognitive growth do not necessarily coincide with the trends in\\npublication volume. Furthermore, we show that the articles produced by larger\\nteams cover significantly smaller cognitive territory than (the same quota of)\\narticles from smaller teams. Our findings provide a new perspective on the role\\nof small teams and individual researchers in expanding the cognitive boundaries\\nof science. The proposed method of quantifying the extent of the cognitive\\nterritory can also be applied to study many other aspects of \"science of\\nscience.\"', 'The study and understanding of human behaviour is relevant to computer\\nscience, artificial intelligence, neural computation, cognitive science,\\nphilosophy, psychology, and several other areas. Presupposing cognition as\\nbasis of behaviour, among the most prominent tools in the modelling of\\nbehaviour are computational-logic systems, connectionist models of cognition,\\nand models of uncertainty. Recent studies in cognitive science, artificial\\nintelligence, and psychology have produced a number of cognitive models of\\nreasoning, learning, and language that are underpinned by computation. In\\naddition, efforts in computer science research have led to the development of\\ncognitive computational systems integrating machine learning and automated\\nreasoning. Such systems have shown promise in a range of applications,\\nincluding computational biology, fault diagnosis, training and assessment in\\nsimulators, and software verification. This joint survey reviews the personal\\nideas and views of several researchers on neural-symbolic learning and\\nreasoning. The article is organised in three parts: Firstly, we frame the scope\\nand goals of neural-symbolic computation and have a look at the theoretical\\nfoundations. We then proceed to describe the realisations of neural-symbolic\\ncomputation, systems, and applications. Finally we present the challenges\\nfacing the area and avenues for further research.', \"In the last decade, Single-Board Computers (SBCs) have been employed more\\nfrequently in engineering and computer science both to technical and\\neducational levels. Several factors such as the versatility, the low-cost, and\\nthe possibility to enhance the learning process through technology have\\ncontributed to the educators and students usually employ these devices.\\nHowever, the implications, possibilities, and constraints of these devices in\\nengineering and Computer Science (CS) education have not been explored in\\ndetail. In this systematic literature review, we explore how the SBCs are\\nemployed in engineering and computer science and what educational results are\\nderived from their usage in the period 2010-2020 at tertiary education. For\\nthat, 154 studies were selected out of n=605 collected from the academic\\ndatabases Ei Compendex, ERIC, and Inspec. The analysis was carried-out in two\\nphases, identifying, e.g., areas of application, learning outcomes, and\\nstudents and researchers' perceptions. The results mainly indicate the\\nfollowing aspects: (1) The areas of laboratories and e-learning, computing\\neducation, robotics, Internet of Things (IoT), and persons with disabilities\\ngather the studies in the review. (2) Researchers highlight the importance of\\nthe SBCs to transform the curricula in engineering and CS for the students to\\nlearn complex topics through experimentation in hands-on activities. (3) The\\ntypical cognitive learning outcomes reported by the authors are the improvement\\nof the students' grades and the technical skills regarding the topics in the\\ncourses. Concerning the affective learning outcomes, the increase of interest,\\nmotivation, and engagement are commonly reported by the authors.\", 'Quantum computing will play a pivotal role in the High Energy Physics (HEP)\\nscience program over the early parts of the 21$^{st}$ Century, both as a major\\nexpansion of our capabilities across the Computational Frontier, and in\\nsynthesis with quantum sensing and quantum networks. This report outlines how\\nQuantum Information Science (QIS) and HEP are deeply intertwined endeavors that\\nbenefit enormously from a strong engagement together. Quantum computers do not\\nrepresent a detour for HEP, rather they are set to become an integral part of\\nour discovery toolkit. Problems ranging from simulating quantum field theories,\\nto fully leveraging the most sensitive sensor suites for new particle searches,\\nand even data analysis will run into limiting bottlenecks if constrained to our\\ncurrent computing paradigms. Easy access to quantum computers is needed to\\nbuild a deeper understanding of these opportunities. In turn, HEP brings\\ncrucial expertise to the national quantum ecosystem in quantum domain\\nknowledge, superconducting technology, cryogenic and fast microelectronics, and\\nmassive-scale project management. The role of quantum technologies across the\\nentire economy is expected to grow rapidly over the next decade, so it is\\nimportant to establish the role of HEP in the efforts surrounding QIS. Fully\\ndelivering on the promise of quantum technologies in the HEP science program\\nrequires robust support. It is important to both invest in the co-design\\nopportunities afforded by the broader quantum computing ecosystem and leverage\\nHEP strengths with the goal of designing quantum computers tailored to HEP\\nscience.', 'The traditional foundation of science lies on the cornerstones of theory and\\nexperiment. Theory is used to explain experiment, which in turn guides the\\ndevelopment of theory. Since the advent of computers and the development of\\ncomputational algorithms, computation has risen as the third cornerstone of\\nscience, joining theory and experiment on an equal footing. Computation has\\nbecome an essential part of modern science, amending experiment by enabling\\naccurate comparison of complicated theories to sophisticated experiments, as\\nwell as guiding by triage both the design and targets of experiments and the\\ndevelopment of novel theories and computational methods.\\n  Like experiment, computation relies on continued investment in\\ninfrastructure: it requires both hardware (the physical computer on which the\\ncalculation is run) as well as software (the source code of the programs that\\nperforms the wanted simulations). In this Perspective, I discuss present-day\\nchallenges on the software side in computational chemistry, which arise from\\nthe fast-paced development of algorithms, programming models, as well as\\nhardware. I argue that many of these challenges could be solved with reusable\\nopen source libraries, which are a public good, enhance the reproducibility of\\nscience, and accelerate the development and availability of state-of-the-art\\nmethods and improved software.', 'Decentralized computation outsourcing should allow anyone to access the large\\namounts of computational power that exists in the Internet of Things.\\nUnfortunately, when trusted third parties are removed to achieve this\\ndecentralization, ensuring an outsourced computation is performed correctly\\nremains a significant challenge. In this paper, we provide a solution to this\\nproblem.\\n  We outline Marvel DC, a fully decentralized blockchain-based\\ndistributed-computing protocol which formally guarantees that computers are\\nstrictly incentivized to correctly perform requested computations. Furthermore,\\nMarvel DC utilizes a reputation management protocol to ensure that, for any\\nminority of computers not performing calculations correctly, these computers\\nare identified and selected for computations with diminishing probability. We\\nthen outline Privacy Marvel DC, a privacy-enhanced version of Marvel DC which\\ndecouples results from the computers which computed them, making the protocol\\nsuitable for computations such as Federated Learning, where results can reveal\\nsensitive information about that computer that computed them. We provide an\\nimplementation of Marvel DC and analyses of both protocols, demonstrating that\\nthey are not only the first protocols to provide the aforementioned formal\\nguarantees, but are also practical, competitive with prior attempts in the\\nfield, and ready to deploy.', 'We propose a simple global computing framework, whose main concern is code\\nmigration. Systems are structured in sites, and each site is divided into two\\nparts: a computing body, and a membrane, which regulates the interactions\\nbetween the computing body and the external environment. More precisely,\\nmembranes are filters which control access to the associated site, and they\\nalso rely on the well-established notion of trust between sites. We develop a\\nbasic theory to express and enforce security policies via membranes. Initially,\\nthese only control the actions incoming agents intend to perform locally. We\\nthen adapt the basic theory to encompass more sophisticated policies, where the\\nnumber of actions an agent wants to perform, and also their order, are\\nconsidered.', \"Mechanism design is now a standard tool in computer science for aligning the\\nincentives of self-interested agents with the objectives of a system designer.\\nThere is, however, a fundamental disconnect between the traditional application\\ndomains of mechanism design (such as auctions) and those arising in computer\\nscience (such as networks): while monetary transfers (i.e., payments) are\\nessential for most of the known positive results in mechanism design, they are\\nundesirable or even technologically infeasible in many computer systems.\\nClassical impossibility results imply that the reach of mechanisms without\\ntransfers is severely limited.\\n  Computer systems typically do have the ability to reduce service\\nquality--routing systems can drop or delay traffic, scheduling protocols can\\ndelay the release of jobs, and computational payment schemes can require\\ncomputational payments from users (e.g., in spam-fighting systems). Service\\ndegradation is tantamount to requiring that users burn money}, and such\\n``payments'' can be used to influence the preferences of the agents at a cost\\nof degrading the social surplus.\\n  We develop a framework for the design and analysis of money-burning\\nmechanisms to maximize the residual surplus--the total value of the chosen\\noutcome minus the payments required.\", 'A Human Computer Interface (HCI) System for playing games is designed here\\nfor more natural communication with the machines. The system presented here is\\na vision-based system for detection of long voluntary eye blinks and\\ninterpretation of blink patterns for communication between man and machine.\\nThis system replaces the mouse with the human face as a new way to interact\\nwith the computer. Facial features (nose tip and eyes) are detected and tracked\\nin realtime to use their actions as mouse events. The coordinates and movement\\nof the nose tip in the live video feed are translated to become the coordinates\\nand movement of the mouse pointer on the application. The left or right eye\\nblinks fire left or right mouse click events. The system works with inexpensive\\nUSB cameras and runs at a frame rate of 30 frames per second.', 'The accelerated development of quantum technology has reached a pivotal\\npoint. Early in 2014, several results were published demonstrating that several\\nexperimental technologies are now accurate enough to satisfy the requirements\\nof fault-tolerant, error corrected quantum computation. While there are many\\ntechnological and experimental issues that still need to be solved, the ability\\nof experimental systems to now have error rates low enough to satisfy the\\nfault-tolerant threshold for several error correction models is a tremendous\\nmilestone. Consequently, it is now a good time for the computer science and\\nclassical engineering community to examine the {\\\\em classical} problems\\nassociated with compiling quantum algorithms and implementing them on future\\nquantum hardware. In this paper, we will review the basic operational rules of\\na topological quantum computing architecture and outline one of the most\\nimportant classical problems that need to be solved; the decoding of error\\ncorrection data for a large-scale quantum computer. We will endeavour to\\npresent these problems independently from the underlying physics as much of\\nthis work can be effectively solved by non-experts in quantum information or\\nquantum mechanics.', 'The paper examines the current trends in designing of systems for convenient\\nand secure remote job submission to various computer resources, including\\nsupercomputers, computer clusters, cloud resources, data storages and\\ndatabases, and grid infrastructures by authorized users, as well as remote job\\nmonitoring and obtaining the results. Currently, high-perfomance computing and\\nstorage resources are capable of solving independently the majority of\\npractical problems in the field of science and technology. Therefore, the focus\\nin the development of a new generation of middleware shifts from the global\\ngrid systems to building convenient and efficient web platforms for remote\\naccess to individual computing resources. The paper examines the general\\nprinciples of the construction and briefly describes some of the specific\\nimplementations of the web platforms.', 'In this paper, we argue that a democratic approach to children\\'s computing\\neducation in a science class must focus on the aesthetics of children\\'s\\nexperience. In Democracy and Education, Dewey links \"democracy\" with a\\ndistinctive understanding of \"experience\". For Dewey, the value of educational\\nexperiences lies in \"the unity or integrity of experience\" (DE, 248). In Art as\\nExperience, Dewey presents aesthetic experience as the fundamental form of\\nhuman experience that undergirds all other forms of experiences, and can also\\nbring together multiple forms of experiences, locating this form of experience\\nin the work of artists. Particularly relevant to our current concern\\n(computational literacy), Dewey calls the process through which a person\\ntransforms a material into an expressive medium an aesthetic experience (AE,\\n68-69). We argue here that the kind of experience that is appropriate for a\\ndemocratic education in the context of children\\'s computational science is\\nessentially aesthetic in nature. Given that aesthetics has received relatively\\nlittle attention in STEM education research, our purpose here is to highlight\\nthe power of Deweyan aesthetic experience in making computational thinking\\navailable to and attractive to all children, including those who are\\ndisinterested in computing, and especially those who are likely to be\\ndiscounted by virtue of location, gender or race.', 'We focus on collaborative activities that engage computer graphics designers\\nand social scientists in systems design processes. Our conceptual symmetrical\\naccount of technology design and theory development is elaborated as a mode of\\nmutual engagement occurring in an interdisciplinary trading zone, where neither\\ndiscipline is placed at the service of the other, and nor do disciplinary\\nboundaries dissolve. To this end, we draw on analyses of mutual engagements\\nbetween computer and social scientists stemming from the fields of\\ncomputer-supported cooperative work (CSCW), human-computer interaction (HCI),\\nand science and technology studies (STS). We especially build on theoretical\\nwork in STS concerning information technology (IT) in health care and extend\\nrecent contributions from STS with respect to the modes of engagement and\\ntrading zones between computer and social sciences. We conceive participative\\ndigital systems design as a form of inquiry for the analysis of cooperative\\nwork settings, particularly when social science becomes part of design\\nprocesses. We illustrate our conceptual approach using data from an\\ninterdisciplinary project involving computer graphics designers, sociologists,\\nand neurosurgeons with the aim of developing patient-centered visualizations\\nfor clinical cooperation on a hospital ward.', 'Computing Wasserstein barycenters (a.k.a. Optimal Transport barycenters) is a\\nfundamental problem in geometry which has recently attracted considerable\\nattention due to many applications in data science. While there exist\\npolynomial-time algorithms in any fixed dimension, all known running times\\nsuffer exponentially in the dimension. It is an open question whether this\\nexponential dependence is improvable to a polynomial dependence. This paper\\nproves that unless P=NP, the answer is no. This uncovers a \"curse of\\ndimensionality\" for Wasserstein barycenter computation which does not occur for\\nOptimal Transport computation. Moreover, our hardness results for computing\\nWasserstein barycenters extend to approximate computation, to seemingly simple\\ncases of the problem, and to averaging probability distributions in other\\nOptimal Transport metrics.', 'Machine learning models are poised to make a transformative impact on\\nchemical sciences by dramatically accelerating computational algorithms and\\namplifying insights available from computational chemistry methods. However,\\nachieving this requires a confluence and coaction of expertise in computer\\nscience and physical sciences. This review is written for new and experienced\\nresearchers working at the intersection of both fields. We first provide\\nconcise tutorials of computational chemistry and machine learning methods,\\nshowing how insights involving both can be achieved. We then follow with a\\ncritical review of noteworthy applications that demonstrate how computational\\nchemistry and machine learning can be used together to provide insightful (and\\nuseful) predictions in molecular and materials modeling, retrosyntheses,\\ncatalysis, and drug design.', 'Domain-Specific Languages (DSLs) improve programmers productivity by\\ndecoupling problem descriptions from algorithmic implementations. However, DSLs\\nfor High-Performance Computing (HPC) have two additional critical requirements:\\nperformance and scalability. This paper presents the automated process of\\ngenerating, from abstract mathematical specifications of Computational Fluid\\nDynamics (CFD) problems, optimised parallel codes that perform and scale as\\nmanually optimised ones. We consciously combine within Saiph, a DSL for solving\\nCFD problems, low-level optimisations and parallelisation strategies, enabling\\nhigh-performance single-core executions which effectively scale to multi-core\\nand distributed environments. Our results demonstrate how high-level DSLs can\\noffer competitive performance by transparently leveraging state-of-the-art HPC\\ntechniques.', 'This volume contains the post-proceedings of the Thirteenth International\\nWorkshop on Graph Computation Models (GCM 2022). The workshop took place in\\nNantes, France on 6th July 2022 as part of STAF 2022 (Software Technologies:\\nApplications and Foundations). Graphs are common mathematical structures that\\nare visual and intuitive. They constitute a natural and seamless way for system\\nmodelling in science, engineering, and beyond, including computer science,\\nbiology, and business process modelling. Graph computation models constitute a\\nclass of very high-level models where graphs are first-class citizens. The aim\\nof the International GCM Workshop series is to bring together researchers\\ninterested in all aspects of computation models based on graphs and graph\\ntransformation. It promotes the cross-fertilising exchange of ideas and\\nexperiences among senior and young researchers from the different communities\\ninterested in the foundations, applications, and implementations of graph\\ncomputation models and related areas.', 'In this note is touched upon an application of quantum information science\\n(QIS) in nanotechnology area. The laws of quantum mechanics may be very\\nimportant for nano-scale objects. A problem with simulating of quantum systems\\nis well known and quantum computer was initially suggested by R. Feynman just\\nas the way to overcome such difficulties. Mathematical methods developed in QIS\\nalso may be applied for description of nano-devices. Few illustrative examples\\nare mentioned and they may be related with so-called fourth generation of\\nnanotechnology products.', 'We represent collaboration of authors in computer science papers in terms of\\nboth affiliation and collaboration networks and observe how these networks\\nevolved over time since 1960. We investigate the temporal evolution of\\nbibliometric properties, like size of the discipline, productivity of scholars,\\nand collaboration level in papers, as well as of large-scale network\\nproperties, like reachability and average separation distance among scientists,\\ndistribution of the number of scholar collaborators, network clustering and\\nnetwork assortativity by number of collaborators.', 'This paper studies context bisimulation for higher-order processes, in the\\npresence of parameterization (viz. abstraction). We show that the extension of\\nhigher-order processes with process parameterization retains the\\ncharacterization of context bisimulation by a much simpler form of bisimulation\\ncalled normal bisimulation (viz. they are coincident), in which universal\\nquantifiers are eliminated; whereas it is not the same with name\\nparameterization. These results clarify further the bisimulation theory of\\nhigher-order processes, and also shed light on the essential distinction\\nbetween the two kinds of parameterization.', \"In this paper, we apply genetic algorithms to the field of electoral studies.\\nForecasting election results is one of the most exciting and demanding tasks in\\nthe area of market research, especially due to the fact that decisions have to\\nbe made within seconds on live television. We show that the proposed method\\noutperforms currently applied approaches and thereby provide an argument to\\ntighten the intersection between computer science and social science,\\nespecially political science, further. We scrutinize the performance of our\\nalgorithm's runtime behavior to evaluate its applicability in the field.\\nNumerical results with real data from a local election in the Austrian province\\nof Styria from 2010 substantiate the applicability of the proposed approach.\", 'We propose extending Alternating-time Temporal Logic (ATL) by an operator <i\\nrefines-to G> F to express that agent i can distribute its powers to a set of\\nsub-agents G in a way which satisfies ATL condition f on the strategic ability\\nof the coalitions they may form, possibly together with others agents. We prove\\nthe decidability of model-checking of formulas whose subformulas with this\\noperator as the main connective have the form <i_1 refines-to G_1>...<i_m\\nrefines-to G_m> f, with no further occurrences of this operator in f.', 'Scientific software often presents very particular requirements regarding\\nusability, which is often completely overlooked in this setting. As\\ncomputational science has emerged as its own discipline, distinct from\\ntheoretical and experimental science, it has put new requirements on future\\nscientific software developments. In this paper, we discuss the background of\\nthese problems and introduce nine aspects of good usability. We also highlight\\nbest practices for each aspect with an emphasis on applications in\\ncomputational science.', 'Signatures provide a succinct description of certain features of paths in a\\nreparametrization invariant way. We propose a method for classifying shapes\\nbased on signatures, and compare it to current approaches based on the SRV\\ntransform and dynamic programming.', 'RADICAL-Cybertools (RCT) are a set of software systems that serve as\\nmiddleware to develop efficient and effective tools for scientific computing.\\nSpecifically, RCT enable executing many-task applications at extreme scale and\\non a variety of computing infrastructures. RCT are building blocks, designed to\\nwork as stand-alone systems, integrated among themselves or integrated with\\nthird-party systems. RCT enables innovative science in multiple domains,\\nincluding but not limited to biophysics, climate science and particle physics,\\nconsuming hundreds of millions of core hours. This paper provides an overview\\nof RCT systems, their impact, and the architectural principles and software\\nengineering underlying RCT', 'For the Royal Society Summer Science Exhibition 2016, the Institute of\\nComputational Cosmology from Durham University created the Galaxy Makers\\nexhibit to communicate our computational cosmology and astronomy research. In\\naddition to the physical exhibit we created an online component to foster\\nre-engagement, create a permanent home for our content and to allow us to\\ncollect the ever-more important information about participation and impact.\\nHere we summarise the details of the exhibit and the successes of creating an\\nonline component. We also share suggestions of further uses and improvements\\nthat could be implemented for online components of other science exhibitions.', 'Evolution of composition patterns in the annealed, single-crystal surface\\nalloy film is considered in the presence of the spinodal decomposition, the\\ncompositional stress and the diffusion anisotropy. While the former two effects\\ncontribute to overall phase separation, the anisotropy, correlated with the\\nsurface crystallographic orientation, guides the in-plane formation and\\norientation of a pattern. The impacts of the anisotropy parameters on patterns\\nare systematically computed for [110], [100], and [111]-oriented fcc cubic\\nalloy surfaces.', \"As educators push for students to learn science by doing science, there is a\\nneed for computational scaffolding to assist students' evaluation of scientific\\nevidence and argument building. In this paper, we present a pilot study of\\nSciNote, a CSCL tool allowing educators to integrate third-party software into\\na flexible and collaborative workspace. We explore how SciNote enables teams to\\nbuild data-driven arguments during inquiry-based learning activities.\", 'This paper presents a methodology for designing a containerized and\\ndistributed open science infrastructure to simplify its reusability,\\nreplicability, and portability in different environments. The methodology is\\ndepicted in a step-by-step schema based on four main phases: (1) Analysis, (2)\\nDesign, (3) Definition, and (4) Managing and provisioning. We accompany the\\ndescription of each step with existing technologies and concrete examples of\\napplication.', \"Neither the hype exemplified in some exaggerated claims about deep neural\\nnetworks (DNNs), nor the gloom expressed by Bowers et al. do DNNs as models in\\nvision science justice: DNNs rapidly evolve, and today's limitations are often\\ntomorrow's successes. In addition, providing explanations as well as prediction\\nand image-computability are model desiderata; one should not be favoured at the\\nexpense of the other.\", 'Context: In the context of exploring the art, science and engineering of\\nprogramming, the question of which programming languages should be taught first\\nhas been fiercely debated since computer science teaching started in\\nuniversities. Failure to grasp programming readily almost certainly implies\\nfailure to progress in computer science. Inquiry: What first programming\\nlanguages are being taught? There have been regular national-scale surveys in\\nAustralia and New Zealand, with the only US survey reporting on a small subset\\nof universities. This the first such national survey of universities in the UK.\\nApproach: We report the results of the first survey of introductory programming\\ncourses (N=80) taught at UK universities as part of their first year computer\\nscience (or related) degree programmes, conducted in the first half of 2016. We\\nreport on student numbers, programming paradigm, programming languages and\\nenvironment/tools used, as well as the underpinning rationale for these\\nchoices. Knowledge: The results in this first UK survey indicate a dominance of\\nJava at a time when universities are still generally teaching students who are\\nnew to programming (and computer science), despite the fact that Python is\\nperceived, by the same respondents, to be both easier to teach as well as to\\nlearn. Grounding: We compare the results of this survey with a related survey\\nconducted since 2010 (as well as earlier surveys from 2001 and 2003) in\\nAustralia and New Zealand. Importance: This survey provides a starting point\\nfor valuable pedagogic baseline data for the analysis of the art, science and\\nengineering of programming, in the context of substantial computer science\\ncurriculum reform in UK schools, as well as increasing scrutiny of teaching\\nexcellence and graduate employability for UK universities.', 'Modern scientific applications predominantly run on large-scale computing\\nplatforms, necessitating collaboration between scientific domain experts and\\nhigh-performance computing (HPC) experts. While domain experts are often\\nskilled in customizing domain-specific scientific computing routines, which\\noften involves various matrix computations, HPC experts are essential for\\nachieving efficient execution of these computations on large-scale platforms.\\nThis process often involves utilizing complex parallel computing libraries\\ntailored to specific matrix computation scenarios. However, the intricate\\nprogramming procedure and the need for deep understanding in both application\\ndomains and HPC poses significant challenges to the widespread adoption of\\nscientific computing. In this research, we observe that matrix computations can\\nbe transformed into equivalent graph representations, and that by utilizing\\ngraph processing engines, HPC experts can be freed from the burden of\\nimplementing efficient scientific computations. Based on this observation, we\\nintroduce a graph engine-based scientific computing (Graph for Science)\\nparadigm, which provides a unified graph programming interface, enabling domain\\nexperts to promptly implement various types of matrix computations. The\\nproposed paradigm leverages the underlying graph processing engine to achieve\\nefficient execution, eliminating the needs for HPC expertise in programming\\nlarge-scale scientific applications. Our results show that the graph\\nengine-based scientific computing paradigm achieves performance comparable to\\nthe best-performing implementations based on existing parallel computing\\nlibraries and bespoke implementations. Importantly, the paradigm greatly\\nsimplifies the development of scientific computations on large-scale platforms,\\nreducing the programming difficulty for scientists and facilitating broader\\nadoption of scientific computing.', 'We use ideas from distributed computing to study dynamic environments in\\nwhich computational nodes, or decision makers, follow adaptive heuristics (Hart\\n2005), i.e., simple and unsophisticated rules of behavior, e.g., repeatedly\\n\"best replying\" to others\\' actions, and minimizing \"regret\", that have been\\nextensively studied in game theory and economics. We explore when convergence\\nof such simple dynamics to an equilibrium is guaranteed in asynchronous\\ncomputational environments, where nodes can act at any time. Our research\\nagenda, distributed computing with adaptive heuristics, lies on the borderline\\nof computer science (including distributed computing and learning) and game\\ntheory (including game dynamics and adaptive heuristics). We exhibit a general\\nnon-termination result for a broad class of heuristics with bounded\\nrecall---that is, simple rules of behavior that depend only on recent history\\nof interaction between nodes. We consider implications of our result across a\\nwide variety of interesting and timely applications: game theory, circuit\\ndesign, social networks, routing and congestion control. We also study the\\ncomputational and communication complexity of asynchronous dynamics and present\\nsome basic observations regarding the effects of asynchrony on no-regret\\ndynamics. We believe that our work opens a new avenue for research in both\\ndistributed computing and game theory.', 'Computational devices combining two or more different parts, one controlling\\nthe operation of the other, for example, derive their power from the\\ninteraction, in addition to the capabilities of the parts. Non-classical\\ncomputation has tended to consider only single computational models: neural,\\nanalog, quantum, chemical, biological, neglecting to account for the\\ncontribution from the experimental controls. In this position paper, we propose\\na framework suitable for analysing combined computational models, from abstract\\ntheory to practical programming tools. Focusing on the simplest example of one\\nsystem controlled by another through a sequence of operations in which only one\\nsystem is active at a time, the output from one system becomes the input to the\\nother for the next step, and vice versa. We outline the categorical machinery\\nrequired for handling diverse computational systems in such combinations, with\\ntheir interactions explicitly accounted for. Drawing on prior work in\\nrefinement and retrenchment, we suggest an appropriate framework for developing\\nprogramming tools from the categorical framework. We place this work in the\\ncontext of two contrasting concepts of \"efficiency\": theoretical comparisons to\\ndetermine the relative computational power do not always reflect the practical\\ncomparison of real resources for a finite-sized computational task, especially\\nwhen the inputs include (approximations of) real numbers. Finally we outline\\nthe limitations of our simple model, and identify some of the extensions that\\nwill be required to treat more complex interacting computational systems.', 'Nowadays, the paradigm of parallel computing is changing. CUDA is now a\\npopular programming model for general purpose computations on GPUs and a great\\nnumber of applications were ported to CUDA obtaining speedups of orders of\\nmagnitude comparing to optimized CPU implementations. Hybrid approaches that\\ncombine the message passing model with the shared memory model for parallel\\ncomputing are a solution for very large applications. We considered a\\nheterogeneous cluster that combines the CPU and GPU computations using MPI and\\nCUDA for developing a high performance linear algebra library. Our library\\ndeals with large linear systems solvers because they are a common problem in\\nthe fields of science and engineering. Direct methods for computing the\\nsolution of such systems can be very expensive due to high memory requirements\\nand computational cost. An efficient alternative are iterative methods which\\ncomputes only an approximation of the solution. In this paper we present an\\nimplementation of a library that uses a hybrid model of computation using MPI\\nand CUDA implementing both direct and iterative linear systems solvers. Our\\nlibrary implements LU and Cholesky factorization based solvers and some of the\\nnon-stationary iterative methods using the MPI/CUDA combination. We compared\\nthe performance of our MPI/CUDA implementation with classic programs written to\\nbe run on a single CPU.', 'Omega-powers of finitary languages are languages of infinite words\\n(omega-languages) in the form V^omega, where V is a finitary language over a\\nfinite alphabet X. They appear very naturally in the characterizaton of regular\\nor context-free omega-languages. Since the set of infinite words over a finite\\nalphabet X can be equipped with the usual Cantor topology, the question of the\\ntopological complexity of omega-powers of finitary languages naturally arises\\nand has been posed by Niwinski (1990), Simonnet (1992) and Staiger (1997). It\\nhas been recently proved that for each integer n > 0, there exist some\\nomega-powers of context free languages which are Pi^0_n-complete Borel sets,\\nthat there exists a context free language L such that L^omega is analytic but\\nnot Borel, and that there exists a finitary language V such that V^omega is a\\nBorel set of infinite rank. But it was still unknown which could be the\\npossible infinite Borel ranks of omega-powers. We fill this gap here, proving\\nthe following very surprising result which shows that omega-powers exhibit a\\ngreat topological complexity: for each non-null countable ordinal alpha, there\\nexist some Sigma^0_alpha-complete omega-powers, and some Pi^0_alpha-complete\\nomega-powers.', 'Answer Set Programming (ASP) is a declarative logic formalism that allows to\\nencode computational problems via logic programs. Despite the declarative\\nnature of the formalism, some advanced expertise is required, in general, for\\ndesigning an ASP encoding that can be efficiently evaluated by an actual ASP\\nsystem. A common way for trying to reduce the burden of manually tweaking an\\nASP program consists in automatically rewriting the input encoding according to\\nsuitable techniques, for producing alternative, yet semantically equivalent,\\nASP programs. However, rewriting does not always grant benefits in terms of\\nperformance; hence, proper means are needed for predicting their effects with\\nthis respect. In this paper we describe an approach based on Machine Learning\\n(ML) to automatically decide whether to rewrite. In particular, given an ASP\\nprogram and a set of input facts, our approach chooses whether and how to\\nrewrite input rules based on a set of features measuring their structural\\nproperties and domain information. To this end, a Multilayer Perceptrons model\\nhas then been trained to guide the ASP grounder I-DLV on rewriting input rules.\\nWe report and discuss the results of an experimental evaluation over a\\nprototypical implementation.', \"DBLP is the largest open-access repository of scientific articles on computer\\nscience and provides metadata associated with publications, authors, and\\nvenues. We retrieved more than 6 million publications from DBLP and extracted\\npertinent metadata (e.g., abstracts, author affiliations, citations) from the\\npublication texts to create the DBLP Discovery Dataset (D3). D3 can be used to\\nidentify trends in research activity, productivity, focus, bias, accessibility,\\nand impact of computer science research. We present an initial analysis focused\\non the volume of computer science research (e.g., number of papers, authors,\\nresearch activity), trends in topics of interest, and citation patterns. Our\\nfindings show that computer science is a growing research field (approx. 15%\\nannually), with an active and collaborative researcher community. While papers\\nin recent years present more bibliographical entries in comparison to previous\\ndecades, the average number of citations has been declining. Investigating\\npapers' abstracts reveals that recent topic trends are clearly reflected in D3.\\nFinally, we list further applications of D3 and pose supplemental research\\nquestions. The D3 dataset, our findings, and source code are publicly available\\nfor research purposes.\", 'Teaching assistants (TAs) are heavily used in computer science courses as a\\nway to handle high enrollment and still being able to offer students individual\\ntutoring and detailed assessments. TAs are themselves students who take on this\\nadditional role in parallel with their own studies at the same institution.\\nPrevious research has shown that being a TA can be challenging but has mainly\\nbeen conducted on TAs from a single institution or within a single course. This\\npaper offers a multi-institutional, multi-national perspective of challenges\\nthat TAs in computer science face. This has been done by conducting a thematic\\nanalysis of 180 reflective essays written by TAs from three institutions across\\nEurope. The thematic analysis resulted in five main challenges: becoming a\\nprofessional TA, student focused challenges, assessment, defining and using\\nbest practice, and threats to best practice. In addition, these challenges were\\nall identified within the essays from all three institutions, indicating that\\nthe identified challenges are not particularly context-dependent. Based on\\nthese findings, we also outline implications for educators involved in TA\\ntraining and coordinators of computer science courses with TAs.', \"The use of AI assistants, along with the challenges they present, has sparked\\nsignificant debate within the community of computer science education. While\\nthese tools demonstrate the potential to support students' learning and\\ninstructors' teaching, they also raise concerns about enabling unethical uses\\nby students. Previous research has suggested various strategies aimed at\\naddressing these issues. However, they concentrate on the introductory\\nprogramming courses and focus on one specific type of problem. The present\\nresearch evaluated the performance of ChatGPT, a state-of-the-art AI assistant,\\nat solving 187 problems spanning three distinct types that were collected from\\nsix undergraduate computer science. The selected courses covered different\\ntopics and targeted different program levels. We then explored methods to\\nmodify these problems to adapt them to ChatGPT's capabilities to reduce\\npotential misuse by students. Finally, we conducted semi-structured interviews\\nwith 11 computer science instructors. The aim was to gather their opinions on\\nour problem modification methods, understand their perspectives on the impact\\nof AI assistants on computer science education, and learn their strategies for\\nadapting their courses to leverage these AI capabilities for educational\\nimprovement. The results revealed issues ranging from academic fairness to\\nlong-term impact on students' mental models. From our results, we derived\\ndesign implications and recommended tools to help instructors design and create\\nfuture course material that could more effectively adapt to AI assistants'\\ncapabilities.\", 'Cross-database non-frontal expression recognition is a very meaningful but\\nrather difficult subject in the fields of computer vision and affect computing.\\nIn this paper, we proposed a novel transductive deep transfer learning\\narchitecture based on widely used VGGface16-Net for this problem. In this\\nframework, the VGGface16-Net is used to jointly learn an common optimal\\nnonlinear discriminative features from the non-frontal facial expression\\nsamples between the source and target databases and then we design a novel\\ntransductive transfer layer to deal with the cross-database non-frontal facial\\nexpression classification task. In order to validate the performance of the\\nproposed transductive deep transfer learning networks, we present extensive\\ncrossdatabase experiments on two famous available facial expression databases,\\nnamely the BU-3DEF and the Multi-PIE database. The final experimental results\\nshow that our transductive deep transfer network outperforms the\\nstate-of-the-art cross-database facial expression recognition methods.', 'In recent years, funding agencies and journals increasingly advocate for open\\nscience practices (e.g. data and method sharing) to improve the transparency,\\naccess, and reproducibility of science. However, quantifying these practices at\\nscale has proven difficult. In this work, we leverage a large-scale dataset of\\n1.1M papers from arXiv that are representative of the fields of physics, math,\\nand computer science to analyze the adoption of data and method link-sharing\\npractices over time and their impact on article reception. To identify links to\\ndata and methods, we train a neural text classification model to automatically\\nclassify URL types based on contextual mentions in papers. We find evidence\\nthat the practice of link-sharing to methods and data is spreading as more\\npapers include such URLs over time. Reproducibility efforts may also be\\nspreading because the same links are being increasingly reused across papers\\n(especially in computer science); and these links are increasingly concentrated\\nwithin fewer web domains (e.g. Github) over time. Lastly, articles that share\\ndata and method links receive increased recognition in terms of citation count,\\nwith a stronger effect when the shared links are active (rather than defunct).\\nTogether, these findings demonstrate the increased spread and perceived value\\nof data and method sharing practices in open science.', 'This paper describes a grammar learning system that combines model-based and\\ndata-driven learning within a single framework. Our results from learning\\ngrammars using the Spoken English Corpus (SEC) suggest that combined\\nmodel-based and data-driven learning can produce a more plausible grammar than\\nis the case when using either learning style isolation.', 'We offer a theoretical design of new systems that show promise for digital\\nbiochemical computing, including realizations of error correction by utilizing\\nredundancy, as well as signal rectification. The approach includes information\\nprocessing using encoded DNA sequences, DNAzyme biocatalyzed reactions and the\\nuse of DNA-functionalized magnetic nanoparticles. Digital XOR and NAND logic\\ngates and copying (fanout) are designed using the same components.', 'The notion of an equational shell is studied to involve the objects and their\\nenvironment. Appropriate methods are studied as valid embeddings of refined\\nobjects. The refinement process determines the linkages between the variety of\\npossible representations giving rise to variants of computations. The case\\nstudy is equipped with the adjusted equational systems that validate the\\ninitial applicative framework.', \"In this paper we investigate fair computations in the pi-calculus. Following\\nCosta and Stirling's approach for CCS-like languages, we consider a method to\\nlabel process actions in order to filter out unfair computations. We contrast\\nthe existing fair-testing notion with those that naturally arise by imposing\\nweak and strong fairness. This comparison provides insight about the\\nexpressiveness of the various `fair' testing semantics and about their\\ndiscriminating power.\", 'Indian languages have long history in World Natural languages. Panini was the\\nfirst to define Grammar for Sanskrit language with about 4000 rules in fifth\\ncentury. These rules contain uncertainty information. It is not possible to\\nComputer processing of Sanskrit language with uncertain information. In this\\npaper, fuzzy logic and fuzzy reasoning are proposed to deal to eliminate\\nuncertain information for reasoning with Sanskrit grammar. The Sanskrit\\nlanguage processing is also discussed in this paper.', 'We develop a version of the pi-calculus, picost, where channels are\\ninterpreted as resources which have costs associated with them. Code runs under\\nthe financial responsibility of owners; they must pay to use resources, but may\\nprofit by providing them. We provide a proof methodology for processes\\ndescribed in picost based on bisimulations. The underlying behavioural theory\\nis justified via a contextual characterisation. We also demonstrate its\\nusefulness via examples.', 'The geological surveying presently uses methods and tools for the computer\\nmodeling of 3D-structures of the geographical subsurface and geotechnical\\ncharacterization as well as the application of geoinformation systems for\\nmanagement and analysis of spatial data, and their cartographic presentation.\\nThe objectives of this paper are to present a 3D geological surface model of\\nLatur district in Maharashtra state of India. This study is undertaken through\\nthe several processes which are discussed in this paper to generate and\\nvisualize the automated 3D geological surface model of a projected area.', 'We show that for any constant d, complex roots of degree d univariate\\nrational (or Gaussian rational) polynomials---given by a list of coefficients\\nin binary---can be computed to a given accuracy by a uniform TC^0 algorithm (a\\nuniform family of constant-depth polynomial-size threshold circuits). The basic\\nidea is to compute the inverse function of the polynomial by a power series. We\\nalso discuss an application to the theory VTC^0 of bounded arithmetic.', \"We discuss historical attempts to formulate a physical hypothesis from which\\nTuring's thesis may be derived, and also discuss some related attempts to\\nestablish the computability of mathematical models in physics. We show that\\nthese attempts are all related to a single, unified hypothesis.\", 'In this paper we propose a hybrid model of a neural oscillator, obtained by\\npartially discretizing a well-known continuous model. Our construction points\\nout that in this case the standard techniques, based on replacing sigmoids with\\nstep functions, is not satisfactory. Then, we study the hybrid model through\\nboth symbolic methods and approximation techniques. This last analysis, in\\nparticular, allows us to show the differences between the considered\\napproximation approaches. Finally, we focus on approximations via\\nepsilon-semantics, proving how these can be computed in practice.', \"In a recent paper, Girard proposes to use his recent construction of a\\ngeometry of interaction in the hyperfinite factor in an innovative way to\\ncharacterize complexity classes. We begin by giving a detailed explanation of\\nboth the choices and the motivations of Girard's definitions. We then provide a\\ncomplete proof that the complexity class co-NL can be characterized using this\\nnew approach. We introduce as a technical tool the non-deterministic pointer\\nmachine, a concrete model to computes algorithms.\", 'Analysis of algorithms with complete knowledge of its inputs is sometimes not\\nup to our expectations. Many times we are surrounded with such scenarios where\\ninputs are generated without any prior knowledge. Online Algorithms have found\\ntheir applicability in broad areas of computer engineering. Among these, an\\nonline financial algorithm is one of the most important areas where lots of\\nefforts have been used to produce an efficient algorithm. In this paper various\\nOnline Algorithms have been reviewed for their efficiency and various\\nalternative measures have been explored for analysis purposes.', 'We present logically based methods for constructing XP and FPT graph\\nalgorithms, parametrized by tree-width or clique-width. We will use\\nfly-automata introduced in a previous article. They make possible to check\\nproperties that are not monadic second-order expressible because their states\\nmay include counters, so that their sets of states may be infinite. We equip\\nthese automata with output functions, so that they can compute values\\nassociated with terms or graphs. Rather than new algorithmic results we present\\ntools for constructing easily certain dynamic programming algorithms by\\ncombining predefined automata for basic functions and properties.', 'Computer Assisted Medical Intervention (CAMI hereafter) is a complex\\nmulti-disciplinary field. CAMI research requires the collaboration of experts\\nin several fields as diverse as medicine, computer science, mathematics,\\ninstrumentation, signal processing, mechanics, modeling, automatics, optics,\\netc.', 'We investigate a model for representing large multiplayer games, which\\nsatisfy strong symmetry properties. This model is made of multiple copies of an\\narena; each player plays in his own arena, and can partially observe what the\\nother players do. Therefore, this game has partial information and symmetry\\nconstraints, which make the computation of Nash equilibria difficult. We show\\nseveral undecidability results, and for bounded-memory strategies, we precisely\\ncharacterize the complexity of computing pure Nash equilibria (for qualitative\\nobjectives) in this game model.', 'Coalgebras generalize various kinds of dynamical systems occuring in\\nmathematics and computer science. Examples of systems that can be modeled as\\ncoalgebras include automata and Markov chains. We will present a coalgebraic\\nrepresentation of systems occuring in the field of quantum computation, using\\nconvex sets of density matrices as state spaces. This will allow us to derive a\\nmethod to convert quantum mechanical systems into simpler probabilistic systems\\nwith the same probabilistic behaviour.', 'We describe the implementation of a subfield of the field of formal Puiseux\\nseries in polymake. This is employed for solving linear programs and computing\\nconvex hulls depending on a real parameter. Moreover, this approach is also\\nuseful for computations in tropical geometry.', 'We propose a formalization of generic algorithms that includes analog\\nalgorithms. This is achieved by reformulating and extending the framework of\\nabstract state machines to include continuous-time models of computation. We\\nprove that every hybrid algorithm satisfying some reasonable postulates may be\\nexpressed precisely by a program in a simple and expressive language.', 'Recently, cloud systems composed of heterogeneous hardware have been\\nincreased to utilize progressed hardware power. However, to program\\napplications for heterogeneous hardware to achieve high performance needs much\\ntechnical skill and is difficult for users. Therefore, to achieve high\\nperformance easily, this paper proposes a PaaS which analyzes application\\nlogics and offloads computations to GPU and FPGA automatically when users\\ndeploy applications to clouds.', 'The main scientific heritage of Corrado B\\\\\"ohm consists of ideas about\\ncomputing, concerning concrete algorithms, as well as models of computability.\\nThe following will be presented. 1. A compiler that can compile itself. 2.\\nStructured programming, eliminating the \\'goto\\' statement. 3. Functional\\nprogramming and an early implementation. 4. Separability in {\\\\lambda}-calculus.\\n5. Compiling combinators without parsing. 6. Self-evaluation in\\n{\\\\lambda}-calculus.', 'The second international workshop on Computational Models for Cell Processes\\n(ComProc 2009) took place on November 3, 2009 at the Eindhoven University of\\nTechnology, in conjunction with Formal Methods 2009. The workshop was jointly\\norganized with the EC-MOAN project. This volume contains the final versions of\\nall contributions accepted for presentation at the workshop.', 'Since the proof of the four color theorem in 1976, computer-generated proofs\\nhave become a reality in mathematics and computer science. During the last\\ndecade, we have seen formal proofs using verified proof assistants being used\\nto verify the validity of such proofs.\\n  In this paper, we describe a formalized theory of size-optimal sorting\\nnetworks. From this formalization we extract a certified checker that\\nsuccessfully verifies computer-generated proofs of optimality on up to 8\\ninputs. The checker relies on an untrusted oracle to shortcut the search for\\nwitnesses on more than 1.6 million NP-complete subproblems.', 'In 1974 E.W. Dijkstra introduced the seminal concept of self-stabilization\\nthat turned out to be one of the main approaches to fault-tolerant computing.\\nWe show here how his three solutions can be formalized and reasoned about using\\nthe concepts of game theory. We also determine the precise number of steps\\nneeded to reach self-stabilization in his first solution.', 'Well-structured transition systems provide the right foundation to compute a\\nfinite basis of the set of predecessors of the upward closure of a state. The\\ndual problem, to compute a finite representation of the set of successors of\\nthe downward closure of a state, is harder: Until now, the theoretical\\nframework for manipulating downward-closed sets was missing. We answer this\\nproblem, using insights from domain theory (dcpos and ideal completions), from\\ntopology (sobrifications), and shed new light on the notion of adequate domains\\nof limits.', 'Computable analysis and effective descriptive set theory are both concerned\\nwith complete metric spaces, functions between them and subsets thereof in an\\neffective setting. The precise relationship of the various definitions used in\\nthe two disciplines has so far been neglected, a situation this paper is meant\\nto remedy.\\n  As the role of the Cauchy completion is relevant for both effective\\napproaches to Polish spaces, we consider the interplay of effectivity and\\ncompletion in some more detail.', 'As computer scientists working in bioinformatics/computational biology, we\\noften face the challenge of coming up with an algorithm to answer a biological\\nquestion. This occurs in many areas, such as variant calling, alignment, and\\nassembly. In this tutorial, we use the example of the genome assembly problem\\nto demonstrate how to go from a question in the biological realm to a solution\\nin the computer science realm. We show the modeling process step-by-step,\\nincluding all the intermediate failed attempts.', 'The monograph defines the conditions of training of future teachers in\\nnatural sciences and mathematics by means of computer simulation, developed a\\nstructural-functional model of training, selected socio-constructivist forms of\\norganization, methods and learning tools of computer modeling for future\\nteachers of natural and mathematical disciplines.', 'Almost all representations considered in computable analysis are partial. We\\nprovide arguments in favor of total representations (by elements of the Baire\\nspace). Total representations make the well known analogy between numberings\\nand representations closer, unify some terminology, simplify some technical\\ndetails, suggest interesting open questions and new invariants of topological\\nspaces relevant to computable analysis.', 'We show that the first order theory of the lattice of open sets in some\\nnatural topological spaces is $m$-equivalent to second order arithmetic. We\\nalso show that for many natural computable metric spaces and computable domains\\nthe first order theory of the lattice of effectively open sets is undecidable.\\nMoreover, for several important spaces (e.g., $\\\\mathbb{R}^n$, $n\\\\geq1$, and the\\ndomain $P\\\\omega$) this theory is $m$-equivalent to first order arithmetic.', \"We show how the language of Krivine's classical realizability may be used to\\nspecify various forms of nondeterminism and relate them with properties of\\nrealizability models. More specifically, we introduce an abstract notion of\\nmulti-evaluation relation which allows us to finely describe various\\nnondeterministic behaviours. This defines a hierarchy of computational models,\\nordered by their degree of nondeterminism, similar to Sazonov's degrees of\\nparallelism. What we show is a duality between the structure of the\\ncharacteristic Boolean algebra of a realizability model and the degree of\\nnondeterminism in its underlying computational model. ACM Reference Format:\\nGuillaume Geoffroy. 2018. Classical realizability as a classifier for\\nnondeter-minism.\", 'In this essay we\\'ll prove G\\\\\"odel\\'s incompleteness theorems twice. First,\\nwe\\'ll prove them the good old-fashioned way. Then we\\'ll repeat the feat in the\\nsetting of computation. In the process we\\'ll discover that G\\\\\"odel\\'s work,\\nrightly viewed, needs to be split into two parts: the transport of computation\\ninto the arena of arithmetic on the one hand and the actual incompleteness\\ntheorems on the other. After we\\'re done there will be cake.', 'The vision of a quantum internet is to fundamentally enhance Internet\\ntechnology by enabling quantum communication between any two points on Earth.\\nWhile the first realisations of small scale quantum networks are expected in\\nthe near future, scaling such networks presents immense challenges to physics,\\ncomputer science and engineering. Here, we provide a gentle introduction to\\nquantum networking targeted at computer scientists, and survey the state of the\\nart. We proceed to discuss key challenges for computer science in order to make\\nsuch networks a reality.', 'We extend the signal flow calculus---a compositional account of the classical\\nsignal flow graph model of computation---to encompass affine behaviour, and\\nfurnish it with a novel operational semantics. The increased expressive power\\nallows us to define a canonical notion of contextual equivalence, which we show\\nto coincide with denotational equality. Finally, we characterise the realisable\\nfragment of the calculus: those terms that express the computations of (affine)\\nsignal flow graphs.', 'Quantum computing is a growing field at the intersection of physics and\\ncomputer science. The goal of this article is to highlight a successfully\\ntrialled quantum computing course for high school students between the ages of\\n15 and 18 years old. This course bridges the gap between popular science\\narticles and advanced undergraduate textbooks. Conceptual ideas in the text are\\nreinforced with active learning techniques, such as interactive problem sets\\nand simulation-based labs at various levels. The course is freely available for\\nuse and download under the Creative Commons \"Attribution-\\nNonCommercial-ShareAlike 4.0 International\" license.', 'The unique challenges associated with imaging a black hole motivated the\\ndevelopment of new computational imaging algorithms. As the Event Horizon\\nTelescope continues to expand, these algorithms will need to evolve to keep\\npace with the increasingly demanding volume and dimensionality of the data.', 'We present an inductive spatio-temporal learning framework rooted in\\ninductive logic programming. With an emphasis on visuo-spatial language, logic,\\nand cognition, the framework supports learning with relational spatio-temporal\\nfeatures identifiable in a range of domains involving the processing and\\ninterpretation of dynamic visuo-spatial imagery. We present a prototypical\\nsystem, and an example application in the domain of computing for visual arts\\nand computational cognitive science.', 'We propose a method to count the number of reachable markings of a Petri net\\nwithout having to enumerate these rst. The method relies on a structural\\nreduction system that reduces the number of places and transitions of the net\\nin such a way that we can faithfully compute the number of reachable markings\\nof the original net from the reduced net and the reduction history. The method\\nhas been implemented and computing experiments show that reductions are eective\\non a large benchmark of models.', 'Place/Transition Petri nets with inhibitor arcs (PTI nets for short), which\\nare a well-known Turing-complete, distributed model of computation, are\\nequipped with a decidable, behavioral equivalence, called pti-place\\nbisimilarity, that conservatively extends place bisimilarity defined over\\nPlace/Transition nets (without inhibitor arcs). We prove that pti-place\\nbisimilarity is sensible, as it respects the causal semantics of PTI nets.', 'ZX-Calculus is a versatile graphical language for quantum computation\\nequipped with an equational theory. Getting inspiration from Geometry of\\nInteraction, in this paper we propose a token-machine-based asynchronous model\\nof both pure ZX-Calculus and its extension to mixed processes. We also show how\\nto connect this new semantics to the usual standard interpretation of\\nZX-diagrams. This model allows us to have a new look at what ZX-diagrams\\ncompute, and give a more local, operational view of the semantics of\\nZX-diagrams.', 'We consider the paradoxical concept of free will from the perspective of\\nTheoretical Computer Science (TCS), a branch of mathematics concerned with\\nunderstanding the underlying principles of computation and complexity,\\nincluding the implications and surprising consequences of resource limitations.', 'The words ``Programming is the second literacy\\'\\' were coined more than 40\\nyears ago but never came to life. This paper is one in the series of papers\\naimed at the analysis of mathematical requirements for a merge of school\\nmathematics with computer science and computer programming. First indications\\nare this demands development of quite serious mathematical tools most of which,\\nhopefully, will be hidden \"under the hood\\'\\' of software systems used in the\\nprocess, but many will feature prominently in the Domain Specific Language\\nneeded for support of mathematical exchanges between Learner, Teacher, and\\nComputer. We focus on \"hardcore\" mathematical aspects of this development.', 'A new Genetic Programming variant called Liquid State Genetic Programming\\n(LSGP) is proposed in this paper. LSGP is a hybrid method combining a dynamic\\nmemory for storing the inputs (the liquid) and a Genetic Programming technique\\nused for the problem solving part. Several numerical experiments with LSGP are\\nperformed by using several benchmarking problems. Numerical experiments show\\nthat LSGP performs similarly and sometimes even better than standard Genetic\\nProgramming for the considered test problems.', 'Computation with advice is suggested as generalization of both computation\\nwith discrete advice and Type-2 Nondeterminism. Several embodiments of the\\ngeneric concept are discussed, and the close connection to Weihrauch\\nreducibility is pointed out. As a novel concept, computability with random\\nadvice is studied; which corresponds to correct solutions being guessable with\\npositive probability. In the framework of computation with advice, it is\\npossible to define computational complexity for certain concepts of\\nhypercomputation. Finally, some examples are given which illuminate the\\ninterplay of uniform and non-uniform techniques in order to investigate both\\ncomputability with advice and the Weihrauch lattice.', 'We introduce the operators \"modified limit\" and \"accumulation\" on a Banach\\nspace, and we use this to define what we mean by being internally computable\\nover the space. We prove that any externally computable function from a\\ncomputable metric space to a computable Banach space is internally computable.\\nWe motivate the need for internal concepts of computability by observing that\\nthe complexity of the set of finite sets of closed balls with a nonempty\\nintersection is not uniformly hyperarithmetical, and thus that approximating an\\nexternally computable function is highly complex.', \"Among the strategic choices made by today's economic actors are choices about\\nalgorithms and computational resources. Different access to computational\\nresources may result in a kind of economic asymmetry analogous to information\\nasymmetry. In order to represent strategic computational choices within a game\\ntheoretic framework, we propose a new game specification, Strategic Bayesian\\nNetworks (SBN). In an SBN, random variables are represented as nodes in a\\ngraph, with edges indicating probabilistic dependence. For some nodes, players\\ncan choose conditional probability distributions as a strategic choice. Using\\nSBN, we present two games that demonstrate computational asymmetry. These games\\nare symmetric except for the computational limitations of the actors. We show\\nthat the better computationally endowed player receives greater payoff.\", \"IT based scientific research requires high computational resources. The\\nlimitation on funding and infrastructure led the high performance computing era\\nfrom supercomputer to cluster and grid computing technology. Parallel\\napplication running well on cluster computer as well as supercomputer, one of\\nthe type is embarrassingly parallel application. Many scientist loves EP\\nbecause it doesn't need any sophisticated technique but gives amazing\\nperformance. This paper discuss the bioinformatics research that used\\nembarrassingly application and show its performance on cluster computer.\", 'Biologically inspired computing is an area of computer science which uses the\\nadvantageous properties of biological systems. It is the amalgamation of\\ncomputational intelligence and collective intelligence. Biologically inspired\\nmechanisms have already proved successful in achieving major advances in a wide\\nrange of problems in computing and communication systems. The consortium of\\nbio-inspired computing are artificial neural networks, evolutionary algorithms,\\nswarm intelligence, artificial immune systems, fractal geometry, DNA computing\\nand quantum computing, etc. This article gives an introduction to swarm\\nintelligence.', 'In computability theory and computable analysis, finite programs can compute\\ninfinite objects. Presenting a computable object via any program for it,\\nprovides at least as much information as presenting the object itself, written\\non an infinite tape. What additional information do programs provide? We\\ncharacterize this additional information to be any upper bound on the\\nKolmogorov complexity of the object. Hence we identify the exact relationship\\nbetween Markov-computability and Type-2-computability. We then use this\\nrelationship to obtain several results characterizing the computational and\\ntopological structure of Markov-semidecidable sets.', 'We investigate the topological aspects of some algebraic computation models,\\nin particular the BSS-model. Our results can be seen as bounds on how different\\nBSS-computability and computability in the sense of computable analysis can be.\\nThe framework for this is Weihrauch reducibility. As a consequence of our\\ncharacterizations, we establish that the solvability complexity index is\\n(mostly) independent of the computational model, and that there thus is common\\nground in the study of non-computability between the BSS and TTE setting.', 'This study examined the influence of computer anxiety on the academic\\nperformance of junior secondary school students in Computer Studies in Nigeria.\\nThe research instrument that was used in the study was computer anxiety scale\\nwhich was validated by the Guidance and Counseling lecturers and Educational\\nMeasurement and Evaluation experts. The result of the study showed that most of\\nthe students used in the study were mildly anxious when dealing with computer.', 'Turing computability is the standard computability paradigm which captures\\nthe computational power of digital computers. To understand whether one can\\ncreate physically realistic devices which have super-Turing power, one needs to\\nunderstand whether it is possible to obtain systems with super-Turing\\ncapabilities which also have other desirable properties such as robustness to\\nperturbations. In this paper we introduce a framework for analyzing whether a\\nnon-computability result is robust over continuous spaces. Then we use this\\nframework to study the degree of robustness of several non-computability\\nresults which involve the wave equation, differentiation, and basins of\\nattraction.', 'Serverless computing has become an important model in cloud computing and\\ninfluenced the design of many applications. Here, we provide our perspective on\\nhow the recent landscape of serverless computing for scientific applications\\nlooks like. We discuss the advantages and problems with serverless computing\\nfor scientific applications, and based on the analysis of existing solutions\\nand approaches, we propose a science-oriented architecture for a serverless\\ncomputing framework that is based on the existing designs. Finally, we provide\\nan outlook of current trends and future directions.', 'Classical models of computation have been successful in capturing the very\\nessence of individual computing devices. Although they are useful to understand\\ncomputability power and limitations in the small, such models are not suitable\\nto study large-scale complex computations. Accordingly, plenty of formalisms\\nhave been proposed in the last half century as an attempt to raise the level of\\nabstraction, with the aim of describing not only a single computing device but\\ninteractions among a collection of them. In this paper, we encompass such\\nformalisms into a common framework which we refer to as Models of High-Level\\nComputation. We particularly discuss the semantics, some of the key properties,\\nparadigms and future directions of such models.', 'At the frontier of most areas in science, computer simulations play a central\\nrole. The traditional division of natural science into experimental and\\ntheoretical investigations is now completely outdated. Instead, theory,\\nsimulation, and experimentation form three equally essential aspects, each with\\nits own unique flavor and challenges. Yet, education in computational science\\nis still lagging far behind, and the number of text books in this area is\\nminuscule compared to the many text books on theoretical and experimental\\nscience. As a result, many researchers still carry out simulations in a\\nhaphazard way, without properly setting up the computational equivalent of a\\nwell equipped laboratory. The art of creating such a virtual laboratory, while\\nproviding proper extensibility and documentation, is still in its infancy. A\\nnew approach is described here, Open Knowledge, as an extension of the notion\\nof Open Source software. Besides open source code, manuals, and primers, an\\nopen knowledge project provides simulated dialogues between code developers,\\nthus sharing not only the code, but also the motivations behind the code.', 'A first-principles method, based on density functional perturbation theory,\\nis presented for computing the leading order tunability of\\nhigh-dielectric-constant materials.', 'This submission was removed because it contained proprietary information that\\nwas distributed without permission.', 'This paper describes the semantics and ideas about SKY, a logic programming\\nlanguage intended in order to specify algorithmic strategies for the evaluation\\nof problems.', 'We explore how different proof orderings induce different notions of\\nsaturation. We relate completion, paramodulation, saturation, redundancy\\nelimination, and rewrite system reduction to proof orderings.', 'We present the bounded delays, the absolute inertia and the relative inertia.', 'This article illustrates the use of a logical specification language to\\ncapture various forms of confidentiality properties used in the security\\nliterature.', 'Withdrawn due to critical error.', 'A introduction to the syntax and Semantics of Answer Set Programming intended\\nas an handout to [under]graduate students taking Artificial Intlligence or\\nLogic Programming classes.', 'This paper discusses the use of fat-tailed distributions in catastrophe\\nprediction as opposed to the more common use of the Normal Distribution.', \"I will present my implementation 'n-units' of physical units into C++\\nprograms. It allows the compiler to check for dimensional consistency.\", 'A few considerations on the nature of Economics and its relationship to human\\ncommunities through the prism of Self-Organizing-Systems.', 'We describe the basic notions of co-induction as they are available in the\\ncoq system. As an application, we describe arithmetic properties for simple\\nrepresentations of real numbers.', 'This article introduce a new data storage method called DWDD(Domain Wall\\nDisplacement Detection) and tell you why it succeed.', 'We study the properties, in particular termination, of dependent types\\nsystems for lambda calculus and rewriting.', 'This article surveys recent advances in applying algebraic techniques to\\nconstraint satisfaction problems.', 'Lower bounds for some explicit decision problems over the complex numbers are\\ngiven.', 'Some years ago I demonstrated a simulated annealing heuristic for the\\nHamiltonian cycle problem (Science 273, 413 (1996)). Here I propose an improved\\nversion of this heuristic.', 'We present here an installation guide, a hand-on mini-tutorial through\\nexamples, and the theoretical foundations of the Hilbert++ code.', 'Please see the content of this report.', 'These are the preparatory notes for a Science & Music essay, \"Playing by\\nnumbers\", appeared in Nature 453 (2008) 988-989.', 'Looks at state interactions from an agent based AI perspective to see state\\ninteractions as an example of emergent intelligent behavior. Exposes basic\\nprinciples of game theory.', 'An attempt of a new kind of complexity anthropology is considered.', 'Lindstr\\\\\"om theorems characterize logics in terms of model-theoretic\\nconditions such as Compactness and the L\\\\\"owenheim-Skolem property. Most\\nexisting characterizations of this kind concern extensions of first-order\\nlogic. But on the other hand, many logics relevant to computer science are\\nfragments or extensions of fragments of first-order logic, e.g., k-variable\\nlogics and various modal logics. Finding Lindstr\\\\\"om theorems for these\\nlanguages can be challenging, as most known techniques rely on coding arguments\\nthat seem to require the full expressive power of first-order logic. In this\\npaper, we provide Lindstr\\\\\"om theorems for several fragments of first-order\\nlogic, including the k-variable fragments for k>2, Tarski\\'s relation algebra,\\ngraded modal logic, and the binary guarded fragment. We use two different proof\\ntechniques. One is a modification of the original Lindstr\\\\\"om proof. The other\\ninvolves the modal concepts of bisimulation, tree unraveling, and finite depth.\\nOur results also imply semantic preservation theorems.', 'Short philosophical essay', 'We prove that the Betti numbers of simplicial complexes of bounded vertex\\ndegrees are testable in constant time.', 'In this paper, design of current controller for a two quadrant DC motor drive\\nwas proposed with the help of model order reduction technique. The calculation\\nof current controller gain with some approximations in the conventional design\\nprocess is replaced by proposed model order reduction method. The model order\\nreduction technique proposed in this paper gives the better controller gain\\nvalue for the DC motor drive. The proposed model order reduction method is a\\nmixed method, where the numerator polynomial of reduced order model is obtained\\nby using stability equation method and the denominator polynomial is obtained\\nby using some approximation technique preceded in this paper. The designed\\ncontrollers responses were simulated with the help of MATLAB to show the\\nvalidity of the proposed method.', 'This paper presents a probabilistic approach for DNA sequence analysis. A DNA\\nsequence consists of an arrangement of the four nucleotides A, C, T and G and\\ndifferent representation schemes are presented according to a probability\\nmeasure associated with them. There are different ways that probability can be\\nassociated with the DNA sequence: one way is when the probability of an\\noccurrence of a letter does not depend on the previous one (termed as\\nunsuccessive probability) and in another scheme the probability of occurrence\\nof a letter depends on its previous letter (termed as successive probability).\\nFurther, based on these probability measures graphical representations of the\\nschemes are also presented. Using the diagram probability measure one can\\neasily calculate an associated probability measure which can serve as a\\nparameter to check how close is a new sequence to already existing ones.', 'Formal semantics offers a complete and rigorous definition of a language. It\\nis important to define different semantic models for a language and different\\nmodels serve different purposes. Building equivalence between different\\nsemantic models of a language strengthen its formal foundation. This paper\\nshows the derivation of denotational semantics from operational semantics of\\nthe language cCSP. The aim is to show the correspondence between operational\\nand trace semantics. We extract traces from operational rules and use induction\\nover traces to show the correspondence between the two semantics of cCSP.', 'The Description Logic EL has recently drawn considerable attention since, on\\nthe one hand, important inference problems such as the subsumption problem are\\npolynomial. On the other hand, EL is used to define large biomedical\\nontologies. Unification in Description Logics has been proposed as a novel\\ninference service that can, for example, be used to detect redundancies in\\nontologies. The main result of this paper is that unification in EL is\\ndecidable. More precisely, EL-unification is NP-complete, and thus has the same\\ncomplexity as EL-matching. We also show that, w.r.t. the unification type, EL\\nis less well-behaved: it is of type zero, which in particular implies that\\nthere are unification problems that have no finite complete set of unifiers.', 'This paper discuss two structures of WLAN system fit to Passenger Information\\nDisplay System which is partly of subway.', 'This is the preface to the 26th International Conference on Logic Programming\\nSpecial Issue', 'Deduction systems and graph rewriting systems are compared within a common\\ncategorical framework. This leads to an improved deduction method in\\ndiagrammatic logics.', 'Sound behavioral equations on open terms may become unsound after\\nconservative extensions of the underlying operational semantics. Providing\\ncriteria under which such equations are preserved is extremely useful; in\\nparticular, it can avoid the need to repeat proofs when extending the specified\\nlanguage.\\n  This paper investigates preservation of sound equations for several notions\\nof bisimilarity on open terms: closed-instance (ci-)bisimilarity and\\nformal-hypothesis (fh-)bisimilarity, both due to Robert de Simone, and\\nhypothesis-preserving (hp-)bisimilarity, due to Arend Rensink. For both\\nfh-bisimilarity and hp-bisimilarity, we prove that arbitrary sound equations on\\nopen terms are preserved by all disjoint extensions which do not add labels. We\\nalso define slight variations of fh- and hp-bisimilarity such that all sound\\nequations are preserved by arbitrary disjoint extensions. Finally, we give two\\nsets of syntactic criteria (on equations, resp. operational extensions) and\\nprove each of them to be sufficient for preserving ci-bisimilarity.', 'We combine previous work on coalgebraic logic with the coalgebraic traces\\nsemantics of Hasuo, Jacobs, and Sokolova.', 'This article defines a complement of a function and conditions for existence\\nof such a complement function and presents few algorithms to construct a\\ncomplement.', 'We solve the game of Babylon when played with chips of two colors, giving a\\nwinning strategy for the second player in all previously unsolved cases.', 'In this paper we generalize the DP framework to a relative DP framework,\\nwhere a so called split is possible.', 'In this paper, we compare different existing approaches employed in data\\nmining of big proof libraries in automated and interactive theorem proving.', 'Rule-based run-time monitoring system for finite traces, with FLTL verdict\\nand quadratic complexity.', 'We introduce basic notions and results about relation liftings on categories\\nenriched in a commutative quantale. We derive two necessary and sufficient\\nconditions for a 2-functor T to admit a functorial relation lifting: one is the\\nexistence of a distributive law of T over the \"powerset monad\" on categories,\\none is the preservation by T of \"exactness\" of certain squares. Both\\ncharacterisations are generalisations of the \"classical\" results known for set\\nfunctors: the first characterisation generalises the existence of a\\ndistributive law over the genuine powerset monad, the second generalises\\npreservation of weak pullbacks. The results presented in this paper enable us\\nto compute predicate liftings of endofunctors of, for example, generalised\\n(ultra)metric spaces. We illustrate this by studying the coalgebraic cover\\nmodality in this setting.', 'Meta SOS is a software framework designed to integrate the results from the\\nmeta-theory of structural operational semantics (SOS). These results include\\nderiving semantic properties of language constructs just by syntactically\\nanalyzing their rule-based definition, as well as automatically deriving sound\\nand ground-complete axiomatizations for languages, when considering a notion of\\nbehavioural equivalence. This paper describes the Meta SOS framework by\\nblending aspects from the meta-theory of SOS, details on their implementation\\nin Maude, and running examples.', \"Not only did Turing help found one of the most exciting areas of modern\\nscience (computer science), but it may be that his contribution to our\\nunderstanding of our physical reality is greater than we had hitherto supposed.\\nHere I explore the path that Alan Turing would have certainly liked to follow,\\nthat of complexity science, which was launched in the wake of his seminal work\\non computability and structure formation. In particular, I will explain how the\\ntheory of algorithmic probability based on Turing's universal machine can also\\nexplain how structure emerges at the most basic level, hence reconnecting two\\nof Turing's most cherished topics: computation and pattern formation.\", 'This paper shows that it is computationally hard to decide (or test) if a\\nconsumption data set is consistent with separable preferences.', 'We present a prototype tool for automated reasoning for Coalition Logic, a\\nnon-normal modal logic that can be used for reasoning about cooperative agency.\\nThe theorem prover CLProver is based on recent work on a resolution-based\\ncalculus for Coalition Logic that operates on coalition problems, a normal form\\nfor Coalition Logic. We provide an overview of coalition problems and of the\\nresolution-based calculus for Coalition Logic. We then give details of the\\nimplementation of CLProver and present the results for a comparison with an\\nexisting tableau-based solver.', 'We introduce the CP*-construction on a dagger compact closed category as a\\ngeneralisation of Selinger\\'s CPM-construction. While the latter takes a dagger\\ncompact closed category and forms its category of \"abstract matrix algebras\"\\nand completely positive maps, the CP*-construction forms its category of\\n\"abstract C*-algebras\" and completely positive maps. This analogy is justified\\nby the case of finite-dimensional Hilbert spaces, where the CP*-construction\\nyields the category of finite-dimensional C*-algebras and completely positive\\nmaps.\\n  The CP*-construction fully embeds Selinger\\'s CPM-construction in such a way\\nthat the objects in the image of the embedding can be thought of as \"purely\\nquantum\" state spaces. It also embeds the category of classical stochastic\\nmaps, whose image consists of \"purely classical\" state spaces. By allowing\\nclassical and quantum data to coexist, this provides elegant abstract notions\\nof preparation, measurement, and more general quantum channels.', 'We resolve an open problem concerning finite logical implication for path\\nfunctional dependencies (PFDs).', 'This article introduce a new model theory call non-predetermined model theory\\nwhere functions and relations need not to be determined already and they are\\ndetermined through time.', 'We consider the value 1 problem for probabilistic automata over finite words:\\nit asks whether a given probabilistic automaton accepts words with probability\\narbitrarily close to 1. This problem is known to be undecidable. However,\\ndifferent algorithms have been proposed to partially solve it; it has been\\nrecently shown that the Markov Monoid algorithm, based on algebra, is the most\\ncorrect algorithm so far. The first contribution of this paper is to give a\\ncharacterisation of the Markov Monoid algorithm. The second contribution is to\\ndevelop a profinite theory for probabilistic automata, called the prostochastic\\ntheory. This new framework gives a topological account of the value 1 problem,\\nwhich in this context is cast as an emptiness problem. The above\\ncharacterisation is reformulated using the prostochastic theory, allowing us to\\ngive a simple and modular proof.', 'This paper is a survey on Deduction modulo theory', \"An axiomatisation of Hurkens's paradox in dependent type theory is given\\nwithout assuming any impredicative feature of said type theory.\", 'We present concisely the method \"Model Elimination\" of D.W.Loveland.\\nEspecially, we explain and prove the correctness of the lemmas generated by\\nthis method.', 'We present the logic iJT4, which is an explicit version of intuitionistic S4\\nand establish soundness and completeness with respect to modular models.', 'This document describes the DRAT format for clausal proofs and the DRAT-trim\\nproof checker.', 'Two Turing Machines may be able to answer questions about each other that\\nthey cannot answer about themselves.', 'A variant of Phragm\\\\\\'en\\'s method for proportional representation via approval\\nvoting is briefly explored. Instead of D\\'Hondt\\'s rule, this variant generalizes\\nSainte-Lagu\\\\\"e\\'s rule.', 'There is still a lot of confusion about \"optimal\" sharing in the lambda\\ncalculus, and its actual efficiency. In this article, we shall try to clarify\\nsome of these issues.', 'Building on the concept of local lexing the concept of parameterized local\\nlexing is introduced.', 'How to bid on a Google shopping account (set of shopping campaigns) with\\nquery-level matching like in Google Adwords.', 'A HelloWord \\\\textsc{Bib}\\\\negthinspace\\\\TeX~stile file .\\\\textbf{bst} is\\ndescribed', 'We present a selective bibliography about efficient SAT solving, focused on\\noptimizations for the CDCL-based algorithms.', \"A fundamental problem of Einstein's theory of classical general relativity is\\nthe existence of singularities such as the big bang. All known laws of physics\\nend at these boundaries of classical space-time. Thanks to recent developments\\nin quantum gravity, supercomputers are now playing an important role in\\nunderstanding the resolution of big bang and black hole singularities. Using\\nsupercomputers, explorations of the very genesis of space and time from quantum\\ngeometry are revealing a novel picture of what lies beyond classical\\nsingularities and the new physics of the birth of our universe.\", 'Report on Humies competition at GECCO 2018 in Japan', 'Using explicit weakenings, we can define alpha-conversion by simple equations\\nwithout any mention of free variables.', 'We propose a new approach to testing conformance to a nondeterministic\\nspecification, in which testing proceeds only as long as increased test\\ncoverage is guaranteed.', \"We introduce two-sorted theories in the style of [CN10] for the complexity\\nclasses \\\\oplusL and DET, whose complete problems include determinants over Z2\\nand Z, respectively. We then describe interpretations of Soltys' linear algebra\\ntheory LAp over arbitrary integral domains, into each of our new theories. The\\nresult shows equivalences of standard theorems of linear algebra over Z2 and Z\\ncan be proved in the corresponding theory, but leaves open the interesting\\nquestion of whether the theorems themselves can be proved.\", 'We discuss how to generate singled peaked votes uniformly from the Impartial\\nCulture model.', 'Data science is an emerging interdisciplinary field that combines elements of\\nmathematics, statistics, computer science, and knowledge in a particular\\napplication domain for the purpose of extracting meaningful information from\\nthe increasingly sophisticated array of data available in many settings. These\\ndata tend to be non-traditional, in the sense that they are often live, large,\\ncomplex, and/or messy. A first course in statistics at the undergraduate level\\ntypically introduces students with a variety of techniques to analyze small,\\nneat, and clean data sets. However, whether they pursue more formal training in\\nstatistics or not, many of these students will end up working with data that is\\nconsiderably more complex, and will need facility with statistical computing\\ntechniques. More importantly, these students require a framework for thinking\\nstructurally about data. We describe an undergraduate course in a liberal arts\\nenvironment that provides students with the tools necessary to apply data\\nscience. The course emphasizes modern, practical, and useful skills that cover\\nthe full data analysis spectrum, from asking an interesting question to\\nacquiring, managing, manipulating, processing, querying, analyzing, and\\nvisualizing data, as well communicating findings in written, graphical, and\\noral forms.', 'We give an interpretation of full classical linear logic, and linear proofs\\nin terms of operations on the blockchain.', 'We Propose 22 unique Solutions to the Genetic Code. An Alternative Cracking,\\nfrom the Perspective of a Mathematician.', 'We consider a specific class of tree structures that can represent basic\\nstructures in linguistics and computer science such as XML documents, parse\\ntrees, and treebanks, namely, finite node-labeled sibling-ordered trees. We\\npresent axiomatizations of the monadic second-order logic (MSO), monadic\\ntransitive closure logic (FO(TC1)) and monadic least fixed-point logic\\n(FO(LFP1)) theories of this class of structures. These logics can express\\nimportant properties such as reachability. Using model-theoretic techniques, we\\nshow by a uniform argument that these axiomatizations are complete, i.e., each\\nformula that is valid on all finite trees is provable using our axioms. As a\\nbackdrop to our positive results, on arbitrary structures, the logics that we\\nstudy are known to be non-recursively axiomatizable.', 'The best way to understand complex data structures or algorithm is to see\\nthem in action. The present work presents a new tool, especially useful for\\nstudents and lecturers in computer science. It is written in Java and developed\\nat Bordeaux University of Sciences and Technology. Its purposes is to help\\nstudents in understanding classical algorithms by illustrating them in\\ndifferent ways: graphical (animated), formal, and descriptive. We think that it\\ncan be useful to everyone interested in algorithms, in particular to students\\nin computer science that want to beef up their readings and university\\nlecturers in their major effort to enhance the data structures and algorithms\\ncourse. The main new thing of this tool is the fact of making it possible to\\nthe user to animate their own algorithms.', 'We present an SMT encoding of a generalized version of the subterm criterion\\nand evaluate its implementation in TTT2.', 'This is an attempt to illustrate the glorious history of logical foundations\\nand to discuss the uncertain future.', 'We introduce some preliminaries about game theory and information security.\\nThen surveying a subset of the literature, we identify opportunities for future\\nresearch.', 'Behavioral synthesis involves compiling an Electronic System-Level (ESL)\\ndesign into its Register-Transfer Level (RTL) implementation. Loop pipelining\\nis one of the most critical and complex transformations employed in behavioral\\nsynthesis. Certifying the loop pipelining algorithm is challenging because\\nthere is a huge semantic gap between the input sequential design and the output\\npipelined implementation making it infeasible to verify their equivalence with\\nautomated sequential equivalence checking techniques. We discuss our ongoing\\neffort using ACL2 to certify loop pipelining transformation. The completion of\\nthe proof is work in progress. However, some of the insights developed so far\\nmay already be of value to the ACL2 community. In particular, we discuss the\\nkey invariant we formalized, which is very different from that used in most\\npipeline proofs. We discuss the needs for this invariant, its formalization in\\nACL2, and our envisioned proof using the invariant. We also discuss some\\ntrade-offs, challenges, and insights developed in course of the project.', 'Online Appendix to: \"Analyzing Control Flow Information to Improve the\\nEffectiveness of Process Model Matching Techniques\" by the same authors.', 'This note is about encoding Turing machines into the lambda-calculus.', 'We prove some constructive results that on first and maybe even on second\\nglance seem impossible.', 'The Halting Problem is ill-conceived and ill-defined.', \"The Halting Problem is a version of the Liar's Paradox.\", 'We present the guarded lambda-calculus, an extension of the simply typed\\nlambda-calculus with guarded recursive and coinductive types. The use of\\nguarded recursive types ensures the productivity of well-typed programs.\\nGuarded recursive types may be transformed into coinductive types by a\\ntype-former inspired by modal logic and Atkey-McBride clock quantification,\\nallowing the typing of acausal functions. We give a call-by-name operational\\nsemantics for the calculus, and define adequate denotational semantics in the\\ntopos of trees. The adequacy proof entails that the evaluation of a program\\nalways terminates. We introduce a program logic with L\\\\\"ob induction for\\nreasoning about the contextual equivalence of programs. We demonstrate the\\nexpressiveness of the calculus by showing the definability of solutions to\\nRutten\\'s behavioural differential equations.', 'Research methods are essential parts in conducting any research project.\\nAlthough they have been theorized and summarized based on best practices, every\\nfield of science requires an adaptation of the overall approaches to perform\\nresearch activities. In addition, any specific research needs a particular\\nadjustment to the generalized approach and specializing them to suit the\\nproject in hand. However, unlike most well-established science disciplines,\\ncomputing research is not supported by well-defined, globally accepted methods.\\nThis is because of its infancy and ambiguity in its definition, on one hand,\\nand its extensive coverage and overlap with other fields, on the other hand.\\nThis article discusses the research methods in science and engineering in\\ngeneral and in computing in particular. It shows that despite several special\\nparameters that make research in computing rather unique, it still follows the\\nsame steps that any other scientific research would do. The article also shows\\nthe particularities that researchers need to consider when they conduct\\nresearch in this field.', 'We discuss the problem of defining a logic for analogical reasoning, and\\nsketch a solution in the style of the semantics for Counterfactual\\nConditionals, Preferential Structures, etc.', 'We correct our proof of a theorem stating that satisfiability of frequency\\nlinear-time temporal logic is undecidable [TASE 2012].', 'In this note, we discuss a number of parametricity features and what their\\nrequirements are in terms of complexity of the type system and its model.', \"In this paper, we deal with the problem of implementing an abstract machine\\nfor a stochastic version of the Brane Calculus. Instead of defining an ad hoc\\nabstract machine, we consider the generic stochastic abstract machine\\nintroduced by Lakin, Paulev\\\\'e and Phillips. The nested structure of membranes\\nis flattened into a set of species where the hierarchical structure is\\nrepresented by means of names. In order to reduce the overhead introduced by\\nthis encoding, we modify the machine by adding a copy-on-write optimization\\nstrategy. We prove that this implementation is adequate with respect to the\\nstochastic structural operational semantics recently given for the Brane\\nCalculus. These techniques can be ported also to other stochastic calculi\\ndealing with nested structures.\", 'We present GPU accelerated implementations of Markov chain algorithms to\\nsample random tilings, dimers, and the six-vertex model.', \"An introduction to algebras for graphs, based on Courcelle's algebras of\\nhyperedge replacement and vertex replacement. The paper uses monad notation.\", 'In this short paper, using category theory, we argue that logical rules can\\nbe seen as fractions and logics as limit sketches.', 'A century ago, discoveries of a serious kind of logical error made separately\\nby several leading mathematicians led to acceptance of a sharply enhanced\\nstandard for rigor within what ultimately became the foundation for Computer\\nScience. By 1931, Godel had obtained a definitive and remarkable result: an\\ninherent limitation to that foundation. The resulting limitation is not\\napplicable to actual human cognition, to even the smallest extent, unless both\\nof these extremely brittle assumptions hold: humans are infallible reasoners\\nand reason solely via formal inference rules. Both assumptions are contradicted\\nby empirical data from well-known Cognitive Science experiments. This article\\ninvestigates how a novel multi-part methodology recasts computability theory\\nwithin Computer Science to obtain a definitive limitation whose application to\\nhuman cognition avoids assumptions contradicting empirical data. The limitation\\napplies to individual humans, to finite sets of humans, and more generally to\\nany real-world entity.', 'A reinforcement learning algorithm accomplishes the task of synthesizing a\\nset-theoretical formula that evaluates to given truth values for given\\nassignments.', \"One of the capabilities which 21st-century skill compulsory a person is\\ncritical thinking and problem-solving skill that becomes top positions rank.\\nFocus on problem-solving skills can be taught to a child, especially begun in\\nelementary school refer to prior research focus on K-12. Computational thinking\\nwas one problem-solving skill that popular to implemented and studied in the\\ncurrent decade. This study was conducted to explore students' capability to be\\nable solving of the problem based on the possibility use the computational\\nthinking way. Participants in this study came from six international students\\nthat study in Taiwan and from two deferent sciences disciplines, engineering,\\nand social science. A qualitative method was used to analyze data interviews,\\ntook example cases from the global issue that is Climate Change. The result\\nfounded that survive in a new environment was become evidence of their\\nimplementation of problem-solving skills. Problem-solving mindset both students\\nof engineering and social science had discrepancy, those are how to use precise\\nstructure in the algorithm.\", 'Recent developments in softwarization of networked infrastructures combined\\nwith containerization of computing workflows promise unprecedented compute\\nanywhere and everywhere capabilities for federations of edge and remote\\ncomputing systems and science instruments. The development and testing of\\nsoftware stacks that implement these capabilities over physical production\\nfederations, however, is not very practical nor cost-effective. In response, we\\ndevelop a digital twin of the physical infrastructure, called the Virtual\\nFederated Science Instrument Environment (VFSIE). This framework emulates the\\nfederation using containers and hosts connected over an emulated network, and\\nsupports the development and testing of federation stacks and workflows. We\\nillustrate its use in a case study involving Jupiter Notebook computations and\\ninstrument control.', 'Randomized algorithms have propelled advances in artificial intelligence and\\nrepresent a foundational research area in advancing AI for Science. Future\\nadvancements in DOE Office of Science priority areas such as climate science,\\nastrophysics, fusion, advanced materials, combustion, and quantum computing all\\nrequire randomized algorithms for surmounting challenges of complexity,\\nrobustness, and scalability. This report summarizes the outcomes of that\\nworkshop, \"Randomized Algorithms for Scientific Computing (RASC),\" held\\nvirtually across four days in December 2020 and January 2021.', 'We present a formulation of the Collatz conjecture that is potentially more\\namenable to modeling and analysis by automated termination checking tools.', 'We investigate the expressive power of the two main kinds of program logics\\nfor complex, non-regular program properties found in the literature: those\\nextending propositional dynamic logic (PDL), and those extending the modal\\nmu-calculus. This is inspired by the recent discovery of a decidable program\\nlogic called Visibly Pushdown Fixpoint Logic with Chop which extends both the\\nmodal mu-calculus and PDL over visibly pushdown languages, which, so far,\\nconstituted the ends of two pillars of decidable program logics.\\n  Here we show that this logic is not only more expressive than either of its\\ntwo fragments, but in fact even more expressive than their union. Hence, the\\ndecidability border amongst program logics has been properly pushed up. We\\ncomplete the picture by providing results separating all the PDL-based and\\nmodal fixpoint logics with regular, visibly pushdown and arbitrary context-free\\nconstructions.', 'In this paper, we highlight a profound difference between conditional\\nstatements in mathematical logic and natural languages. This difference exists\\neven when the conditional statements are used in mathematical theorems.', 'We establish coNExpTime-completeness of the problem of deciding\\norder-invariance of a given two variable first-order formula, improving and\\nsignificantly simplifying coTwoNExpTime bound by Zeume and Harwath.', 'This paper is simply a collection of process diagrams for further use and\\nreference. These are diagrams about different approaches to research.', 'We have a quick look at various finite model properties for residuated\\nsemigroups. In particular, we solve Problem 19.17 from Relation Algebras by\\nGames by Hirsch and Hodkinson.', 'I present a technical embedding of alternating-time temporal logic into stit\\ntheory.', 'Data science workflows are human-centered processes involving on-demand\\nprogramming and analysis. While programmable and interactive interfaces such as\\nwidgets embedded within computational notebooks are suitable for these\\nworkflows, they lack robust state management capabilities and do not support\\nuser-defined customization of the interactive components. The absence of such\\ncapabilities hinders workflow reusability and transparency while limiting the\\nscope of exploration of the end-users. In response, we developed MAGNETON, a\\nframework for authoring interactive widgets within computational notebooks that\\nenables transparent, reusable, and customizable data science workflows. The\\nframework enhances existing widgets to support fine-grained interaction history\\nmanagement, reusable states, and user-defined customizations. We conducted\\nthree case studies in a real-world knowledge graph construction and serving\\nplatform to evaluate the effectiveness of these widgets. Based on the\\nobservations, we discuss future implications of employing MAGNETON widgets for\\ngeneral-purpose data science workflows.', 'Logic really is just algebra, given one uses the right kind of algebra, and\\nthe right kind of logic. The right kind of algebra is abstraction algebra, and\\nthe right kind of logic is abstraction logic.', 'We give a presentation of Simple Type Theory as a clausal rewrite system in\\nPolarized deduction modulo.', 'The rewriting system sigma is the set of rules propagating explicit\\nsubstitutions in the lambda-calculus with explicit substitutions. In this note,\\nwe prove the undecidability of unification modulo sigma.', 'In this note, we uncover three connections between the metric distortion\\nproblem and voting methods and axioms from the social choice literature.', 'We present an operational semantics for the language MeTTa.', 'We robustify PCTL and PCTL*, the most important specification languages for\\nprobabilistic systems, and show that robustness does not increase the\\ncomplexity of the model-checking problems.', 'The TARK conference (Theoretical Aspects of Rationality and Knowledge) is a\\nconference that aims to bring together researchers from a wide variety of\\nfields, including computer science, artificial intelligence, game theory,\\ndecision theory, philosophy, logic, linguistics, and cognitive science. Its\\ngoal is to further our understanding of interdisciplinary issues involving\\nreasoning about rationality and knowledge.\\n  Previous conferences have been held biennially around the world since 1986,\\non the initiative of Joe Halpern (Cornell University). Topics of interest\\ninclude, but are not limited to, semantic models for knowledge, belief,\\nawareness and uncertainty, bounded rationality and resource-bounded reasoning,\\ncommonsense epistemic reasoning, epistemic logic, epistemic game theory,\\nknowledge and action, applications of reasoning about knowledge and other\\nmental states, belief revision, computational social choice, algorithmic game\\ntheory, and foundations of multi-agent systems. Information about TARK,\\nincluding conference proceedings, is available at http://www.tark.org/\\n  These proceedings contain the papers that have been accepted for presentation\\nat the Nineteenth Conference on Theoretical Aspects of Rationality and\\nKnowledge (TARK 2023), held between June 28 and June 30, 2023, at the\\nUniversity of Oxford, United Kingdom. The conference website can be found at\\nhttps://sites.google.com/view/tark-2023', 'We present a variation of Hurkens paradox, which can itself be seen as a\\nvariation of Reynolds result that there is no set theoretic model of\\npolymorphism.', 'We give an algorithm for the class of second order unification problems in\\nwhich second order variables have at most one occurrence.', 'We present constructive arithmetic in Deduction modulo with rewrite rules\\nonly.', 'We determine the complexity of second-order HyperLTL satisfiability and\\nmodel-checking: Both are as hard as truth in third-order arithmetic.', 'In this survey paper we present classical and recent results relating the\\nauction design and the optimal transportation theory.', 'Child Impact Statements (CIS) are instrumental in helping to foreground the\\nconcerns and needs of minor community members who are too young to vote and\\noften unable to advocate for themselves politically. While many politicians and\\npolicymakers assert they make decisions in the best interests of children, they\\noften lack the necessary information to meaningfully accomplish this. CISs are\\nakin to Environmental Impact Statements in that both give voice to constituents\\nwho are often under-represented in policymaking. This paper highlights an\\ninterdisciplinary collaboration between Social Science and Computer Science to\\ncreate a CIS tool for policymakers and community members in Shelby County, TN.\\nFurthermore, this type of collaboration is fruitful beyond the scope of the CIS\\ntool. Social scientists and computer scientists can leverage their\\ncomplementary skill sets in data management and data interpretation for the\\nbenefit of their communities, advance scientific knowledge, and bridge\\ndisciplinary divides within the academy.', 'These lecture notes are an informal introduction to the theory of\\ncomputational complexity and its links to quantum computing and statistical\\nmechanics.', 'A syntactical proof is given that all functions definable in a certain affine\\nlinear typed lambda-calculus with iteration in all types are polynomial time\\ncomputable. The proof provides explicit polynomial bounds that can easily be\\ncalculated.', 'I consider issues in distributed computation that should be of relevance to\\ngame theory. In particular, I focus on (a) representing knowledge and\\nuncertainty, (b) dealing with failures, and (c) specification of mechanisms.', 'This paper presents a method for computing a least fixpoint of a system of\\nequations over booleans. The resulting computation can be significantly shorter\\nthan the result of iteratively evaluating the entire system until a fixpoint is\\nreached.', 'A computational method for numeric resolution of a PDEs system, based on a\\nFinite Differences schema integrated by interpolations of partial results, and\\nan estimate of the error of its solution respect to the normal FD solution.', 'So far, the scope of computer algebra has been needlessly restricted to exact\\nalgebraic methods. Its possible extension to approximate analytical methods is\\ndiscussed. The entangled roles of functional analysis and symbolic programming,\\nespecially the functional and transformational paradigms, are put forward. In\\nthe future, algebraic algorithms could constitute the core of extended symbolic\\nmanipulation systems including primitives for symbolic approximations.', 'Evolutionary computing (EC) is an exciting development in Computer Science.\\nIt amounts to building, applying and studying algorithms based on the Darwinian\\nprinciples of natural selection. In this paper we briefly introduce the main\\nconcepts behind evolutionary computing. We present the main components all\\nevolutionary algorithms (EA), sketch the differences between different types of\\nEAs and survey application areas ranging from optimization, modeling and\\nsimulation to entertainment.', 'We analyze numerical stability of a recursive computation scheme of present\\nvalue (PV) amd show that the absolute error increases exponentially for\\npositive discount rates. We show that reversing the direction of calculations\\nin the recurrence equation yields a robust PV computation routine.', 'Recent development in quantum computation and quantum information theory\\nallows to extend the scope of game theory for the quantum world. The authors\\nhave recently proposed a quantum description of financial market in terms of\\nquantum game theory. The paper contain an analysis of such markets that shows\\nthat there would be advantage in using quantum computers and quantum\\nstrategies.', 'Deoxyribonucleic acid is increasingly being understood to be an informational\\nmolecule, capable of information processing.It has found application in the\\ndetermination of non-deterministic algorithms and in the design of molecular\\ncomputing devices. This is a theoretical analysis of the mathematical\\nproperties and relations of the molecules which constituting DNA, which\\nexplains in part why DNA is a successful computing molecule.', 'This survey paper examines the effective model theory obtained with the BSS\\nmodel of real number computation. It treats the following topics: computable\\nordinals, satisfaction of computable infinitary formulas, forcing as a\\nconstruction technique, effective categoricity, effective topology, and\\nrelations with other models for the effective theory of uncountable structures.', 'This article surveys quantum computational complexity, with a focus on three\\nfundamental notions: polynomial-time quantum computations, the efficient\\nverification of quantum proofs, and quantum interactive proof systems.\\nProperties of quantum complexity classes based on these notions, such as BQP,\\nQMA, and QIP, are presented. Other topics in quantum complexity, including\\nquantum advice, space-bounded quantum computation, and bounded-depth quantum\\ncircuits, are also discussed.', 'Paper has been withdrawn due to non-compliance with IJCSI terms and\\nconditions.', 'We show that the computational complexity of Riemann mappings can be bounded\\nby the complexity needed to compute conformal mappings locally at boundary\\npoints. As a consequence we get first formally proven upper bounds for\\nSchwarz-Christoffel mappings and, more generally, Riemann mappings of domains\\nwith piecewise analytic boundaries.', 'In a recent paper, two multi-representations for the measurable sets in a\\ncomputable measure space have been introduced, which prove to be topologically\\ncomplete w.r.t. certain topological properties. In this contribution, we show\\nthem recursively complete w.r.t. computability of measure and set-theoretical\\noperations.', 'We propose a novel model of unconventional computing where a structural part\\nof computation is presented by dynamics of plasmodium of Physarum polycephalum,\\na large single cell. We sketch a new logical approach combining conventional\\nlogic with process calculus to demonstrate how to employ formal methods in\\ndesign of unconventional computing media presented by Physarum polycephalum.', 'An algorithm for Electric Power System (EPS) quantum/relativistic security\\nand efficiency computation for a day-ahead via perturbative renormalization of\\nthe EPS, finding the computation flowcharts, verification and validation is\\nbuilt in this paper.', 'Adopting former term rewriting characterisations of polytime and\\nexponential-time computable functions, we introduce a new reduction order, the\\nPath Order for ETIME (POE* for short), that is sound and complete for ETIME\\ncomputable functions. The proposed reduction order for ETIME makes contrasts to\\nthose related complexity classes clear.', 'This note introduces a generalization to the setting of infinite-time\\ncomputation of the busy beaver problem from classical computability theory, and\\nproves some results concerning the growth rate of an associated function. In\\nour view, these results indicate that the generalization is both natural and\\npromising.', 'Computing transitive closures of integer relations is the key to finding\\nprecise invariants of integer programs. In this paper, we study difference\\nbounds and octagonal relations and prove that their transitive closure is a\\nPTIME-computable formula in the existential fragment of Presburger arithmetic.\\nThis result marks a significant complexity improvement, as the known algorithms\\nhave EXPTIME worst case complexity.', 'We provide a simple proof of a computable analogue to the Jayne Rogers\\nTheorem from descriptive set theory. The difficulty of the proof is delegated\\nto a simulation result pertaining to non-deterministic type-2 machines. Thus,\\nwe demonstrate that developments in computational models can have applications\\nin fields thought to be far removed from it.', 'Computer hardware and software are resources without which the modern\\nbusiness of any organization, from manufacturing to services, is impossible.\\nNot enough attention is being payed to maintenance of computer systems as an\\naspect of business. This paper gives some recommendations for the selection of\\nthe computer systems maintenance approach, based on many years of experience\\nmaintaining these systems at the University of Zenica.', 'secure multi-party computation is widely studied area in computer science. It\\nis touching all most every aspect of human life. This paper demonstrates\\ntheoretical and experimental results of one of the secure multi-party\\ncomputation protocols proposed by Shukla et al. implemented using visual C++.\\nData outflow probability is computed by changing parameters. At the end, time\\nand space complexity is calculated using theoretical and experimental results.', 'Konrad Zuse built the Z1, a mechanical programmable computing machine,\\nbetween 1935/36 and 1937/38. The Z1 was a binary floating-point computing\\ndevice. The individual logical gates were constructed using metallic plates and\\ninterconnection rods. This paper describes the design principles Zuse followed\\nin order to complete a complex calculating machine, as the Z1 was. Zuse called\\nhis basic switching elements \"mechanical relays\" in analogy to the electrical\\nrelays used in telephony.', 'This is the Proceedings of the eleventh Workshop on Answer Set Programming\\nand Other Computing Paradigms (ASPOCP) 2018, which was held in Oxford, UK, July\\n18th, 2018.', 'A research programme is set out for developing the use of high-level methods\\nfor quantum computation and information, based on the categorical formulation\\nof quantum mechanics introduced by the author and Bob Coecke.', 'In this paper we survey the computational time complexity of assorted simple\\nstochastic game problems, and we give an overview of the best known algorithms\\nassociated with each problem.', 'Human labeled datasets, along with their corresponding evaluation algorithms,\\nplay an important role in boundary detection. We here present a psychophysical\\nexperiment that addresses the reliability of such benchmarks. To find better\\nremedies to evaluate the performance of any boundary detection algorithm, we\\npropose a computational framework to remove inappropriate human labels and\\nestimate the intrinsic properties of boundaries.', 'This volume contains the proceedings of TERMGRAPH 2016, the Ninth\\nInternational Workshop on Computing with Terms and Graphs which was held on\\nApril 8, 2016 in Eindhoven, The Netherlands, as a satellite event of the\\nEuropean Joint Conferences on Theory and Practice of Software (ETAPS 2016).', 'We compute that the index set of PAC-learnable concept classes is\\n$m$-complete $\\\\Sigma^0_3$ within the set of indices for all concept classes of\\na reasonable form. All concept classes considered are computable enumerations\\nof computable $\\\\Pi^0_1$ classes, in a sense made precise here. This family of\\nconcept classes is sufficient to cover all standard examples, and also has the\\nproperty that PAC learnability is equivalent to finite VC dimension.', \"We show that probabilistic computable functions, i.e., those functions\\noutputting distributions and computed by probabilistic Turing machines, can be\\ncharacterized by a natural generalization of Church and Kleene's partial\\nrecursive functions. The obtained algebra, following Leivant, can be restricted\\nso as to capture the notion of polytime sampleable distributions, a key concept\\nin average-case complexity and cryptography.\", 'This volume contains a final and revised selection of papers presented at\\nTwelfth Workshop on Developments in Computational Models (DCM 2018) and the\\nNinth Workshop on Intersection Types and Related Systems (ITRS 2018), held on\\nJuly 8, 2018 in Oxford, in affiliation with FLOC 2018.', 'We analyze the computational complexity of the popular computer games\\nThrees!, 1024!, 2048 and many of their variants. For most known versions\\nexpanded to an m x n board, we show that it is NP-hard to decide whether a\\ngiven starting position can be played to reach a specific (constant) tile\\nvalue.', 'While the computational complexity of many game-theoretic solution concepts,\\nnotably Nash equilibrium, has now been settled, the question of determining the\\nexact complexity of computing an evolutionarily stable strategy has resisted\\nsolution since attention was drawn to it in 2004. In this paper, I settle this\\nquestion by proving that deciding the existence of an evolutionarily stable\\nstrategy is $\\\\Sigma_2^P$-complete.', \"We compare performance of the genetic algorithm and the counterfactual regret\\nminimization algorithm in computing the near-equilibrium strategies in the\\nsimplified poker games. We focus on the von Neumann poker and the simplified\\nversion of the Texas Hold'Em poker, and test outputs of the considered\\nalgorithms against analytical expressions defining the Nash equilibrium\\nstrategies. We comment on the performance of the studied algorithms against\\nopponents deviating from equilibrium.\", 'In the past several years I have written two SMT solvers called STP and HAMPI\\nthat have found widespread use in computer security research by leading groups\\nin academia, industry and the government. In this brief note I summarize the\\nfeatures of STP/HAMPI that make them particularly suited for computer security\\nresearch, and a listing of some of the more important projects that use them.', 'Cloud computing is one of the latest emerging innovations of the modern\\ninternet and technological landscape. With everyone from the White house to\\nmajor online technological leaders like Amazon and Google using or offering\\ncloud computing services it is truly presents itself as an exciting and\\ninnovative method to store and use data on the internet.', 'We revisit the definition of effective local compactness, and propose an\\napproach that works for arbitrary countably-based spaces extending the previous\\nwork on computable metric spaces. We use this to show that effective local\\ncompactness suffices to ensure that the hyperspace of closed-and-overt sets\\n(aka located sets, aka closed sets with full information) is computably compact\\nand computably metrizable.', 'In this paper, a computably definable predicate is defined and characterized.\\nThen, it is proved that every separable infinite-dimensional Hilbert structure\\nin an effectively presented language is computable. Moreover, every definable\\npredicate in these structures is computable.', 'I survey recent progress on a classic and challenging problem in social\\nchoice: the fair division of indivisible items. I discuss how a computational\\nperspective has provided interesting insights into and understanding of how to\\ndivide items fairly and efficiently. This has involved bringing to bear tools\\nsuch as those used in knowledge representation, computational complexity,\\napproximation methods, game theory, online analysis and communication\\ncomplexity', 'We study the problem of computing the preimage of a set under a neural\\nnetwork with piecewise-affine activation functions. We recall an old result\\nthat the preimage of a polyhedral set is again a union of polyhedral sets and\\ncan be effectively computed. We show several applications of computing the\\npreimage for analysis and interpretability of neural networks.', 'To learn how cognition is implemented in the brain, we must build\\ncomputational models that can perform cognitive tasks, and test such models\\nwith brain and behavioral experiments. Cognitive science has developed\\ncomputational models of human cognition, decomposing task performance into\\ncomputational components. However, its algorithms still fall short of human\\nintelligence and are not grounded in neurobiology. Computational neuroscience\\nhas investigated how interacting neurons can implement component functions of\\nbrain computation. However, it has yet to explain how those components interact\\nto explain human cognition and behavior. Modern technologies enable us to\\nmeasure and manipulate brain activity in unprecedentedly rich ways in animals\\nand humans. However, experiments will yield theoretical insight only when\\nemployed to test brain-computational models. It is time to assemble the pieces\\nof the puzzle of brain computation. Here we review recent work in the\\nintersection of cognitive science, computational neuroscience, and artificial\\nintelligence. Computational models that mimic brain information processing\\nduring perceptual, cognitive, and control tasks are beginning to be developed\\nand tested with brain and behavioral data.', 'Literate computing has emerged as an important tool for computational studies\\nand open science, with growing folklore of best practices. In this work, we\\nreport two case studies - one in computational magnetism and another in\\ncomputational mathematics - where domain-specific software was exposed to the\\nJupyter environment. This enables high-level control of simulations and\\ncomputation, interactive exploration of computational results, batch processing\\non HPC resources, and reproducible workflow documentation in Jupyter notebooks.\\nIn the first study, Ubermag drives existing computational micromagnetics\\nsoftware through a domain-specific language embedded in Python. In the second\\nstudy, a dedicated Jupyter kernel interfaces with the GAP system for\\ncomputational discrete algebra and its dedicated programming language. In light\\nof these case studies, we discuss the benefits of this approach, including\\nprogress toward more reproducible and reusable research results and outputs,\\nnotably through the use of infrastructure such as JupyterHub and Binder.', 'Computational complexity is a core theory of computer science, which dictates\\nthe degree of difficulty of computation. There are many problems with high\\ncomplexity that we have to deal, which is especially true for AI. This raises a\\nbig question: Is there a better way to deal with these highly complex problems\\nother than bounded by computational complexity? We believe that ideas and\\nmethods from intelligence science can be applied to these problems and help us\\nto exceed computational complexity. In this paper, we try to clarify concepts,\\nand we propose definitions such as unparticularized computing, particularized\\ncomputing, computing agents, and dynamic search. We also propose and discuss a\\nframework, i.e., trial-and-error + dynamic search. Number Partition Problem is\\na well-known NP-complete problem, and we use this problem as an example to\\nillustrate the ideas discussed.', \"As is well known, Buss' theory of bounded arithmetic $S^{1}_{2}$ proves\\n$\\\\Sigma_{0}^{b}(\\\\Sigma_{1}^{b})-LIND$; however, we show that Allen's\\n$D_{2}^{1}$ does not prove $\\\\Sigma_{0}^{b}(\\\\Sigma_{1}^{b})-LLIND$ unless $P =\\nNC$. We also give some interesting alternative axiomatisations of $S^{1}_{2}$.\", 'We present a type inference algorithm for lambda-terms in Elementary Affine\\nLogic using linear constraints. We prove that the algorithm is correct and\\ncomplete.', 'The replacement (or collection or choice) axiom scheme asserts bounded\\nquantifier exchange. We prove the independence of this scheme from various weak\\ntheories of arithmetic, sometimes under a complexity assumption.', 'Results about the redundancy of circumscriptive and default theories are\\npresented. In particular, the complexity of establishing whether a given theory\\nis redundant is establihsed.', 'This paper has been withdrawn by the author due to errors.', 'We present some mathematical results concerning the strip projection method\\nand a computer program for generating quasiperiodic packings of decagonal two\\nshell-clusters.', 'We argue that entanglement is the essential non-classical ingredient which\\nprovides the computational speed-up in quantum algorithms as compared to\\nalgorithms based on the processes of classical physics.', 'In this paper we treat a cavity QED quantum computation. Namely, we consider\\na model of quantum computation based on n atoms of laser-cooled and trapped\\nlinearly in a cavity and realize it as the n atoms Tavis-Cummings Hamiltonian\\ninteracting with n external (laser) fields.\\n  We solve the Schr{\\\\\" o}dinger equation of the model in the weak coupling\\nregime to construct the controlled NOT gate in the case of n=2, and to\\nconstruct the controlled-controlled NOT gate in the case of n=3 by making use\\nof several resonance conditions and rotating wave approximation associated to\\nthem. We also present an idea to construct general quantum circuits.\\n  The approach is more sophisticated than that of the paper [K. Fujii,\\nHigashida, Kato and Wada, Cavity QED and Quantum Computation in the Weak\\nCoupling Regime, J. Opt. B : Quantum Semiclass. Opt. {\\\\bf 6} (2004), 502].\\n  Our method is not heuristic but completely mathematical, and the significant\\nfeature is based on a consistent use of Rabi oscillations.', 'As computability implies value definiteness, certain sequences of quantum\\noutcomes cannot be computable.', 'We show that it is NP-Complete to decide whether a bimatrix game is\\ndegenerate and it is Co-NP-Complete to decide whether a bimatrix game is\\nnondegenerate.', 'In [EGZ09] it has been shown that infinitary strong normalization (SNi) is\\nPi-1-1-complete. Suprisingly, it turns out that infinitary weak normalization\\n(WNi) is a harder problem, being Pi-1-2-complete, and thereby strictly higher\\nin the analytical hierarchy.', 'A dissertation submitted to the University of Bristol in accordance with the\\nrequirements of the degree of Doctor of Philosophy (PhD) in the Faculty of\\nEngineering, Department of Computer Science, July 2009.', 'We study the combinatorial two-player game Tron. We answer the extremal\\nquestion on general graphs and also consider smaller graph classes. Bodlaender\\nand Kloks conjectured in [2] PSPACE- completeness. We proof this conjecture.', 'We initiate a program of parameterized proof complexity that aims to provide\\nevidence that FPT is different from W[1]. A similar program already exists for\\nthe classes W[2] and W[SAT]. We contrast these programs and prove upper and\\nlower bounds for W[1]-parameterized Resolution.', 'This paper presents convergence acceleration, a method for computing\\nefficiently the limit of numerical sequences as a typical application of\\nstreams and higher-order functions.', 'Zipper logic is a graph rewrite system, consisting in only local rewrites on\\na class of zipper graphs. Connections with the chemlambda artificial chemistry\\nand with knot diagrammatics based computation are explored in the article.', 'We develop formal foundations for notions and mechanisms needed to support\\nservice-oriented computing. Our work builds on recent theoretical advancements\\nin the algebraic structures that capture the way services are orchestrated and\\nin the processes that formalize the discovery and binding of services to given\\nclient applications by means of logical representations of required and\\nprovided services. We show how the denotational and the operational semantics\\nspecific to conventional logic programming can be generalized using the theory\\nof institutions to address both static and dynamic aspects of service-oriented\\ncomputing. Our results rely upon a strong analogy between the discovery of a\\nservice that can be bound to an application and the search for a clause that\\ncan be used for computing an answer to a query; they explore the manner in\\nwhich requests for external services can be described as service queries, and\\nexplain how the computation of their answers can be performed through\\nservice-oriented derivatives of unification and resolution, which characterize\\nthe binding of services and the reconfiguration of applications.', \"Conway and Kochen's Free Will Theory is examined as an important foundational\\nelement in a new area of activity in computer science - developing protocols\\nfor quantum computing\", 'We define and study a general framework for approval-based budgeting methods\\nand compare certain methods within this framework by their axiomatic and\\ncomputational properties. Furthermore, we visualize their behavior on certain\\nEuclidean distributions and analyze them experimentally.', 'This study aims to determine privacy awareness among people in ubiquitous\\nenvironment through a questionnaire based survey.', 'Where a 2D problem of optimal profile in variable speed flow is resolved in a\\nclass of convex Bezier curves, using symbolic and numerical computations.', 'In the dawn of computer science and the eve of neuroscience we participate in\\nrebirth of neuroscience due to new technology that allows us to deeply and\\nprecisely explore whole new world that dwells in our brains.', 'In the paper we consider a graph model of message passing processes and\\npresent a method verification of message passing processes. The method is\\nillustrated by an example of a verification of sliding window protocol.', 'In this brief note, we prove that the existence of Nash equilibria on integer\\nprogramming games is $\\\\Sigma^p_2$-complete.', 'We solve the classical \"Game of Pure Strategy\" using linear programming. We\\nnotice an intricate even-odd behavior in the results of our computations, that\\nseems to encourage odd or maximal bids.', \"We present a history of Hoare's logic.\", 'In this note we introduce a notion of a generically (strongly generically)\\nNP-complete problem and show that the randomized bounded version of the halting\\nproblem is strongly generically NP-complete.', 'Given a $\\\\Pi^{\\\\mu}_2$ formula of the modal $\\\\mu$ calculus, it is decidable\\nwhether it is equivalent to a $\\\\Sigma^{\\\\mu}_2$ formula.', 'We prove an $N^{2-o(1)}$ lower bound on the randomized communication\\ncomplexity of finding an $\\\\epsilon$-approximate Nash equilibrium (for constant\\n$\\\\epsilon>0$) in a two-player $N\\\\times N$ game.', 'Fairness is becoming an increasingly important concern when designing\\nmarkets, allocation procedures, and computer systems. I survey some recent\\ndevelopments in the field of multi-agent fair allocation.', 'We develop a notion of realizability for Classical Linear Logic based on a\\nconcurrent process calculus.', 'A practical guide for university academics who need to create learning\\nmaterials that support flexible delivery methods. Examples from the Computer\\nScience domain are used to illustrate innovative approaches to engaging\\nstudents with online and blended teaching resources.', 'We give a novel descriptive-complexity theoretic characterization of L and NL\\ncomputable queries over finite structures using traversal invariance. We\\nsummarize this as (N)L = FO + (breadth-first) traversal-invariance.', 'We add local variables to quantum relational Hoare logic (Unruh, POPL 2019).\\nWe derive reasoning rules for supporting local variables (including an improved\\n\"adversary rule\"). We extended the qrhl-tool for computer-aided verification of\\nqRHL to support local variables and our new reasoning rules.', 'The width of a well partial ordering (wpo) is the ordinal rank of the set of\\nits antichains ordered by inclusion. We compute the width of wpos obtained as\\ncartesian products of finitely many well-orderings.', \"We discuss the theory of Lie algebras in Lean's Mathlib library. Using\\nnilpotency as the theme, we outline a computer formalisation of Engel's theorem\\nand an application to root space theory. We emphasise that all arguments work\\nwith coefficients in any commutative ring.\", 'On the real numbers, the notions of a semi-decidable relation and that of an\\neffectively enumerable relation differ. The second only seems to be adequate to\\nexpress, in an algorithmic way, non deterministic physical theories, where\\nmagnitudes are represented by real numbers.', 'We discuss the deal of imperfectness of atomic actions in reality with the\\nbackground of process algebras. And we show the applications of the imperfect\\nactions in verification of computational systems.', \"One of the primary motivations of the research in the field of computation is\\nto optimize the cost of computation. The major ingredient that a computer needs\\nis the energy to run a process, i.e., the thermodynamic cost. The analysis of\\nthe thermodynamic cost of computation is one of the prime focuses of research.\\nIt started back since the seminal work of Landauer where it was commented that\\nthe computer spends kB T ln2 amount of energy to erase a bit of information\\n(here T is the temperature of the system and kB represents the Boltzmann's\\nconstant). The advancement of statistical mechanics has provided us the\\nnecessary tool to understand and analyze the thermodynamic cost for the\\ncomplicated processes that exist in nature, even the computation of modern\\ncomputers. The advancement of physics has helped us to understand the\\nconnection of the statistical mechanics (the thermodynamics cost) with\\ncomputation. Another important factor that remains a matter of concern in the\\nfield of computer science is the error correction of the error that occurs\\nwhile transmitting the information through a communication channel. Here in\\nthis article, we have reviewed the progress of the thermodynamics of\\ncomputation starting from Landauer's principle to the latest model, which\\nsimulates the modern complex computation mechanism. After exploring the salient\\nparts of computation in computer science theory and information theory, we have\\nreviewed the thermodynamic cost of computation and error correction. We have\\nalso discussed about the alternative computation models that have been proposed\\nwith thermodynamically cost-efficient.\", \"Knowledge representation and reasoning (KRR) systems represent knowledge as\\ncollections of facts and rules. Like databases, KRR systems contain information\\nabout domains of human activities like industrial enterprises, science, and\\nbusiness. KRRs can represent complex concepts and relations, and they can query\\nand manipulate information in sophisticated ways. Unfortunately, the KRR\\ntechnology has been hindered by the fact that specifying the requisite\\nknowledge requires skills that most domain experts do not have, and\\nprofessional knowledge engineers are hard to find. One solution could be to\\nextract knowledge from English text, and a number of works have attempted to do\\nso (OpenSesame, Google's Sling, etc.). Unfortunately, at present, extraction of\\nlogical facts from unrestricted natural language is still too inaccurate to be\\nused for reasoning, while restricting the grammar of the language (so-called\\ncontrolled natural language, or CNL) is hard for the users to learn and use.\\nNevertheless, some recent CNL-based approaches, such as the Knowledge Authoring\\nLogic Machine (KALM), have shown to have very high accuracy compared to others,\\nand a natural question is to what extent the CNL restrictions can be lifted. In\\nthis paper, we address this issue by transplanting the KALM framework to a\\nneural natural language parser, mStanza. Here we limit our attention to\\nauthoring facts and queries and therefore our focus is what we call factual\\nEnglish statements. Authoring other types of knowledge, such as rules, will be\\nconsidered in our followup work. As it turns out, neural network based parsers\\nhave problems of their own and the mistakes they make range from part-of-speech\\ntagging to lemmatization to dependency errors. We present a number of\\ntechniques for combating these problems and test the new system, KALMFL (i.e.,\\nKALM for factual language), on a number of benchmarks, which show KALMFL\\nachieves correctness in excess of 95%.\", \"We analyze the changes in the training and educational efforts of the SciNet\\nHPC Consortium, a Canadian academic High Performance Computing center, in the\\nareas of Scientific Computing and High-Performance Computing, over the last six\\nyears. Initially, SciNet offered isolated training events on how to use HPC\\nsystems and write parallel code, but the training program now consists of a\\nbroad range of workshops and courses that users can take toward certificates in\\nscientific computing, data science, or high-performance computing. Using data\\non enrollment, attendence, and certificate numbers from SciNet's education\\nwebsite, used by almost 1800 users so far, we extract trends on the growth,\\ndemand, and breadth of SciNet's training program. Among the results are a\\nsteady overall growth, a sharp and steady increase in the demand for data\\nscience training, and a wider participation of 'non-traditional' computing\\ndisciplines, which has motivated an increasingly broad spectrum of training\\nofferings. Of interest is also that many of the training initiatives have\\nevolved into courses that can be taken as part of the graduate curriculum at\\nthe University of Toronto.\", \"A theory of one-tape (one-head) linear-time Turing machines is essentially\\ndifferent from its polynomial-time counterpart since these machines are closely\\nrelated to finite state automata. This paper discusses structural-complexity\\nissues of one-tape Turing machines of various types (deterministic,\\nnondeterministic, reversible, alternating, probabilistic, counting, and quantum\\nTuring machines) that halt in linear time, where the running time of a machine\\nis defined as the length of any longest computation path. We explore structural\\nproperties of one-tape linear-time Turing machines and clarify how the\\nmachines' resources affect their computational patterns and power.\", 'This paper initiates a systematic study of quantum functions, which are\\n(partial) functions defined in terms of quantum mechanical computations. Of all\\nquantum functions, we focus on resource-bounded quantum functions whose inputs\\nare classical bit strings. We prove complexity-theoretical properties and\\nunique characteristics of these quantum functions by recent techniques\\ndeveloped for the analysis of quantum computations. We also discuss relativized\\nquantum functions that make adaptive and nonadaptive oracle queries.', 'Context is a rich concept and is an elusive concept to define. The concept of\\ncontext has been studied by philosophers, linguists, psychologists, and\\nrecently by computer scientists. Within each research community the term\\ncontext was interpreted in a certain way that is well-suited for their goals,\\nhowever no attempt was made to define context. In many areas of research in\\ncomputer science, notably on web-based services, human-computer interaction\\n(HCI), ubiquitous computing applications, and context-aware systems there is a\\nneed to provide a formal operational definition of context. In this brief\\nsurvey an account of the early work on context, as well as the recent work on\\nmany working definitions of context, context modeling, and a formalization of\\ncontext are given. An attempt is made to unify the different context models\\nwithin the formalization. A brief commentary on the usefulness of the\\nformalization in the development of context-aware and dependable systems is\\nincluded.', 'A method for the design of Fast Haar wavelet for signal processing and image\\nprocessing has been proposed. In the proposed work, the analysis bank and\\nsynthesis bank of Haar wavelet is modified by using polyphase structure.\\nFinally, the Fast Haar wavelet was designed and it satisfies alias free and\\nperfect reconstruction condition. Computational time and computational\\ncomplexity is reduced in Fast Haar wavelet transform.', 'The game of chess as always been viewed as an iconic representation of\\nintellectual prowess. Since the very beginning of computer science, the\\nchallenge of being able to program a computer capable of playing chess and\\nbeating humans has been alive and used both as a mark to measure\\nhardware/software progresses and as an ongoing programming challenge leading to\\nnumerous discoveries. In the early days of computer science it was a topic for\\nspecialists. But as computers were democratized, and the strength of chess\\nengines began to increase, chess players started to appropriate to themselves\\nthese new tools. We show how these interactions between the world of chess and\\ninformation technologies have been herald of broader social impacts of\\ninformation technologies. The game of chess, and more broadly the world of\\nchess (chess players, literature, computer softwares and websites dedicated to\\nchess, etc.), turns out to be a surprisingly and particularly sharp indicator\\nof the changes induced in our everyday life by the information technologies.\\nMoreover, in the same way that chess is a modelization of war that captures the\\nraw features of strategic thinking, chess world can be seen as small society\\nmaking the study of the information technologies impact easier to analyze and\\nto grasp.', \"The Engage to Excel (PCAST) report, the National Research Council's Framework\\nfor K-12 Science Education, and the Next Generation Science Standards all call\\nfor transforming the physics classroom into an environment that teaches\\nstudents real scientific practices. This work describes the early stages of one\\nsuch attempt to transform a high school physics classroom. Specifically, a\\nseries of model-building and computational modeling exercises were piloted in a\\nninth grade Physics First classroom. Student use of computation was assessed\\nusing a proctored programming assignment, where the students produced and\\ndiscussed a computational model of a baseball in motion via a high-level\\nprogramming environment (VPython). Student views on computation and its link to\\nmechanics was assessed with a written essay and a series of think-aloud\\ninterviews. This pilot study shows computation's ability for connecting\\nscientific practice to the high school science classroom.\", 'We begin to study classical dimension theory from the computable analysis\\n(TTE) point of view. For computable metric spaces, several effectivisations of\\nzero-dimensionality are shown to be equivalent. The part of this\\ncharacterisation that concerns covering dimension extends to higher dimensions\\nand to closed shrinkings of finite open covers. To deal with zero-dimensional\\nsubspaces uniformly, four operations (relative to the space and a class of\\nsubspaces) are defined; these correspond to definitions of inductive and\\ncovering dimensions and a countable basis condition. Finally, an effective\\nretract characterisation of zero-dimensionality is proven under an effective\\ncompactness condition. In one direction this uses a version of the construction\\nof bilocated sets.', 'As the amount of scientific data continues to grow at ever faster rates, the\\nresearch community is increasingly in need of flexible computational\\ninfrastructure that can support the entirety of the data science lifecycle,\\nincluding long-term data storage, data exploration and discovery services, and\\ncompute capabilities to support data analysis and re-analysis, as new data are\\nadded and as scientific pipelines are refined. We describe our experience\\ndeveloping data commons-- interoperable infrastructure that co-locates data,\\nstorage, and compute with common analysis tools--and present several cases\\nstudies. Across these case studies, several common requirements emerge,\\nincluding the need for persistent digital identifier and metadata services,\\nAPIs, data portability, pay for compute capabilities, and data peering\\nagreements between data commons. Though many challenges, including\\nsustainability and developing appropriate standards remain, interoperable data\\ncommons bring us one step closer to effective Data Science as Service for the\\nscientific research community.', 'This paper presents an algebraic theory of instruction sequences with\\ninstructions for Turing tapes as basic instructions, the behaviours produced by\\nthe instruction sequences concerned under execution, and the interaction\\nbetween such behaviours and Turing tapes provided by an execution environment.\\nThis theory provides a setting for the development of theory in areas such as\\ncomputability and computational complexity that distinguishes itself by\\noffering the possibility of equational reasoning and being more general than\\nthe setting provided by a known version of the Turing-machine model of\\ncomputation. The theory is essentially an instantiation of a parameterized\\nalgebraic theory which is the basis of a line of research in which issues\\nrelating to a wide variety of subjects from computer science have been\\nrigorously investigated thinking in terms of instruction sequences.', 'Climate change, vaccination, abortion, Trump: Many topics are surrounded by\\nfierce controversies. The nature of such heated debates and their elements have\\nbeen studied extensively in the social science literature. More recently,\\nvarious computational approaches to controversy analysis have appeared, using\\nnew data sources such as Wikipedia, which help us now better understand these\\nphenomena. However, compared to what social sciences have discovered about such\\ndebates, the existing computational approaches mostly focus on just a few of\\nthe many important aspects around the concept of controversies. In order to\\nlink the two strands, we provide and evaluate here a controversy model that is\\nboth, rooted in the findings of the social science literature and at the same\\ntime strongly linked to computational methods. We show how this model can lead\\nto computational controversy analytics that have full coverage over all the\\ncrucial aspects that make up a controversy.', 'G\\\\\"odel logic with the projection operator Delta (G_Delta) is an important\\nmany-valued as well as intermediate logic. In contrast to classical logic, the\\nvalidity and the satisfiability problems of G_Delta are not directly dual to\\neach other. We nevertheless provide a uniform, computational treatment of both\\nproblems for prenex formulas by describing appropriate translations into sets\\nof order clauses that can be subjected to chaining resolution. For validity a\\nversion of Herbrand\\'s Theorem allows us to show the soundness of standard\\nSkolemization. For satisfiability the translation involves a novel, extended\\nSkolemization method.', 'The act of sharing scientific knowledge is rapidly evolving away from\\ntraditional articles and presentations to the delivery of executable objects\\nthat integrate the data and computational details (e.g., scripts and workflows)\\nupon which the findings rely. This envisioned coupling of data and process is\\nessential to advancing science but faces technical and institutional barriers.\\nThe Whole Tale project aims to address these barriers by connecting\\ncomputational, data-intensive research efforts with the larger research\\nprocess--transforming the knowledge discovery and dissemination process into\\none where data products are united with research articles to create \"living\\npublications\" or \"tales\". The Whole Tale focuses on the full spectrum of\\nscience, empowering users in the long tail of science, and power users with\\ndemands for access to big data and compute resources. We report here on the\\ndesign, architecture, and implementation of the Whole Tale environment.', 'We review algorithms for protein design in general. Although these algorithms\\nhave a rich combinatorial, geometric, and mathematical structure, they are\\nalmost never covered in computer science classes. Furthermore, many of these\\nalgorithms admit provable guarantees of accuracy, soundness, complexity,\\ncompleteness, optimality, and approximation bounds. The algorithms represent a\\ndelicate and beautiful balance between discrete and continuous computation and\\nmodeling, analogous to that which is seen in robotics, computational geometry,\\nand other fields in computational science. Finally, computer scientists may be\\nunaware of the almost direct impact of these algorithms for predicting and\\nintroducing molecular therapies that have gone in a short time from mathematics\\nto algorithms to software to predictions to preclinical testing to clinical\\ntrials. Indeed, the overarching goal of these algorithms is to enable the\\ndevelopment of new therapeutics that might be impossible or too expensive to\\ndiscover using experimental methods. Thus the potential impact of these\\nalgorithms on individual, community, and global health has the potential to be\\nquite significant.', 'MFPS conferences are dedicated to the areas of mathematics, logic, and\\ncomputer science that are related to models of computation in general, and to\\nsemantics of programming languages in particular. This is a forum where\\nresearchers in mathematics and computer science can meet and exchange ideas.\\nThe participation of researchers in neighbouring areas is strongly encouraged.\\n  Topics include, but are not limited to, the following: bio-computation;\\nconcurrent qualitative and quantitative distributed systems; process calculi;\\nprobabilistic systems; constructive mathematics; domain theory and categorical\\nmodels; formal languages; formal methods; game semantics; lambda calculus;\\nprogramming-language theory; quantum computation; security; topological models;\\nlogic; type systems; type theory. We also welcome contributions that address\\napplications of semantics to novel areas such as complex systems, markets, and\\nnetworks, for example.', 'Reversible Boolean Circuits are an interesting computational model under many\\naspects and in different fields, ranging from Reversible Computing to Quantum\\nComputing. Our contribution is to describe a specific class of Reversible\\nBoolean Circuits - which is as expressive as classical circuits - as a\\nbi-dimensional diagrammatic programming language. We uniformly represent the\\nReversible Boolean Circuits we focus on as a free 3-category Toff. This\\nformalism allows us to incorporate the representation of circuits and of\\nrewriting rules on them, and to prove termination of rewriting. Termination\\nfollows from defining a non-identities-preserving functor from our free\\n3-category Toff into a suitable 3-category Move that traces the \"moves\" applied\\nto wires inside circuits.', \"We present NetSci High, our NSF-funded educational outreach program that\\nconnects high school students who are underrepresented in STEM (Science\\nTechnology Engineering and Mathematics), and their teachers, with regional\\nuniversity research labs and provides them with the opportunity to work with\\nresearchers and graduate students on team-based, year-long network science\\nresearch projects, culminating in a formal presentation at a network science\\nconference. This short paper reports the content and materials that we have\\ndeveloped to date, including lesson plans and tools for introducing high school\\nstudents and teachers to network science; empirical evaluation data on the\\neffect of participation on students' motivation and interest in pursuing STEM\\ncareers; the application of professional development materials for teachers\\nthat are intended to encourage them to use network science concepts in their\\nlesson plans and curriculum; promoting district-level interest and engagement;\\nbest practices gained from our experiences; and the future goals for this\\nproject and its subsequent outgrowth.\", 'Crowdscience games may hold unique potentials as learning opportunities\\ncompared to games made for fun or education. They are part of an actual science\\nproblem solving process: By playing, players help scientists, and thereby\\ninteract with real continuous research processes. This mixes the two worlds of\\nplay and science in new ways. During usability testing we discovered that users\\nof the crowdscience game Quantum Dreams tended to answer questions in game\\nterms, even when directed explicitly to give science explanations.We then\\nexamined these competing frames of understanding through a mixed correlational\\nand grounded theory analysis. This essay presents the core ideas of\\ncrowdscience games as learning opportunities, and reports how a group of\\nplayers used \"game\", \"science\" and \"conceptual\" frames to interpret their\\nexperience. Our results suggest that oscillating between the frames instead of\\nsticking to just one led to the largest number of correct science\\ninterpretations, as players could participate legitimately and autonomously at\\nmultiple levels of understanding.', 'Science is a crowning glory of the human spirit and its applications remain\\nour best hope for social progress. But there are limitations to current science\\nand perhaps to any science. The general mind-body problem is known to be\\nintractable and currently mysterious. This is one of many deep problems that\\nare universally agreed to be beyond the current purview of Science, including\\nquantum phenomena, etc. But all of these famous unsolved problems are either\\nremote from everyday experience (entanglement, dark matter) or are hard to even\\ndefine sharply (phenomenology, consciousness, etc.).\\n  An updated summary of this work has been published as: Feldman, J. (2022).\\nComputation, perception, and mind. Behavioral and Brain Sciences, 45, E48.\\ndoi:10.1017/S0140525X21001886 A more readable, open access, version is:\\nhttps://escholarship.org/uc/item/6cs78450', \"Donoho's JCGS (in press) paper is a spirited call to action for\\nstatisticians, who he points out are losing ground in the field of data science\\nby refusing to accept that data science is its own domain. (Or, at least, a\\ndomain that is becoming distinctly defined.) He calls on writings by John\\nTukey, Bill Cleveland, and Leo Breiman, among others, to remind us that\\nstatisticians have been dealing with data science for years, and encourages\\nacceptance of the direction of the field while also ensuring that statistics is\\ntightly integrated.\\n  As faculty at baccalaureate institutions (where the growth of undergraduate\\nstatistics programs has been dramatic), we are keen to ensure statistics has a\\nplace in data science and data science education. In his paper, Donoho is\\nprimarily focused on graduate education. At our undergraduate institutions, we\\nare considering many of the same questions.\", \"This article provides an overview of the web video production context related\\nto science communication, based on a quantitative analysis of 190 YouTube\\nvideos. The authors explore the main characteristics and ongoing strategies of\\nproducers, focusing on three topics: professionalism, producer's gender and age\\nprofile, and community building. In the discussion, the authors compare the\\nquantitative results with recently published qualitative research on producers\\nof popular science web videos. This complementary approach gives further\\nevidence on the main characteristics of most popular science communicators on\\nYouTube, it shows a new type of professionalism that surpasses the hitherto\\nexisting distinction between User Generated Content (UGC) and Professional\\nGenerated Content (PGC), raises gender issues, and questions the participatory\\nculture of science communicators on YouTube.\", 'This study introduces a method based on link analysis to investigate the\\nstructure of the R&D support infrastructure associated with science parks in\\norder to determine whether this webometric approach gives plausible results.\\nThree science parks from Yorkshire and the Humber in the UK were analysed with\\nwebometric and social network analysis techniques. Interlinking networks were\\ngenerated through the combination of two different data sets extracted from\\nthree sources (Yahoo!, Bing, SocSciBot). These networks suggest that\\ninstitutional sectors, representing business, universities and public bodies,\\nare primarily tied together by a core formed by research institutions, support\\nstructure organisations and business developers. The comparison of the findings\\nwith traditional indicators suggests that the web-based networks reflect the\\noffline conditions and policy measures adopted in the region, giving some\\nevidence that the webometric approach is plausible to investigating science\\npark networks. This is the first study that applies a web-based approach to\\ninvestigate to what extent the science parks facilitate a closer interaction\\nbetween the heterogeneous organisations that converge in R&D networks. This\\nindicates that link analysis may help to get a first insight into the\\norganisation of the R&D support infrastructure provided by science parks.', 'This short article presents a summary of the NetSciEd (Network Science and\\nEducation) initiative that aims to address the need for curricula, resources,\\naccessible materials, and tools for introducing K-12 students and the general\\npublic to the concept of networks, a crucial framework in understanding\\ncomplexity. NetSciEd activities include (1) the NetSci High educational\\noutreach program (since 2010), which connects high school students and their\\nteachers with regional university research labs and provides them with the\\nopportunity to work on network science research projects; (2) the NetSciEd\\nsymposium series (since 2012), which brings network science researchers and\\neducators together to discuss how network science can help and be integrated\\ninto formal and informal education; and (3) the Network Literacy: Essential\\nConcepts and Core Ideas booklet (since 2014), which was created collaboratively\\nand subsequently translated into 18 languages by an extensive group of network\\nscience researchers and educators worldwide.', 'In this paper we describe the design, and implementation of the Open Science\\nData Cloud, or OSDC. The goal of the OSDC is to provide petabyte-scale data\\ncloud infrastructure and related services for scientists working with large\\nquantities of data. Currently, the OSDC consists of more than 2000 cores and 2\\nPB of storage distributed across four data centers connected by 10G networks.\\nWe discuss some of the lessons learned during the past three years of operation\\nand describe the software stacks used in the OSDC. We also describe some of the\\nresearch projects in biology, the earth sciences, and social sciences enabled\\nby the OSDC.', 'Science has a data management problem, as well as a project management\\nproblem. While industrial-grade data science teams have embraced the agile\\nmindset, and adopted or created all kind of tools to create reproducible\\nworkflows, academia-based science is still (mostly) mired in a mindset that is\\nfocused on a single final product (a paper), without focusing on incremental\\nimprovement, on any specific problem or customer, or, paying any attention\\nreproducibility. In this report we argue towards the adoption of the agile\\nmindset and agile data science tools in academia, to make a more responsible,\\nand over all, reproducible science.', 'Although there are various ways to represent data patterns and models,\\nvisualization has been primarily taught in many data science courses for its\\nefficiency. Such vision-dependent output may cause critical barriers against\\nthose who are blind and visually impaired and people with learning\\ndisabilities. We argue that instructors need to teach multiple data\\nrepresentation methods so that all students can produce data products that are\\nmore accessible. In this paper, we argue that accessibility should be taught as\\nearly as the introductory course as part of the data science curriculum so that\\nregardless of whether learners major in data science or not, they can have\\nfoundational exposure to accessibility. As data science educators who teach\\naccessibility as part of our lower-division courses in two different\\ninstitutions, we share specific examples that can be utilized by other data\\nscience instructors.', 'Many Natural Language Processing (NLP) systems use annotated corpora for\\ntraining and evaluation. However, labeled data is often costly to obtain and\\nscaling annotation projects is difficult, which is why annotation tasks are\\noften outsourced to paid crowdworkers. Citizen Science is an alternative to\\ncrowdsourcing that is relatively unexplored in the context of NLP. To\\ninvestigate whether and how well Citizen Science can be applied in this\\nsetting, we conduct an exploratory study into engaging different groups of\\nvolunteers in Citizen Science for NLP by re-annotating parts of a pre-existing\\ncrowdsourced dataset. Our results show that this can yield high-quality\\nannotations and attract motivated volunteers, but also requires considering\\nfactors such as scalability, participation over time, and legal and ethical\\nissues. We summarize lessons learned in the form of guidelines and provide our\\ncode and data to aid future work on Citizen Science.', 'Ethics have become an urgent concern for data science research, practice, and\\ninstruction in the wake of growing critique of algorithms and systems showing\\nthat they reinforce structural oppression. There has been increasing desire on\\nthe part of data science educators to craft curricula that speak to these\\ncritiques, yet much ethics education remains individualized, focused on\\nspecific cases, or too abstract and unapplicable. We synthesized some of the\\nmost popular critical data science works and designed a data science ethics\\ncourse that spoke to the social phenomena at the root of critical data studies\\n-- theories of oppression, social systems, power, history, and change --\\nthrough analysis of a pressing sociotechnical system: surveillance systems.\\nThrough analysis of student reflections and final projects, we determined that\\nat the conclusion of the semester, all students had developed critical analysis\\nskills that allowed them to investigate surveillance systems of their own and\\nidentify their benefits, harms, main proponents, those who resist them, and\\ntheir interplay with social systems, all while considering dimensions of race,\\nclass, gender, and more. We argue that this type of instruction -- directly\\nteaching data science ethics alongside social theory -- is a crucial next step\\nfor the field.', \"Science and Engineering fairs offer K-12 students opportunities to engage\\nwith authentic STEM practices. Particularly, students are given the chance to\\nexperience authentic and open inquiry processes, by defining which themes,\\nquestions and approaches will guide their scientific endeavors. In this study,\\nwe analyzed data from over 5,000 projects presented at a nationwide science\\nfair in Brazil over the past 20 years using topic modeling to identify the main\\ntopics that have driven students' inquiry and design. Our analysis identified a\\nbroad range of topics being explored, with significant variations over time,\\nregion, and school setting. We argue those results and proposed methodology can\\nnot only support further research in the context of science fairs, but also\\ninform instruction and design of contexts-specific resources to support\\nstudents in open inquiry experiences in different settings.\", 'The entity alignment of science and technology patents aims to link the\\nequivalent entities in the knowledge graph of different science and technology\\npatent data sources. Most entity alignment methods only use graph neural\\nnetwork to obtain the embedding of graph structure or use attribute text\\ndescription to obtain semantic representation, ignoring the process of\\nmulti-information fusion in science and technology patents. In order to make\\nuse of the graphic structure and auxiliary information such as the name,\\ndescription and attribute of the patent entity, this paper proposes an entity\\nalignment method based on the graph convolution network for science and\\ntechnology patent information fusion. Through the graph convolution network and\\nBERT model, the structure information and entity attribute information of the\\nscience and technology patent knowledge graph are embedded and represented to\\nachieve multi-information fusion, thus improving the performance of entity\\nalignment. Experiments on three benchmark data sets show that the proposed\\nmethod Hit@K The evaluation indicators are better than the existing methods.', 'In light of the rapidly evolving capabilities of large language models\\n(LLMs), it becomes imperative to develop rigorous domain-specific evaluation\\nbenchmarks to accurately assess their capabilities. In response to this need,\\nthis paper introduces ArcMMLU, a specialized benchmark tailored for the Library\\n& Information Science (LIS) domain in Chinese. This benchmark aims to measure\\nthe knowledge and reasoning capability of LLMs within four key sub-domains:\\nArchival Science, Data Science, Library Science, and Information Science.\\nFollowing the format of MMLU/CMMLU, we collected over 6,000 high-quality\\nquestions for the compilation of ArcMMLU. This extensive compilation can\\nreflect the diverse nature of the LIS domain and offer a robust foundation for\\nLLM evaluation. Our comprehensive evaluation reveals that while most mainstream\\nLLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a\\nnotable performance gap, suggesting substantial headroom for refinement in LLM\\ncapabilities within the LIS domain. Further analysis explores the effectiveness\\nof few-shot examples on model performance and highlights challenging questions\\nwhere models consistently underperform, providing valuable insights for\\ntargeted improvements. ArcMMLU fills a critical gap in LLM evaluations within\\nthe Chinese LIS domain and paves the way for future development of LLMs\\ntailored to this specialized area.', 'While microscopic laws of physics are invariant under the reversal of the\\narrow of time, the transport of energy and information in most devices is an\\nirreversible process. It is this irreversibility that leads to intrinsic\\ndissipations in electronic devices and limits the possibility of quantum\\ncomputation. We theoreticallypredict that the electric field can induce a\\nsubstantial amount of dissipationless quantum spin current at room temperature,\\nin hole doped semiconductors such as Si, Ge and GaAs. Based on a generalization\\nof the quantum Hall effect, the predicted effect leads to efficient spin\\ninjection without the need for metallic ferromagnets. Principles found in this\\nwork could enable quantum spintronic devices with integrated information\\nprocessing and storage units, operating with low power consumption and\\nperforming reversible quantum computation.', 'In order to assess Microsoft Academic as a useful data source for evaluative\\nbibliometrics it is crucial to know, if citation counts from Microsoft Academic\\ncould be used in common normalization procedures and whether the normalized\\nscores agree with the scores calculated on the basis of established databases.\\nTo this end, we calculate the field-normalized citation scores of the\\npublications of a computer science institute based on Microsoft Academic and\\nthe Web of Science and estimate the statistical concordance of the scores. Our\\nresults suggest that field-normalized citation scores can be calculated with\\nMicrosoft Academic and that these scores are in good agreement with the\\ncorresponding scores from the Web of Science.', 'Cyber-security solutions are traditionally static and signature-based. The\\ntraditional solutions along with the use of analytic models, machine learning\\nand big data could be improved by automatically trigger mitigation or provide\\nrelevant awareness to control or limit consequences of threats. This kind of\\nintelligent solutions is covered in the context of Data Science for\\nCyber-security. Data Science provides a significant role in cyber-security by\\nutilising the power of data (and big data), high-performance computing and data\\nmining (and machine learning) to protect users against cyber-crimes. For this\\npurpose, a successful data science project requires an effective methodology to\\ncover all issues and provide adequate resources. In this paper, we are\\nintroducing popular data science methodologies and will compare them in\\naccordance with cyber-security challenges. A comparison discussion has also\\ndelivered to explain methodologies strengths and weaknesses in case of\\ncyber-security projects.', 'The upcoming Square Kilometre Array (SKA) radio telescope will become the\\nlargest astronomical observation facility, and is expected to introduce\\nrevolutionary changes in major fields of natural sciences. These revolutionary\\nchanges help us to answer the fundamental questions related to the origins of\\nthe universe, life, cosmic magnetic field, the nature of gravity, and to search\\nfor extraterrestrial civilizations. The unprecedented power of the SKA is\\ncharacterized by an extremely high sensitivity, wide field of view, ultra-fast\\nsurvey speed, and ultra-high time, space, frequency resolutions, which ensures\\nthat SKA will have a leading position in radio astronomy in future; this will\\nbe accompanied by a vast amount of observational data at exabyte (EB) level.\\nThe transportation, storage, reading, writing, computation, curation, and\\narchiving of the SKA-level data and the release of SKA science products are\\nposing serious challenges to the field of information communication technology\\n(ICT). The China SKA science team will work together with the information,\\ncommunication and computer industries to tackle the challenges associated with\\nthe SKA big data, which will not only promote major original scientific\\ndiscoveries, but also apply the obtained technological achievements for\\nstimulating the national economy.', 'Associated Legendre polynomials and spherical harmonics are central to\\ncalculations in many fields of science and mathematics - not only chemistry but\\ncomputer graphics, magnetic, seismology and geodesy. There are a number of\\nalgorithms for these functions published since 1960 but none of them satisfy\\nour requirements. In this paper, we present a comprehensive review of\\nalgorithms in the literature and, based on them, propose an efficient and\\naccurate code for quantum chemistry. Our requirements are to efficiently\\ncalculate these functions for all non-negative integer degrees and orders up to\\na given number (<=1000) and the absolute or the relative error of each\\ncalculated value should not exceed 10E-10. We achieve this by normalizing the\\npolynomials, employing efficient and stable recurrence relations, and\\nprecomputing coefficients. The algorithm presented here is straightforward and\\nmay be used in other areas of science.', 'Citation network analysis has become one of methods to study how scientific\\nknowledge flows from one domain to another. Health informatics is a\\nmultidisciplinary field that includes social science, software engineering,\\nbehavioral science, medical science and others. In this study, we perform an\\nanalysis of citation statistics from health informatics journals using data set\\nextracted from CrossRef. For each health informatics journal, we extract the\\nnumber of citations from/to studies related to computer science,\\nmedicine/clinical medicine and other fields, including the number of\\nself-citations from the health informatics journal. With a similar number of\\narticles used in our analysis, we show that the Journal of the American Medical\\nInformatics Association (JAMIA) has more in-citations than the Journal of\\nMedical Internet Research (JMIR); while JMIR has a higher number of\\nout-citations and self-citations. We also show that JMIR cites more articles\\nfrom health informatics journals and medicine related journals. In addition,\\nthe Journal of Medical Systems (JMS) cites more articles from computer science\\njournals compared with other health informatics journals included in our\\nanalysis.', 'This article is a position paper written in reaction to the now-infamous\\npaper titled \"On the Dangers of Stochastic Parrots: Can Language Models Be Too\\nBig?\" by Timnit Gebru, Emily Bender, and others who were, as of the date of\\nthis writing, still unnamed. I find the ethics of the Parrot Paper lacking, and\\nin that lack, I worry about the direction in which computer science, machine\\nlearning, and artificial intelligence are heading. At best, I would describe\\nthe argumentation and evidentiary practices embodied in the Parrot Paper as\\nSlodderwetenschap (Dutch for Sloppy Science) -- a word which the academic world\\nlast widely used in conjunction with the Diederik Stapel affair in psychology\\n[2]. What is missing in the Parrot Paper are three critical elements: 1)\\nacknowledgment that it is a position paper/advocacy piece rather than research,\\n2) explicit articulation of the critical presuppositions, and 3) explicit\\nconsideration of cost/benefit trade-offs rather than a mere recitation of\\npotential \"harms\" as if benefits did not matter. To leave out these three\\nelements is not good practice for either science or research.', 'Born in the late 20s, R is one of the most popular software for statistical\\ncomputing and graphics. With the development of information technology and the\\nadvent of the big data era, great changes have taken place in the R ecosystem.\\nBased on the meta information of the Comprehensive R Archive Network (CRAN) and\\nthe bibliometric data of literature citing R, we discovered that while R is\\ninitiated by statistics, its development is benefited greatly from computer\\nscience and the main user group in academics come from various disciplines such\\nas agricultural science, biological science, environmental science and medical\\nscience. In addition, we displayed the collaboration patterns among R\\ndevelopers and analyze the possible effects of collaboration in the R\\ncommunity.', 'Citizen science has become a popular tool for preliminary data processing\\ntasks, such as identifying and counting Lunar impact craters in modern\\nhigh-resolution imagery. However, use of such data requires that citizen\\nscience products are understandable and reliable. Contamination and missing\\ndata can reduce the usefulness of datasets so it is important that such effects\\nare quantified. This paper presents a method, based upon a newly developed\\nquantitative pattern recognition system (Linear Poisson Models) for estimating\\nlevels of contamination within MoonZoo citizen science crater data. Evidence\\nwill show that it is possible to remove the effects of contamination, with\\nreference to some agreed upon ground truth, resulting in estimated crater\\ncounts which are highly repeatable. However, it will also be shown that\\ncorrecting for missing data is currently more difficult to achieve. The\\ntechniques are tested on MoonZoo citizen science crater annotations from the\\nApollo 17 site and also undergraduate and expert results from the same region.', 'Word embeddings are an essential instrument in many NLP tasks. Most available\\nresources are trained on general language from Web corpora or Wikipedia dumps.\\nHowever, word embeddings for domain-specific language are rare, in particular\\nfor the social science domain. Therefore, in this work, we describe the\\ncreation and evaluation of word embedding models based on 37,604 open-access\\nsocial science research papers. In the evaluation, we compare domain-specific\\nand general language models for (i) language coverage, (ii) diversity, and\\n(iii) semantic relationships. We found that the created domain-specific model,\\neven with a relatively small vocabulary size, covers a large part of social\\nscience concepts, their neighborhoods are diverse in comparison to more general\\nmodels. Across all relation types, we found a more extensive coverage of\\nsemantic relationships.', \"This paper presents the development of a specialized chatbot for materials\\nscience, leveraging the Llama-2 language model, and continuing pre-training on\\nthe expansive research articles in the materials science domain from the S2ORC\\ndataset. The methodology involves an initial pretraining phase on over one\\nmillion domain-specific papers, followed by an instruction-tuning process to\\nrefine the chatbot's capabilities. The chatbot is designed to assist\\nresearchers, educators, and students by providing instant, context-aware\\nresponses to queries in the field of materials science. We make the four\\ntrained checkpoints (7B, 13B, with or without chat ability) freely available to\\nthe research community at https://github.com/Xianjun-Yang/Quokka.\", 'This study examines the differences between Scopus and Web of Science in the\\ncitation counting, citation ranking, and h-index of 22 top human-computer\\ninteraction (HCI) researchers from EQUATOR--a large British Interdisciplinary\\nResearch Collaboration project. Results indicate that Scopus provides\\nsignificantly more coverage of HCI literature than Web of Science, primarily\\ndue to coverage of relevant ACM and IEEE peer-reviewed conference proceedings.\\nNo significant differences exist between the two databases if citations in\\njournals only are compared. Although broader coverage of the literature does\\nnot significantly alter the relative citation ranking of individual\\nresearchers, Scopus helps distinguish between the researchers in a more nuanced\\nfashion than Web of Science in both citation counting and h-index. Scopus also\\ngenerates significantly different maps of citation networks of individual\\nscholars than those generated by Web of Science. The study also presents a\\ncomparison of h-index scores based on Google Scholar with those based on the\\nunion of Scopus and Web of Science. The study concludes that Scopus can be used\\nas a sole data source for citation-based research and evaluation in HCI,\\nespecially if citations in conference proceedings are sought and that h scores\\nshould be manually calculated instead of relying on system calculations.', 'The basic objective of data visualization is to provide an efficient\\ngraphical display for summarizing and reasoning about quantitative information.\\nDuring the last decades, political science has accumulated a large corpus of\\nvarious kinds of data such as comprehensive factbooks and atlases,\\ncharacterizing all or most of existing states by multiple and objectively\\nassessed numerical indicators within certain time lapse. As a consequence,\\nthere exists a continuous trend for political science to gradually become a\\nmore quantitative scientific field and to use quantitative information in the\\nanalysis and reasoning. It is believed that any objective analysis in political\\nscience must be multidimensional and combine various sources of quantitative\\ninformation; however, human capabilities for perception of large massifs of\\nnumerical information are limited. Hence, methods and approaches for\\nvisualization of quantitative and qualitative data (and, especially\\nmultivariate data) is an extremely important topic. Data visualization\\napproaches can be classified into several groups, starting from creating\\ninformative charts and diagrams (statistical graphics and infographics) and\\nending with advanced statistical methods for visualizing multidimensional\\ntables containing both quantitative and qualitative information. In this\\narticle we provide a short review of existing methods of data visualization\\nmethods with applications in political and social science.', \"The recent emergence of online citizen science is illustrative of an\\nefficient and effective means to harness the crowd in order to achieve a range\\nof scientific discoveries. Fundamentally, citizen science projects draw upon\\ncrowds of non-expert volunteers to complete short Tasks, which can vary in\\ndomain and complexity. However, unlike most human-computational systems,\\nparticipants in these systems, the `citizen scientists' are volunteers, whereby\\nno incentives, financial or otherwise, are offered. Furthermore, encouraged by\\ncitizen science platforms such as Zooniverse, online communities have emerged,\\nproviding them with an environment to discuss, share ideas, and solve problems.\\nIn fact, it is the result of these forums that has enabled a number of\\nscientific discoveries to be made. In this paper we explore the phenomenon of\\ncollective intelligence via the relationship between the activities of online\\ncitizen science communities and the discovery of scientific knowledge. We\\nperform a cross-project analysis of ten Zooniverse citizen science projects and\\nanalyse the behaviour of users with regards to their Task completion activity\\nand participation in discussion and discover collective behaviour amongst\\nhighly active users. Whilst our findings have implications for future citizen\\nscience design, we also consider the wider implications for understanding\\ncollective intelligence research in general.\", \"Ocean science, which delves into the oceans that are reservoirs of life and\\nbiodiversity, is of great significance given that oceans cover over 70% of our\\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\\ntransformed the paradigm in science. Despite the success in other domains,\\ncurrent LLMs often fall short in catering to the needs of domain experts like\\noceanographers, and the potential of LLMs for ocean science is under-explored.\\nThe intrinsic reason may be the immense and intricate nature of ocean data as\\nwell as the necessity for higher granularity and richness in knowledge. To\\nalleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean\\ndomain, which is expert in various ocean science tasks. We propose DoInstruct,\\na novel framework to automatically obtain a large volume of ocean domain\\ninstruction data, which generates instructions based on multi-agent\\ncollaboration. Additionally, we construct the first oceanography benchmark,\\nOceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though\\ncomprehensive experiments, OceanGPT not only shows a higher level of knowledge\\nexpertise for oceans science tasks but also gains preliminary embodied\\nintelligence capabilities in ocean technology. Codes, data and checkpoints will\\nsoon be available at https://github.com/zjunlp/KnowLM.\", 'The strict globular $\\\\omega$-categories formalize the execution paths of a\\nparallel automaton and the homotopies between them. One associates to such (and\\nany) $\\\\omega$-category $\\\\C$ three homology theories. The first one is called\\nthe globular homology. It contains the oriented loops of $\\\\C$. The two other\\nones are called the negative (resp. positive) corner homology. They contain in\\na certain manner the branching areas of execution paths or negative corners\\n(resp. the merging areas of execution paths or positive corners) of $\\\\C$. Two\\nnatural linear maps called the negative (resp. the positive) Hurewicz morphism\\nfrom the globular homology to the negative (resp. positive) corner homology are\\nconstructed. We explain the reason why these constructions allow to\\nreinterprete some geometric problems coming from computer science.', \"We present a novel computational framework that connects Blue Waters, the\\nNSF-supported, leadership-class supercomputer operated by NCSA, to the Laser\\nInterferometer Gravitational-Wave Observatory (LIGO) Data Grid via Open Science\\nGrid technology. To enable this computational infrastructure, we configured,\\nfor the first time, a LIGO Data Grid Tier-1 Center that can submit\\nheterogeneous LIGO workflows using Open Science Grid facilities. In order to\\nenable a seamless connection between the LIGO Data Grid and Blue Waters via\\nOpen Science Grid, we utilize Shifter to containerize LIGO's workflow software.\\nThis work represents the first time Open Science Grid, Shifter, and Blue Waters\\nare unified to tackle a scientific problem and, in particular, it is the first\\ntime a framework of this nature is used in the context of large scale\\ngravitational wave data analysis. This new framework has been used in the last\\nseveral weeks of LIGO's second discovery campaign to run the most\\ncomputationally demanding gravitational wave search workflows on Blue Waters,\\nand accelerate discovery in the emergent field of gravitational wave\\nastrophysics. We discuss the implications of this novel framework for a wider\\necosystem of Higher Performance Computing users.\", \"Without sufficient information about research data practices occurring in a\\nparticular research organisation, there is a risk of mismatching research data\\nservice efforts with the needs of its researchers. This study describes how\\ndata acquiring and data sharing occurring within a particular research\\norganisation can be investigated by using current research information system\\npublication data. A sample of 193 journal articles published by researchers in\\nthe computer science department of the case study's university during 2019 were\\nextracted for scrutiny from the current research information system. For these\\n193 articles, a classification of the main study types was developed to\\naccommodate the multidisciplinary nature of the case department's research\\nagenda. Furthermore, a coding framework was developed to capture the key\\nelements of data acquiring and data sharing. The articles representing life\\nsciences and computational research relatively frequently reused open data,\\nwhereas data acquisition of experimental research, human interaction studies\\nand human intervention studies often relied on collecting original data. Data\\nsharing also differed between the computationally intensive study types of life\\nsciences and computational research and the study types relying on collection\\nof original data. Research data were not available for reuse in only a minority\\nof life science (n= 2; 7%) and computational research (n = 15; 14%) studies.\\nThe study types of experimental research, human interaction studies and human\\nintervention studies less frequently made their data available for reuse. This\\nstudy demonstrates that analyses of publications listed in current research\\ninformation systems provide detailed descriptions how the affiliated\\nresearchers acquire and share research data.\", \"Computational modeling and numerical simulations have become indispensable\\ntools in science, technology, engineering and mathematics (STEM), and in\\nindustrial research and development. Consequently, there is growing demand for\\ncomputational skills in undergraduate and postgraduate students and a need to\\nrevise existing curricula. We report from a case study where an existing\\nmaterials science module was modified to contain numerical simulation of the\\nmaterials: students use simulation software to explore the material's behavior\\nin simulated experiments. In particular, the Ubermag micromagnetic simulation\\nsoftware package is used by the students in order to solve problems\\ncomputationally that are linked to current research in the field of magnetism.\\nThe simulation software is controlled through Python code in Jupyter notebooks.\\nThis setup provides a computational environment in which simulations can be\\nstarted and analyzed in the same notebook. A key learning activity is a project\\nin which students tackle a given task over a period of approximately 2 months\\nin a small group. Our experience is that the self-paced problem-solving nature\\nof the project work -- combined with the freedom to explore the system's\\nbehavior through the simulation -- can facilitate a better in-depth exploration\\nof the course contents. We report feedback from students and educators both on\\nthe training in material science and the Jupyter notebook as a computational\\nenvironment for education. Finally, we discuss which aspects of the Ubermag and\\nthe Jupyter notebook have been beneficial for the students' learning experience\\nand which could be transferred to similar teaching activities in other subject\\nareas.\", \"Across the African continent, students grapple with various educational\\nchallenges, including limited access to essential resources such as computers,\\ninternet connectivity, reliable electricity, and a shortage of qualified\\nteachers. Despite these challenges, recent advances in AI such as BERT, and\\nGPT-4 have demonstrated their potential for advancing education. Yet, these AI\\ntools tend to be deployed and evaluated predominantly within the context of\\nWestern educational settings, with limited attention directed towards the\\nunique needs and challenges faced by students in Africa. In this book chapter,\\nwe describe our works developing and deploying AI in Education tools in Africa:\\n(1) SuaCode, an AI-powered app that enables Africans to learn to code using\\ntheir smartphones, (2) AutoGrad, an automated grading, and feedback tool for\\ngraphical and interactive coding assignments, (3) a tool for code plagiarism\\ndetection that shows visual evidence of plagiarism, (4) Kwame, a bilingual AI\\nteaching assistant for coding courses, (5) Kwame for Science, a web-based AI\\nteaching assistant that provides instant answers to students' science questions\\nand (6) Brilla AI, an AI contestant for the National Science and Maths Quiz\\ncompetition. We discuss challenges and potential opportunities to use AI to\\nadvance science and computing education across Africa.\", 'Monoidal computer is a categorical model of intensional computation, where\\nmany different programs correspond to the same input-output behavior. The\\nupshot of yet another model of computation is that a categorical formalism\\nshould provide a much needed high level language for theory of computation,\\nflexible enough to allow abstracting away the low level implementation details\\nwhen they are irrelevant, or taking them into account when they are genuinely\\nneeded. A salient feature of the approach through monoidal categories is the\\nformal graphical language of string diagrams, which supports visual reasoning\\nabout programs and computations.\\n  In the present paper, we provide a coalgebraic characterization of monoidal\\ncomputer. It turns out that the availability of interpreters and specializers,\\nthat make a monoidal category into a monoidal computer, is equivalent with the\\nexistence of a *universal state space*, that carries a weakly final state\\nmachine for any pair of input and output types. Being able to program state\\nmachines in monoidal computers allows us to represent Turing machines, to\\ncapture their execution, count their steps, as well as, e.g., the memory cells\\nthat they use. The coalgebraic view of monoidal computer thus provides a\\nconvenient diagrammatic language for studying computability and complexity.', \"The gradual crowding out of singleton and small team science by large team\\nendeavors is challenging key features of research culture. It is therefore\\nimportant for the future of scientific practice to reflect upon the individual\\nscientist's ethical responsibilities within teams. To facilitate this\\nreflection we show labor force trends in the US revealing a skewed growth in\\nacademic ranks and increased levels of competition for promotion within the\\nsystem; we analyze teaming trends across disciplines and national borders\\ndemonstrating why it is becoming difficult to distribute credit and to avoid\\nconflicts of interest; and we use more than a century of Nobel prize data to\\nshow how science is outgrowing its old institutions of singleton awards. Of\\nparticular concern within the large team environment is the weakening of the\\nmentor-mentee relation, which undermines the cultivation of virtue ethics\\nacross scientific generations. These trends and emerging organizational\\ncomplexities call for a universal set of behavioral norms that transcend team\\nheterogeneity and hierarchy. To this end, our expository analysis provides a\\nsurvey of ethical issues in team settings to inform science ethics education\\nand science policy.\", 'The provision of open science is defined as a general policy aimed at\\novercoming the barriers that hinder the implementation of the European Research\\nArea (ERA). An open science foundation seeks to capture all the elements needed\\nfor the functioning of ERA: research data, scientific instruments, ICT services\\n(connections, calculations, platforms, and specific studies such as portals).\\nManaging shared resources for the community of scholars maximizes the benefits\\nto society. In the field of digital infrastructure, this has already\\ndemonstrated great benefits. It is expected that applying this principle to an\\nopen science process will improve management by funding organizations in\\ncollaboration with stakeholders through mechanisms such as public consultation.\\nThis will increase the perception of joint ownership of the infrastructure. It\\nwill also create clear and non-discriminatory access rules, along with a sense\\nof joint ownership that stimulates a higher level of participation,\\ncollaboration and social reciprocity. The article deals with the concept of\\nopen science. The concept of the European cloud of open science and its\\nstructure are presented. According to the study, it has been shown that the\\nstructure of the cloud of open science includes an augmented reality as an\\nopen-science platform. An example of the practical application of this tool is\\nthe general description of MaxWhere, developed by Hungarian scientists, and is\\na platform of aggregates of individual 3D spaces.', 'The uptake of machine learning (ML) approaches in the social and health\\nsciences has been rather slow, and research using ML for social and health\\nresearch questions remains fragmented. This may be due to the separate\\ndevelopment of research in the computational/data versus social and health\\nsciences as well as a lack of accessible overviews and adequate training in ML\\ntechniques for non data science researchers. This paper provides a meta-mapping\\nof research questions in the social and health sciences to appropriate ML\\napproaches, by incorporating the necessary requirements to statistical analysis\\nin these disciplines. We map the established classification into description,\\nprediction, and causal inference to common research goals, such as estimating\\nprevalence of adverse health or social outcomes, predicting the risk of an\\nevent, and identifying risk factors or causes of adverse outcomes. This\\nmeta-mapping aims at overcoming disciplinary barriers and starting a fluid\\ndialogue between researchers from the social and health sciences and\\nmethodologically trained researchers. Such mapping may also help to fully\\nexploit the benefits of ML while considering domain-specific aspects relevant\\nto the social and health sciences, and hopefully contribute to the acceleration\\nof the uptake of ML applications to advance both basic and applied social and\\nhealth sciences research.', \"The Science Gateways Community Institute (SGCI) is an NSF Software\\nInfrastructure for Sustained Innovation (S2I2) funded project that leads and\\nsupports the science gateway community. Major activities for SGCI include a)\\nsustainability training, including the Focus Week week-long course designed to\\nhelp science gateway operators develop sustainability plans, and the Jumpstart\\nvirtual short-course; b) usability and user experience consulting; c) a\\ncommunity catalog of science gateways and science gateway software; d)\\nworkforce development activities, including a coding institute for students,\\ninternship opportunities, and hackathons; e) an annual conference; and f)\\nin-depth technical support for client gateway projects. The goals of SGCI's\\nEmbedded Technical Support component are to help the institute's clients to\\ncreate new science gateways or to significantly enhance existing science\\ngateways. Examples of the latter include helping to implement major new\\ncapabilities and to implement significant usability improvements suggested by\\nSGCI's usability consultants. The Embedded Technical Support component was\\nmanaged by Indiana University and involved research software engineers at San\\nDiego Supercomputer Center, Texas Advanced Computing Center, Indiana\\nUniversity, and Purdue University (through 2019). Since 2016, the component has\\ninvolved 20 research software engineers as consultants and has conducted 59\\nclient consultations. This short paper provides a summary of lessons learned\\nfrom the Embedded Technical Support program that may be useful for the research\\nsoftware engineering community.\", \"This study aims at improving the performance of scoring student responses in\\nscience education automatically. BERT-based language models have shown\\nsignificant superiority over traditional NLP models in various language-related\\ntasks. However, science writing of students, including argumentation and\\nexplanation, is domain-specific. In addition, the language used by students is\\ndifferent from the language in journals and Wikipedia, which are training\\nsources of BERT and its existing variants. All these suggest that a\\ndomain-specific model pre-trained using science education data may improve\\nmodel performance. However, the ideal type of data to contextualize pre-trained\\nlanguage model and improve the performance in automatically scoring student\\nwritten responses remains unclear. Therefore, we employ different data in this\\nstudy to contextualize both BERT and SciBERT models and compare their\\nperformance on automatic scoring of assessment tasks for scientific\\nargumentation. We use three datasets to pre-train the model: 1) journal\\narticles in science education, 2) a large dataset of students' written\\nresponses (sample size over 50,000), and 3) a small dataset of students'\\nwritten responses of scientific argumentation tasks. Our experimental results\\nshow that in-domain training corpora constructed from science questions and\\nresponses improve language model performance on a wide variety of downstream\\ntasks. Our study confirms the effectiveness of continual pre-training on\\ndomain-specific data in the education domain and demonstrates a generalizable\\nstrategy for automating science education tasks with high accuracy. We plan to\\nrelease our data and SciEdBERT models for public use and community engagement.\", 'Data science is an interdisciplinary research area where scientists are\\ntypically working with data coming from different fields. When using and\\nanalyzing data, the scientists implicitly agree to follow standards,\\nprocedures, and rules set in these fields. However, guidance on the\\nresponsibilities of the data scientists and the other involved actors in a data\\nscience project is typically missing. While literature shows that novel\\nframeworks and tools are being proposed in support of open-science, data reuse,\\nand research data management, there are currently no frameworks that can fully\\nexpress responsibilities of a data science project. In this paper, we describe\\nthe Transparency, Accountability, Privacy, and Societal Responsibility Matrix\\n(TAPS-RM) as framework to explore social, legal, and ethical aspects of data\\nscience projects. TAPS-RM acts as a tool to provide users with a holistic view\\nof their project beyond key outcomes and clarifies the responsibilities of\\nactors. We map the developed model of TAPS-RM with well-known initiatives for\\nopen data (such as FACT, FAIR and Datasheets for datasets). We conclude that\\nTAPS-RM is a tool to reflect on responsibilities at a data science project\\nlevel and can be used to advance responsible data science by design.', 'It has long been known that photonic science and especially photonic\\ncommunications can raise the speed of technologies and producing manufacturing.\\nMore recently, photonic science has also been interested in its capabilities to\\nimplement low-precision linear operations, such as matrix multiplications, fast\\nand effciently. For a long time most scientists taught that Electronics is the\\nend of science but after many years and about 35 years ago had been understood\\nthat electronics do not answer alone and should have a new science. Today we\\nface modern ways and instruments for doing tasks as soon as possible in\\nproportion to many decays before. The velocity of progress in science is very\\nfast. All our progress in science area is dependent on modern knowledge about\\nnew methods. In this research, we want to review the concept of a photonic\\nneural network. For this research was selected 18 main articles were among the\\nmain 30 articles on this subject from 2015 to the 2022 year. These articles\\nnoticed three principles: 1- Experimental concepts, 2- Theoretical concepts,\\nand, finally 3- Mathematic concepts. We should be careful with this research\\nbecause mathematics has a very important and constructive role in our topics!\\nOne of the topics that are very valid and also new, is simulation. We used to\\nwork with simulation in some parts of this research. First, briefly, we start\\nby introducing photonics and neural networks. In the second we explain the\\nadvantages and disadvantages of a combination of both in the science world and\\nindustries and technologies about them. Also, we are talking about the\\nachievements of a thin modern science. Third, we try to introduce some\\nimportant and valid parameters in neural networks. In this manner, we use many\\nmathematic tools in some portions of this article.', 'We present MatSci-NLP, a natural language benchmark for evaluating the\\nperformance of natural language processing (NLP) models on materials science\\ntext. We construct the benchmark from publicly available materials science text\\ndata to encompass seven different NLP tasks, including conventional NLP tasks\\nlike named entity recognition and relation classification, as well as NLP tasks\\nspecific to materials science, such as synthesis action retrieval which relates\\nto creating synthesis procedures for materials. We study various BERT-based\\nmodels pretrained on different scientific text corpora on MatSci-NLP to\\nunderstand the impact of pretraining strategies on understanding materials\\nscience text. Given the scarcity of high-quality annotated data in the\\nmaterials science domain, we perform our fine-tuning experiments with limited\\ntraining data to encourage the generalize across MatSci-NLP tasks. Our\\nexperiments in this low-resource training setting show that language models\\npretrained on scientific text outperform BERT trained on general text. MatBERT,\\na model pretrained specifically on materials science journals, generally\\nperforms best for most tasks. Moreover, we propose a unified text-to-schema for\\nmultitask learning on \\\\benchmark and compare its performance with traditional\\nfine-tuning methods. In our analysis of different training methods, we find\\nthat our proposed text-to-schema methods inspired by question-answering\\nconsistently outperform single and multitask NLP fine-tuning methods. The code\\nand datasets are publicly available at\\n\\\\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23}.', 'Emerging tools bring forth fresh approaches to work, and the field of natural\\nscience is no different. In natural science, traditional manual, serial, and\\nlabour-intensive work is being augmented by automated, parallel, and iterative\\nprocesses driven by artificial intelligence-based experimental automation and\\nmore. To add new capabilities in natural science, enabling the acceleration and\\nenrichment of automation of the discovery process, we present DARWIN, a series\\nof tailored LLMs for natural science, mainly in physics, chemistry, and\\nmaterial science. This series relies on open-source LLM, incorporating\\nstructured and unstructured scientific knowledge from public datasets and\\nliterature. We fine-tuned the models using over 60,000 instruction data points,\\nemphasizing factual correctness. During the fine-tuning, we introduce the\\nScientific Instruction Generation (SIG) model, automating instruction\\ngeneration from scientific texts. This eliminates the need for manual\\nextraction or domain-specific knowledge graphs and efficiently injects\\nscientific knowledge into the model. We also explore multi-task training\\nstrategies, revealing interconnections between scientific tasks. DARWIN series\\nnot only achieves state-of-the-art results on various scientific tasks but also\\ndiminishes reliance on closed-source AI models. Our research showcases the\\nability of LLM in the scientific domain, with the overarching goal of fostering\\nprosperity within the broader AI for science community.', 'Gradient methods are experiencing a growth in methodological and theoretical\\ndevelopments owing to the challenges of optimization problems arising in data\\nscience. Focusing on data science applications with expensive objective\\nfunction evaluations yet inexpensive gradient function evaluations, gradient\\nmethods that never make objective function evaluations are either being\\nrejuvenated or actively developed. However, as we show, such gradient methods\\nare all susceptible to catastrophic divergence under realistic conditions for\\ndata science applications. In light of this, gradient methods which make use of\\nobjective function evaluations become more appealing, yet, as we show, can\\nresult in an exponential increase in objective evaluations between accepted\\niterates. As a result, existing gradient methods are poorly suited to the needs\\nof optimization problems arising from data science. In this work, we address\\nthis gap by developing a generic methodology that economically uses objective\\nfunction evaluations in a problem-driven manner to prevent catastrophic\\ndivergence and avoid an explosion in objective evaluations between accepted\\niterates. Our methodology allows for specific procedures that can make use of\\nspecific step size selection methodologies or search direction strategies, and\\nwe develop a novel step size selection methodology that is well-suited to data\\nscience applications. We show that a procedure resulting from our methodology\\nis highly competitive with standard optimization methods on CUTEst test\\nproblems. We then show a procedure resulting from our methodology is highly\\nfavorable relative to standard optimization methods on optimization problems\\narising in our target data science applications. Thus, we provide a novel\\ngradient methodology that is better suited to optimization problems arising in\\ndata science.', \"We propose an instruction-based process for trustworthy data curation in\\nmaterials science (MatSci-Instruct), which we then apply to finetune a\\nLLaMa-based language model targeted for materials science (HoneyBee).\\nMatSci-Instruct helps alleviate the scarcity of relevant, high-quality\\nmaterials science textual data available in the open literature, and HoneyBee\\nis the first billion-parameter language model specialized to materials science.\\nIn MatSci-Instruct we improve the trustworthiness of generated data by\\nprompting multiple commercially available large language models for generation\\nwith an Instructor module (e.g. Chat-GPT) and verification from an independent\\nVerifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of\\nmultiple tasks and measure the quality of our dataset along multiple\\ndimensions, including accuracy against known facts, relevance to materials\\nscience, as well as completeness and reasonableness of the data. Moreover, we\\niteratively generate more targeted instructions and instruction-data in a\\nfinetuning-evaluation-feedback loop leading to progressively better performance\\nfor our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark\\nshows HoneyBee's outperformance of existing language models on materials\\nscience tasks and iterative improvement in successive stages of\\ninstruction-data refinement. We study the quality of HoneyBee's language\\nmodeling through automatic evaluation and analyze case studies to further\\nunderstand the model's capabilities and limitations. Our code and relevant\\ndatasets are publicly available at\\n\\\\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee}.\", 'Data science has been described as the fourth paradigm for scientific\\ndiscovery. The latest wave of data science research, pertaining to machine\\nlearning and artificial intelligence (AI), is growing exponentially and\\ngarnering millions of annual citations. However, this growth has been\\naccompanied by a diminishing emphasis on social good challenges - our analysis\\nreveals that the proportion of data science research focusing on social good is\\nless than it has ever been. At the same time, the proliferation of machine\\nlearning and generative AI have sparked debates about the socio-technical\\nprospects and challenges associated with data science for human flourishing,\\norganizations, and society. Against this backdrop, we present a framework for\\n\"data science for social good\" (DSSG) research that considers the interplay\\nbetween relevant data science research genres, social good challenges, and\\ndifferent levels of socio-technical abstraction. We perform an analysis of the\\nliterature to empirically demonstrate the paucity of work on DSSG in\\ninformation systems (and other related disciplines) and highlight current\\nimpediments. We then use our proposed framework to introduce the articles\\nappearing in the special issue. We hope that this article and the special issue\\nwill spur future DSSG research and help reverse the alarming trend across data\\nscience research over the past 30-plus years in which social good challenges\\nare garnering proportionately less attention with each passing day.', 'The (meta)logic underlying classical theory of computation is Boolean\\n(two-valued) logic. Quantum logic was proposed by Birkhoff and von Neumann as a\\nlogic of quantum mechanics more than sixty years ago. The major difference\\nbetween Boolean logic and quantum logic is that the latter does not enjoy\\ndistributivity in general. The rapid development of quantum computation in\\nrecent years stimulates us to establish a theory of computation based on\\nquantum logic. The present paper is the first step toward such a new theory and\\nit focuses on the simplest models of computation, namely finite automata. It is\\nfound that the universal validity of many properties of automata depend heavily\\nupon the distributivity of the underlying logic. This indicates that these\\nproperties does not universally hold in the realm of quantum logic. On the\\nother hand, we show that a local validity of them can be recovered by imposing\\na certain commutativity to the (atomic) statements about the automata under\\nconsideration. This reveals an essential difference between the classical\\ntheory of computation and the computation theory based on quantum logic.', \"We define the model of quantum circuits with density matrices, where\\nnon-unitary gates are allowed. Measurements in the middle of the computation,\\nnoise and decoherence are implemented in a natural way in this model, which is\\nshown to be equivalent in computational power to standard quantum circuits.\\n  The main result in this paper is a solution for the subroutine problem: The\\ngeneral function that a quantum circuit outputs is a probabilistic function,\\nbut using pure state language, such a function can not be used as a black box\\nin other computations. We give a natural definition of using general\\nsubroutines, and analyze their computational power.\\n  We suggest convenient metrics for quantum computing with mixed states. For\\ndensity matrices we analyze the so called ``trace metric'', and using this\\nmetric, we define and discuss the ``diamond metric'' on superoperators. These\\nmetrics enable a formal discussion of errors in the computation.\\n  Using a ``causality'' lemma for density matrices, we also prove a simple\\nlower bound for probabilistic functions.\", 'A pseudorandom point in an ergodic dynamical system over a computable metric\\nspace is a point which is computable but its dynamics has the same statistical\\nbehavior as a typical point of the system.\\n  It was proved in [Avigad et al. 2010, Local stability of ergodic averages]\\nthat in a system whose dynamics is computable the ergodic averages of\\ncomputable observables converge effectively. We give an alternative, simpler\\nproof of this result.\\n  This implies that if also the invariant measure is computable then the\\npseudorandom points are a set which is dense (hence nonempty) on the support of\\nthe invariant measure.', \"We investigate the connection between measure, capacity and algorithmic\\nrandomness for the space of closed sets. For any computable measure m, a\\ncomputable capacity T may be defined by letting T(Q) be the measure of the\\nfamily of closed sets K which have nonempty intersection with Q. We prove an\\neffective version of Choquet's capacity theorem by showing that every\\ncomputable capacity may be obtained from a computable measure in this way. We\\nestablish conditions on the measure m that characterize when the capacity of an\\nm-random closed set equals zero. This includes new results in classical\\nprobability theory as well as results for algorithmic randomness. For certain\\ncomputable measures, we construct effectively closed sets with positive\\ncapacity and with Lebesgue measure zero. We show that for computable measures,\\na real q is upper semi-computable if and only if there is an effectively closed\\nset with capacity q.\", \"We give a rigorous framework for the interaction of physical computing\\ndevices with abstract computation. Device and program are mediated by the\\nnon-logical 'representation relation'; we give the conditions under which\\nrepresentation and device theory give rise to commuting diagrams between\\nlogical and physical domains, and the conditions for computation to occur. We\\ngive the interface of this new framework with currently existing formal\\nmethods, showing in particular its close relationship to refinement theory, and\\nthe implications for questions of meaning and reference in theoretical computer\\nscience. The case of hybrid computing is considered in detail, addressing in\\nparticular the example of an internet-mediated 'social machine', and the\\nabstraction/representation framework used to provide a formal distinction\\nbetween heterotic and hybrid computing. This forms the basis for future use of\\nthe framework in formal treatments of nonstandard physical computers.\", 'We introduce the notion of feedback computable functions from $2^\\\\omega$ to\\n$2^\\\\omega$, extending feedback Turing computation in analogy with the standard\\nnotion of computability for functions from $2^\\\\omega$ to $2^\\\\omega$. We then\\nshow that the feedback computable functions are precisely the effectively Borel\\nfunctions. With this as motivation we define the notion of a feedback\\ncomputable function on a structure, independent of any coding of the structure\\nas a real. We show that this notion is absolute, and as an example characterize\\nthose functions that are computable from a Gandy ordinal with some finite\\nsubset distinguished.', 'Our society is digital: industry, science, governance, and individuals\\ndepend, often transparently, on the inter-operation of large numbers of\\ndistributed computer systems. Although the society takes them almost for\\ngranted, these computer ecosystems are not available for all, may not be\\naffordable for long, and raise numerous other research challenges. Inspired by\\nthese challenges and by our experience with distributed computer systems, we\\nenvision Massivizing Computer Systems, a domain of computer science focusing on\\nunderstanding, controlling, and evolving successfully such ecosystems. Beyond\\nestablishing and growing a body of knowledge about computer ecosystems and\\ntheir constituent systems, the community in this domain should also aim to\\neducate many about design and engineering for this domain, and all people about\\nits principles. This is a call to the entire community: there is much to\\ndiscover and achieve.', 'This article is a fundamental study in computable measure theory. We use the\\nframework of TTE, the representation approach, where computability on an\\nabstract set X is defined by representing its elements with concrete \"names\",\\npossibly countably infinite, over some alphabet {\\\\Sigma}. As a basic\\ncomputability structure we consider a computable measure on a computable\\n$\\\\sigma$-algebra. We introduce and compare w.r.t. reducibility several natural\\nrepresentations of measurable sets. They are admissible and generally form four\\ndifferent equivalence classes. We then compare our representations with those\\nintroduced by Y. Wu and D. Ding in 2005 and 2006 and claim that one of our\\nrepresentations is the most useful one for studying computability on measurable\\nfunctions.', 'By the Riesz representation theorem using the Riemann-Stieltjes integral,\\nlinear continuous functionals on the set of continuous functions from the unit\\ninterval into the reals can either be characterized by functions of bounded\\nvariation from the unit interval into the reals, or by signed measures on the\\nBorel-subsets. Each of these objects has an (even minimal) Jordan decomposition\\ninto non-negative or non-decreasing objects. Using the representation approach\\nto computable analysis, a computable version of the Riesz representation\\ntheorem has been proved by Jafarikhah, Lu and Weihrauch. In this article we\\nextend this result. We study the computable relation between three Banach\\nspaces, the space of linear continuous functionals with operator norm, the\\nspace of (normalized) functions of bounded variation with total variation norm,\\nand the space of bounded signed Borel measures with variation norm. We\\nintroduce natural representations for defining computability. We prove that the\\ncanonical linear bijections between these spaces and their inverses are\\ncomputable. We also prove that Jordan decomposition is computable on each of\\nthese spaces.', 'We show that lambda calculus is a computation model which can step by step\\nsimulate any sequential deterministic algorithm for any computable function\\nover integers or words or any datatype. More formally, given an algorithm above\\na family of computable functions (taken as primitive tools, i.e., kind of\\noracle functions for the algorithm), for every constant K big enough, each\\ncomputation step of the algorithm can be simulated by exactly K successive\\nreductions in a natural extension of lambda calculus with constants for\\nfunctions in the above considered family. The proof is based on a fixed point\\ntechnique in lambda calculus and on Gurevich sequential Thesis which allows to\\nidentify sequential deterministic algorithms with Abstract State Machines. This\\nextends to algorithms for partial computable functions in such a way that\\nfinite computations ending with exceptions are associated to finite reductions\\nleading to terms with a particular very simple feature.', 'Analog computers can be revived as a feasible technology platform for low\\nprecision, energy efficient and fast computing. We justify this statement by\\nmeasuring the performance of a modern analog computer and comparing it with\\nthat of traditional digital processors. General statements are made about the\\nsolution of ordinary and partial differential equations. Computational fluid\\ndynamics are discussed as an example of large scale scientific computing\\napplications. Several models are proposed which demonstrate the benefits of\\nanalog and digital-analog hybrid computing.', 'Constraint satisfaction problems are a central pillar of modern computational\\ncomplexity theory. This survey provides an introduction to the rapidly growing\\nfield of Quantum Hamiltonian Complexity, which includes the study of quantum\\nconstraint satisfaction problems. Over the past decade and a half, this field\\nhas witnessed fundamental breakthroughs, ranging from the establishment of a\\n\"Quantum Cook-Levin Theorem\" to deep insights into the structure of 1D\\nlow-temperature quantum systems via so-called area laws. Our aim here is to\\nprovide a computer science-oriented introduction to the subject in order to\\nhelp bridge the language barrier between computer scientists and physicists in\\nthe field. As such, we include the following in this survey: (1) The\\nmotivations and history of the field, (2) a glossary of condensed matter\\nphysics terms explained in computer-science friendly language, (3) overviews of\\ncentral ideas from condensed matter physics, such as indistinguishable\\nparticles, mean field theory, tensor networks, and area laws, and (4) brief\\nexpositions of selected computer science-based results in the area. For\\nexample, as part of the latter, we provide a novel information theoretic\\npresentation of Bravyi\\'s polynomial time algorithm for Quantum 2-SAT.', 'Energy and power consumption are major limitations to continued scaling of\\ncomputing systems. Inexactness, where the quality of the solution can be traded\\nfor energy savings, has been proposed as an approach to overcoming those\\nlimitations. In the past, however, inexactness necessitated the need for highly\\ncustomized or specialized hardware. The current evolution of commercial\\noff-the-shelf(COTS) processors facilitates the use of lower-precision\\narithmetic in ways that reduce energy consumption. We study these new\\nopportunities in this paper, using the example of an inexact Newton algorithm\\nfor solving nonlinear equations. Moreover, we have begun developing a set of\\ntechniques we call reinvestment that, paradoxically, use reduced precision to\\nimprove the quality of the computed result: They do so by reinvesting the\\nenergy saved by reduced precision.', 'We give a finite presentation by generators and relations of the unitary\\noperators expressible over the {CNOT, T, X} gate set, also known as\\nCNOT-dihedral operators. To this end, we introduce a notion of normal form for\\nCNOT-dihedral circuits and prove that every CNOT-dihedral operator admits a\\nunique normal form. Moreover, we show that in the presence of certain\\nstructural rules only finitely many circuit identities are required to reduce\\nan arbitrary CNOT-dihedral circuit to its normal form.\\n  By appropriately restricting our relations, we obtain a finite presentation\\nof unitary operators expressible over the {CNOT, T} gate set as a corollary.', 'Recent experimental advances in neuroscience have opened new vistas into the\\nimmense complexity of neuronal networks. This proliferation of data challenges\\nus on two parallel fronts. First, how can we form adequate theoretical\\nframeworks for understanding how dynamical network processes cooperate across\\nwidely disparate spatiotemporal scales to solve important computational\\nproblems? And second, how can we extract meaningful models of neuronal systems\\nfrom high dimensional datasets? To aid in these challenges, we give a\\npedagogical review of a collection of ideas and theoretical methods arising at\\nthe intersection of statistical physics, computer science and neurobiology. We\\nintroduce the interrelated replica and cavity methods, which originated in\\nstatistical physics as powerful ways to quantitatively analyze large highly\\nheterogeneous systems of many interacting degrees of freedom. We also introduce\\nthe closely related notion of message passing in graphical models, which\\noriginated in computer science as a distributed algorithm capable of solving\\nlarge inference and optimization problems involving many coupled variables. We\\nthen show how both the statistical physics and computer science perspectives\\ncan be applied in a wide diversity of contexts to problems arising in\\ntheoretical neuroscience and data analysis. Along the way we discuss spin\\nglasses, learning theory, illusions of structure in noise, random matrices,\\ndimensionality reduction, and compressed sensing, all within the unified\\nformalism of the replica method. Moreover, we review recent conceptual\\nconnections between message passing in graphical models, and neural computation\\nand learning. Overall, these ideas illustrate how statistical physics and\\ncomputer science might provide a lens through which we can uncover emergent\\ncomputational functions buried deep within the dynamical complexities of\\nneuronal networks.', \"This article reports on a study investigating how computational essays can be\\nused to redistribute epistemic agency--cognitive control and responsibility\\nover one's own learning--to students in higher education STEM. Computational\\nessays are a genre of scientific writing that combine live, executable computer\\ncode with narrative text to present a computational model or analysis. The\\nstudy took place across two contrasting university contexts: an\\ninterdisciplinary data science and modeling course at Michigan State\\nUniversity, USA, and a third-semester physics course at the University of Oslo,\\nNorway. Over the course of a semester, computational essays were simultaneously\\nand independently used in both courses, and comparable datasets of student\\nartifacts and retrospective interviews were collected from both student\\npopulations. These data were analyzed using a framework which operationalized\\nthe construct of epistemic agency across the dimensions of programming,\\ninquiry, data analysis and modeling, and communication. Based on this analysis,\\nwe argue that computational essays can be a useful tool in redistributing\\nepistemic agency to students within higher education science due to their\\ncombination of adaptability and disciplinary authenticity. However, we also\\nargue that educational contexts, scaffolding, expectations, and student\\nbackgrounds can constrain and influence the ways in which students choose to\\ntake up epistemic agency.\", 'We are at a unique moment in history where there is a confluence of\\ntechnologies which will synergistically come together to transform the practice\\nof neurosurgery. These technological transformations will be all-encompassing,\\nincluding improved tools and methods for intraoperative performance of\\nneurosurgery, scalable solutions for asynchronous neurosurgical training and\\nsimulation, as well as broad aggregation of operative data allowing fundamental\\nchanges in quality assessment, billing, outcome measures, and dissemination of\\nsurgical best practices. The ability to perform surgery more safely and more\\nefficiently while capturing the operative details and parsing each component of\\nthe operation will open an entirely new epoch advancing our field and all\\nsurgical specialties. The digitization of all components within the operating\\nroom will allow us to leverage the various fields within computer and\\ncomputational science to obtain new insights that will improve care and\\ndelivery of the highest quality neurosurgery regardless of location. The\\ndemocratization of neurosurgery is at hand and will be driven by our\\ndevelopment, extraction, and adoption of these tools of the modern world.\\nVirtual reality provides a good example of how consumer-facing technologies are\\nfinding a clear role in industry and medicine and serves as a notable example\\nof the confluence of various computer science technologies creating a novel\\nparadigm for scaling human ability and interactions. The authors describe the\\ntechnology ecosystem that has come and highlight a myriad of computational and\\ndata sciences that will be necessary to enable the operating room of the near\\nfuture.', 'The goal of this article is to inspire data scientists to participate in the\\ndebate on the impact that their professional work has on society, and to become\\nactive in public debates on the digital world as data science professionals.\\nHow do ethical principles (e.g., fairness, justice, beneficence, and\\nnon-maleficence) relate to our professional lives? What lies in our\\nresponsibility as professionals by our expertise in the field? More\\nspecifically this article makes an appeal to statisticians to join that debate,\\nand to be part of the community that establishes data science as a proper\\nprofession in the sense of Airaksinen, a philosopher working on professional\\nethics. As we will argue, data science has one of its roots in statistics and\\nextends beyond it. To shape the future of statistics, and to take\\nresponsibility for the statistical contributions to data science, statisticians\\nshould actively engage in the discussions. First the term data science is\\ndefined, and the technical changes that have led to a strong influence of data\\nscience on society are outlined. Next the systematic approach from CNIL is\\nintroduced. Prominent examples are given for ethical issues arising from the\\nwork of data scientists. Further we provide reasons why data scientists should\\nengage in shaping morality around and to formulate codes of conduct and codes\\nof practice for data science. Next we present established ethical guidelines\\nfor the related fields of statistics and computing machinery. Thereafter\\nnecessary steps in the community to develop professional ethics for data\\nscience are described. Finally we give our starting statement for the debate:\\nData science is in the focal point of current societal development. Without\\nbecoming a profession with professional ethics, data science will fail in\\nbuilding trust in its interaction with and its much needed contributions to\\nsociety!', \"Consensus based publications of both competencies and undergraduate\\ncurriculum guidance documents targeting data science instruction for higher\\neducation have recently been published. Recommendations for curriculum features\\nfrom diverse sources may not result in consistent training across programs. A\\nMastery Rubric was developed that prioritizes the promotion and documentation\\nof formal growth as well as the development of independence needed for the 13\\nrequisite knowledge, skills, and abilities for professional practice in\\nstatistics and data science, SDS. The Mastery Rubric, MR, driven curriculum can\\nemphasize computation, statistics, or a third discipline in which the other\\nwould be deployed or, all three can be featured. The MR SDS supports each of\\nthese program structures while promoting consistency with international,\\nconsensus based, curricular recommendations for statistics and data science,\\nand allows 'statistics', 'data science', and 'statistics and data science'\\ncurricula to consistently educate students with a focus on increasing learners\\nindependence. The Mastery Rubric construct integrates findings from the\\nlearning sciences, cognitive and educational psychology, to support teachers\\nand students through the learning enterprise. The MR SDS will support higher\\neducation as well as the interests of business, government, and academic work\\nforce development, bringing a consistent framework to address challenges that\\nexist for a domain that is claimed to be both an independent discipline and\\npart of other disciplines, including computer science, engineering, and\\nstatistics. The MR-SDS can be used for development or revision of an evaluable\\ncurriculum that will reliably support the preparation of early e.g.,\\nundergraduate degree programs, middle e.g., upskilling and training programs,\\nand late e.g., doctoral level training practitioners.\", 'All sciences, including astronomy, are now entering the era of information\\nabundance. The exponentially increasing volume and complexity of modern data\\nsets promises to transform the scientific practice, but also poses a number of\\ncommon technological challenges. The Virtual Observatory concept is the\\nastronomical community\\'s response to these challenges: it aims to harness the\\nprogress in information technology in the service of astronomy, and at the same\\ntime provide a valuable testbed for information technology and applied computer\\nscience. Challenges broadly fall into two categories: data handling (or \"data\\nfarming\"), including issues such as archives, intelligent storage, databases,\\ninteroperability, fast networks, etc., and data mining, data understanding, and\\nknowledge discovery, which include issues such as automated clustering and\\nclassification, multivariate correlation searches, pattern recognition,\\nvisualization in highly hyperdimensional parameter spaces, etc., as well as\\nvarious applications of machine learning in these contexts. Such techniques are\\nforming a methodological foundation for science with massive and complex data\\nsets in general, and are likely to have a much broather impact on the modern\\nsociety, commerce, information economy, security, etc. There is a powerful\\nemerging synergy between the computationally enabled science and the\\nscience-driven computing, which will drive the progress in science,\\nscholarship, and many other venues in the 21st century.', 'This work introduces a general multi-level model for self-adaptive systems. A\\nself-adaptive system is seen as composed by two levels: the lower level\\ndescribing the actual behaviour of the system and the upper level accounting\\nfor the dynamically changing environmental constraints on the system. In order\\nto keep our description as general as possible, the lower level is modelled as\\na state machine and the upper level as a second-order state machine whose\\nstates have associated formulas over observable variables of the lower level.\\nThus, each state of the second-order machine identifies the set of lower-level\\nstates satisfying the constraints. Adaptation is triggered when a second-order\\ntransition is performed; this means that the current system no longer can\\nsatisfy the current high-level constraints and, thus, it has to adapt its\\nbehaviour by reaching a state that meets the new constraints. The semantics of\\nthe multi-level system is given by a flattened transition system that can be\\nstatically checked in order to prove the correctness of the adaptation model.\\nTo this aim we formalize two concepts of weak and strong adaptability providing\\nboth a relational and a logical characterization. We report that this work\\ngives a formal computational characterization of multi-level self-adaptive\\nsystems, evidencing the important role that (theoretical) computer science\\ncould play in the emerging science of complex systems.', 'Demand for data science education is surging and traditional courses offered\\nby statistics departments are not meeting the needs of those seeking training.\\nThis has led to a number of opinion pieces advocating for an update to the\\nStatistics curriculum. The unifying recommendation is computing should play a\\nmore prominent role. We strongly agree with this recommendation, but advocate\\nthe main priority is to bring applications to the forefront as proposed by\\nNolan and Speed (1999). We also argue that the individuals tasked with\\ndeveloping data science courses should not only have statistical training, but\\nalso have experience analyzing data with the main objective of solving\\nreal-world problems. Here, we share a set of general principles and offer a\\ndetailed guide derived from our successful experience developing and teaching a\\ngraduate-level, introductory data science course centered entirely on case\\nstudies. We argue for the importance of statistical thinking, as defined by\\nWild and Pfannkuck (1999) and describe how our approach teaches students three\\nkey skills needed to succeed in data science, which we refer to as creating,\\nconnecting, and computing. This guide can also be used for statisticians\\nwanting to gain more practical knowledge about data science before embarking on\\nteaching an introductory course.', \"Reproducibility should be a cornerstone of science as it enables validation\\nand reuse. In recent years, the scientific community and the general public\\nbecame increasingly aware of the reproducibility crisis, i.e. the wide-spread\\ninability of researchers to reproduce published work, including their own.\\nScientific research is increasingly focused on the creation, observation,\\nprocessing, and analysis of large data volumes. On the one hand, this\\ntransition towards computational and data-intensive science poses new\\nchallenges for research reproducibility and reuse. On the other hand, increased\\navailability and advances in computation and web technologies offer new\\nopportunities to address the reproducibility crisis. This thesis reports on\\nuser-centered design research conducted at CERN, a key laboratory in\\ndata-intensive particle physics.\\n  In this thesis, we build a wider understanding of researchers' interactions\\nwith tools that support research documentation, preservation, and sharing. From\\na Human-Computer Interaction (HCI) perspective the following aspects are\\nfundamental: (1) Characterize and map requirements and practices around\\nresearch preservation and reuse. (2) Understand the wider role and impact of\\nresearch data management (RDM) tools in scientific workflows. (3) Design tools\\nand interactions that promote, motivate, and acknowledge reproducible research\\npractices. Research reported in this thesis represents the first systematic\\napplication of HCI methods in the study and design of interactive tools for\\nreproducible science. We advocate the unique role of HCI in supporting,\\nmotivating, and transforming reproducible research practices through the design\\nof tools that enable effective RDM. This thesis paves new ways for interaction\\nwith RDM tools that support and motivate reproducible science.\", 'Many theories of scientific and technological progress imagine science as an\\niterative, developmental process periodically interrupted by innovations which\\ndisrupt and restructure the status quo. Due to the immense societal value\\ncreated by these disruptive scientific and technological innovations,\\naccurately operationalizing this perspective into quantifiable terms represents\\na key challenge for researchers seeking to understand the history and\\nmechanisms underlying scientific and technological progress. Researchers have\\nrecently proposed a number of quantitative measures that seek to quantify the\\nextent to which works in science and technology are disruptive with respect to\\ntheir scientific context. While these disruption measures show promise in their\\nability to quantify potentially disruptive works of science and technology,\\ntheir definitions are bespoke to the science of science and lack a broader\\ntheoretical framework, obscuring their interrelationships and limiting their\\nadoption within broader network science paradigms. We propose a mathematical\\nframework for conceptualizing and measuring disruptive scientific contributions\\nwithin citation networks through the lens of network centrality, and formally\\nrelate the CD Index disruption measure and its variants to betweenness\\ncentrality. By reinterpreting disruption through the lens of centrality, we\\nunify a number of existing citation-based disruption measures while\\nsimultaneously providing natural generalizations which enjoy empirical and\\ncomputational efficiencies. We validate these theoretical observations by\\ncomputing a variety of disruption measures on real citation data and find that\\ncomputing these centrality-based disruption measures over ego networks of\\nincreasing radius results in better discernment of award-winning scientific\\ninnovations relative to conventional disruption metrics which rely on local\\ncitation context alone.', 'In this survey I should like to introduce some concepts of algebraic geometry\\nand try to demonstrate the fruitful interaction between algebraic geometry and\\ncomputer algebra and, more generally, between mathematics and computer science.\\nOne of the aims of this article is to show, by means of examples, the\\nusefulness of computer algebra to mathematical research.', 'The article presents some aspects on the use of computer in teaching general\\nrelativity for undergraduate students with some experience in computer\\nmanipulation. The article presents some simple algebraic programming (in\\nREDUCE+EXCALC package) procedures for obtaining and the study of some exact\\nsolutions of the Einstein equations in order to convince a dedicated student in\\ngeneral relativity about the utility of a computer algebra system.', \"Quantum Computing is a new and exciting field at the intersection of\\nmathematics, computer science and physics. It concerns a utilization of quantum\\nmechanics to improve the efficiency of computation. Here we present a gentle\\nintroduction to some of the ideas in quantum computing. The paper begins by\\nmotivating the central ideas of quantum mechanics and quantum computation with\\nsimple toy models. From there we move on to a formal presentation of the small\\nfraction of (finite dimensional) quantum mechanics that we will need for basic\\nquantum computation. Central notions of quantum architecture (qubits and\\nquantum gates) are described. The paper ends with a presentation of one of the\\nsimplest quantum algorithms: Deutsch's algorithm. Our presentation demands\\nneither advanced mathematics nor advanced physics.\", 'The paper presents a comparison of various soft computing techniques used for\\nfiltering and enhancing speech signals. The three major techniques that fall\\nunder soft computing are neural networks, fuzzy systems and genetic algorithms.\\nOther hybrid techniques such as neuro-fuzzy systems are also available. In\\ngeneral, soft computing techniques have been experimentally observed to give\\nfar superior performance as compared to non-soft computing techniques in terms\\nof robustness and accuracy.', 'We introduce a model of infinitary computation which enhances the infinite\\ntime Turing machine model slightly but in a natural way by giving the machines\\nthe capability of detecting cardinal stages of computation. The computational\\nstrength with respect to ITTMs is determined to be precisely that of the strong\\nhalting problem and the nature of the new characteristic ordinals (clockable,\\nwritable, etc.) is explored.', 'We have unified quantum and classical computing in open quantum systems\\ncalled qACP which is a quantum generalization of process algebra ACP. But, an\\naxiomatization for quantum and classical processes with an assumption of closed\\nquantum systems is still missing. For closed quantum systems, unitary operator,\\nquantum measurement and quantum entanglement are three basic components for\\nquantum computing. This leads to probability unavoidable. Along the solution of\\nqACP to unify quantum and classical computing in open quantum systems, we unify\\nquantum and classical computing with an assumption of closed systems under the\\nframework of ACP-like probabilistic process algebra. This unification make it\\ncan be used widely in verification for quantum and classical computing mixed\\nsystems, such as most quantum communication protocols.', 'This paper presents a study on the advancement of computational models for\\nthe analysis of illicit activities. Computational models are being adapted to\\naddress a number of social problems since the development of computers.\\nComputational model are divided into three categories and discussed that how\\ncomputational models can help in analyzing the illicit activities. The present\\nstudy sheds a new light on the area of research that will aid to researchers in\\nthe field as well as the law and enforcement agencies.', 'Understanding the properties of games played under computational constraints\\nremains challenging. For example, how do we expect rational (but\\ncomputationally bounded) players to play games with a prohibitively large\\nnumber of states, such as chess? This paper presents a novel model for the\\nprecomputation (preparing moves in advance) aspect of computationally\\nconstrained games. A fundamental trade-off is shown between randomness of play,\\nand susceptibility to precomputation, suggesting that randomization is\\nnecessary in games with computational constraints. We present efficient\\nalgorithms for computing how susceptible a strategy is to precomputation, and\\ncomputing an $\\\\epsilon$-Nash equilibrium of our model. Numerical experiments\\nmeasuring the trade-off between randomness and precomputation are provided for\\nStockfish (a well-known chess playing algorithm).', 'We introduce definitions of computable PAC learning for binary classification\\nover computable metric spaces. We provide sufficient conditions for learners\\nthat are empirical risk minimizers (ERM) to be computable, and bound the strong\\nWeihrauch degree of an ERM learner under more general conditions. We also give\\na presentation of a hypothesis class that does not admit any proper computable\\nPAC learner with computable sample function, despite the underlying class being\\nPAC learnable.', 'We show that for both the unary relation of transcendence and the finitary\\nrelation of algebraic independence on a field, the degree spectra of these\\nrelations may consist of any single computably enumerable Turing degree, or of\\nthose c.e. degrees above an arbitrary fixed $\\\\Delta^0_2$ degree. In other\\ncases, these spectra may be characterized by the ability to enumerate an\\narbitrary $\\\\Sigma^0_2$ set. This is the first proof that a computable field can\\nfail to have a computable copy with a computable transcendence basis.', 'Smart Contracts use computer technology to automate the performance of\\naspects of commercial agreements. Yet how can there be confidence that the\\ncomputer code is faithful to the intentions of the parties? To understand the\\ndepth and subtlety of this question requires an exploration of natural and\\ncomputer languages, of the semantics of expressions in those languages, and of\\nthe gap that exists between the disciplines of law and computer science. Here\\nwe provide a perspective on some of the key issues, explore some current\\nresearch directions, and explain the importance of language design in the\\ndevelopment of reliable Smart Contracts, including the specific methodology of\\nComputable Contracts.', 'This is an extended abstract for a presentation at The 17th International\\nConference on CUPUM - Computational Urban Planning and Urban Management in June\\n2021. This study presents an interdisciplinary synthesis of the\\nstate-of-the-art approaches in computer vision technologies to extract built\\nenvironment features that could improve the robustness of empirical research in\\nplanning. We discussed the findings from the review of studies in both planning\\nand computer science.', \"The physical Church thesis is a thesis about nature that expresses that all\\nthat can be computed by a physical system-a machine-is computable in the sense\\nof computability theory. At a first look, this thesis seems contradictory with\\nthe existence, in nature, of chaotic dynamical systems, that is systems whose\\nevolution cannot be ''computed'' because of their sensitivity to initial\\nconditions. The goal of this note is to show that there exist dynamical systems\\nthat are both computable and chaotic, and thus that the existence in nature of\\nchaotic dynamical system is not, per se, a refutation of the physical Church\\nthesis. Thus, chaos seems to be compatible with computability, in the same way\\nas it is compatible with determinism.\", 'This paper investigates the reproducibility of computational science research\\nand identifies key challenges facing the community today. It is the result of\\nthe First Summer School on Experimental Methodology in Computational Science\\nResearch (https://blogs.cs.st-andrews.ac.uk/emcsr2014/).\\n  First, we consider how to reproduce experiments that involve human subjects,\\nand in particular how to deal with different ethics requirements at different\\ninstitutions. Second, we look at whether parallel and distributed computational\\nexperiments are more or less reproducible than serial ones. Third, we consider\\nreproducible computational experiments from fields outside computer science.\\nOur final case study looks at whether reproducibility for one researcher is the\\nsame as for another, by having an author attempt to have others reproduce their\\nown, reproducible, paper.\\n  This paper is open, executable and reproducible: the whole process of writing\\nthis paper is captured in the source control repository hosting both the source\\nof the paper, supplementary codes and data; we are providing setup for several\\nexperiments on which we were working; finally, we try to describe what we have\\nachieved during the week of the school in a way that others may reproduce (and\\nhopefully improve) our experiments.', \"Even with recent increases in enrollments, computer science departments in\\nthe United States are not producing the number of graduates that the computing\\nworkforce needs, an issue that impacts the scientific and economic growth of\\nour country. Because the computer science field is predicted to grow, it is\\nimportant to draw from demographic groups that are growing in the US. At the\\nsame time, increasing the representation of students from minority groups will\\ninclude a more diverse perspective in the development of new technologies.\\nPrevious work has addressed the low representation of students of color in\\ncomputer science classes at the high-school level and explored what are the\\ncauses for those low numbers. In this paper, we study patterns of recruitment\\nand retention among minority students at a large R1 research university in\\norder to understand the unique challenges in racial and ethnic diversity that\\ncomputer science departments face. We use student data from a set of three core\\ncurriculum computer science classes at a large public research university and\\nanswer questions about the ethnic gap in our department, how it has changed\\nwith the recent increase in student enrollments, and how it changes as students\\nprogress through the major. We also analyze our students' intent to major when\\nthey are taking our introductory programming class, and how many of our CS1\\nstudents take more advanced classes. We measure retention rates for students in\\neach ethnic group, how do their prior experiences differ, if there is a\\ndifference between groups in how many of them change their minds about majoring\\nafter taking CS1, and whether or not their grades are correlated with a change\\nin their intent to major.We show that students from different race/ethnicity\\ngroups are not as different as it is perceived by the public.\", \"As we begin to reach the limits of classical computing, quantum computing has\\nemerged as a technology that has captured the imagination of the scientific\\nworld. While for many years, the ability to execute quantum algorithms was only\\na theoretical possibility, recent advances in hardware mean that quantum\\ncomputing devices now exist that can carry out quantum computation on a limited\\nscale. Thus it is now a real possibility, and of central importance at this\\ntime, to assess the potential impact of quantum computers on real problems of\\ninterest. One of the earliest and most compelling applications for quantum\\ncomputers is Feynman's idea of simulating quantum systems with many degrees of\\nfreedom. Such systems are found across chemistry, physics, and materials\\nscience. The particular way in which quantum computing extends classical\\ncomputing means that one cannot expect arbitrary simulations to be sped up by a\\nquantum computer, thus one must carefully identify areas where quantum\\nadvantage may be achieved. In this review, we briefly describe central problems\\nin chemistry and materials science, in areas of electronic structure, quantum\\nstatistical mechanics, and quantum dynamics, that are of potential interest for\\nsolution on a quantum computer. We then take a detailed snapshot of current\\nprogress in quantum algorithms for ground-state, dynamics, and thermal state\\nsimulation, and analyze their strengths and weaknesses for future developments.\", 'The advent of digital computing in the 1950s sparked a revolution in the\\nscience of weather and climate. Meteorology, long based on extrapolating\\npatterns in space and time, gave way to computational methods in a decade of\\nadvances in numerical weather forecasting. Those same methods also gave rise to\\ncomputational climate science, studying the behaviour of those same numerical\\nequations over intervals much longer than weather events, and changes in\\nexternal boundary conditions. Several subsequent decades of exponential growth\\nin computational power have brought us to the present day, where models ever\\ngrow in resolution and complexity, capable of mastery of many small-scale\\nphenomena with global repercussions, and ever more intricate feedbacks in the\\nEarth system.\\n  The current juncture in computing, seven decades later, heralds an end to\\nwhat is called Dennard scaling, the physics behind ever smaller computational\\nunits and ever faster arithmetic. This is prompting a fundamental change in our\\napproach to the simulation of weather and climate, potentially as revolutionary\\nas that wrought by John von Neumann in the 1950s. One approach could return us\\nto an earlier era of pattern recognition and extrapolation, this time aided by\\ncomputational power. Another approach could lead us to insights that continue\\nto be expressed in mathematical equations. In either approach, or any synthesis\\nof those, it is clearly no longer the steady march of the last few decades,\\ncontinuing to add detail to ever more elaborate models. In this prospectus, we\\nattempt to show the outlines of how this may unfold in the coming decades, a\\nnew harnessing of physical knowledge, computation, and data.', 'Computational thinking is a way of reasoning about the world in terms of\\ndata. This mindset channels number crunching toward an ambition to discover\\nknowledge through logic, models and simulations. Here we show how computational\\ncognitive science can be used to reconstruct and analyse the structure of\\ncomputational thinking mindsets (forma mentis in Latin) through complex\\nnetworks. As a case study, we investigate cognitive networks tied to key\\nconcepts of computational thinking provided by: (i) 159 high school students\\nenrolled in a science curriculum and (ii) 59 researchers in complex systems and\\nsimulations. Researchers\\' reconstructed forma mentis highlighted a positive\\nmindset about scientific modelling, semantically framing data and simulations\\nas ways of discovering nature. Students correctly identified different aspects\\nof logic reasoning but perceived \"computation\" as a distressing,\\nanxiety-eliciting task, framed with math jargon and lacking links to real-world\\ndiscovery. Students\\' mindsets around \"data\", \"model\" and \"simulations\"\\ncritically revealed no awareness of numerical modelling as a way for\\nunderstanding the world. Our findings provide evidence of a crippled\\ncomputational thinking mindset in students, who acquire mathematical skills\\nthat are not channelled toward real-world discovery through coding. This\\nunlinked knowledge ends up being perceived as distressing number-crunching\\nexpertise with no relevant outcome. The virtuous mindset of researchers\\nreported here indicates that computational thinking can be restored by training\\nstudents specifically in coding, modelling and simulations in relation to\\ndiscovering nature. Our approach opens innovative ways for quantifying\\ncomputational thinking and enhancing its development through mindset\\nreconstruction.', 'In the Hopfield model the ability of the network to generalization is studied\\nin the case of the network trained by one input image ({\\\\it the standard}).', 'The room temperature structure of aluminum, copper and gold infinite\\nnanowires is studied. The molecular dynamics simulation method and the same\\ntype of the embedded atom potentials made by Voter and coworkers are used. It\\nwas found that multi-shelled and various filled metallic nanowires exist\\ndepending on the metal and the initial configuration. The results were compared\\nwith previous simulations for gold nanowires using different type of the\\npotential.', 'Diffusion of ions through a fluctuating polymeric host is studied both by\\nMonte Carlo simulation of the complete system dynamics and by dynamic bond\\npercolation (DBP) theory. Comparison of both methods suggests a multiscale-like\\napproach for calculating the diffusion coefficients of the ions', 'By means of linear theory of elastoplasticity, solutions are given for screw\\nand edge dislocations situated in an isotropic solid. The force stresses,\\nstrain fields, displacements, distortions, dislocation densities and moment\\nstresses are calculated. The force stresses, strain fields, displacements and\\ndistortions are devoid of singularities predicted by the classical elasticity.\\nUsing the so-called stress function method we found modified stress functions\\nof screw and edge dislocations.', 'Low-dimensional free-standing aggregates of bare gold clusters are studied by\\nthe molecular dynamics simulation. Icosahedra of 55 and 147 atoms are\\nequilibrated at T=300 K. Then, their one- and two-dimensional assemblies are\\ninvestigated. It is found that icosahedra do not coalescence into large drops,\\nbut stable amorphous nanostructures are formed: nanowires for one-dimensional\\nand nanofilms for two-dimensional assemblies. The high-temperature stability of\\nthese nanostructures is also investigated.', 'Evolutionary role of the separation into two sexes from a cyberneticist\\'s\\npoint of view. [I translated this 1965 article from Russian \"Nauka i Zhizn\"\\n(Science and Life) in 1988. In a popular form, the article puts forward several\\nuseful ideas not all of which even today are necessarily well known or widely\\naccepted. Boris Lubachevsky, bdl@bell-labs.com ]', 'The chapter from the book introduces the delay theory, whose purpose is the\\nmodeling of the asynchronous circuits from digital electrical engineering with\\nordinary and differential pseudo-boolean equations.', 'A framework for simulation of dynamics of mechanical aggregates has been\\ndeveloped. This framework enables us to build model of aggregate from models of\\nits parts. Framework is a part of universal framework for science and\\nengineering.', 'We establish a connection between finite fields and finite dynamical systems.\\nWe show how this connection can be used to shed light on some problems in\\nfinite dynamical systems and in particular, in linear systems.', 'This article is composed of two parts; In the first part (Sec. 1), the\\nultra-large-scale electronic structure theory is reviewed for (i) its\\nfundamental numerical algorithm and (ii) its role in nano-material science. The\\nsecond part (Sec. 2) is devoted to the mathematical foundation of the\\nlarge-scale electronic structure theory and their numerical aspects.', 'This article addresses broad trends in interdisciplinary research in physics\\nwhere interactions with colleagues in fields such as computer science, ecology,\\nor economics can often be derailed by unintentional clashes of methodologies\\nand perspectives on the core science. Key causes of such breakdowns in\\ninterdisciplinary work are detailed and solutions offered.', 'The article presents a comparative analysis of the accuracy of the distance\\nto the observed object for geometric methods in noisy observations of\\nbearings-only information.', '1. Translated by Thomas E. Cecil, Department of Mathematics and Computer\\nScience, College of the Holy Cross, Worcester, MA 01610, USA; E-mail address:\\ncecil@mathcs.holycross.edu 2. Typed by Wenjiao Yan, School of Mathematical\\nSciences, Laboratory of Mathematics and Complex Systems, Beijing Normal\\nUniversity, Beijing 100875, China. E-mail address: wjyan@mail.bnu.edu.cn', 'The Synchrosqueezing transform is a time-frequency analysis method that can\\ndecompose complex signals into time-varying oscillatory components. It is a\\nform of time-frequency reassignment that is both sparse and invertible,\\nallowing for the recovery of the signal. This article presents an overview of\\nthe theory and stability properties of Synchrosqueezing, as well as\\napplications of the technique to topics in cardiology, climate science and\\neconomics.', 'Like the miniaturization of modern computers, next-generation radial velocity\\ninstruments will be significantly smaller and more powerful than their\\npredecessors.', \"This paper provides a concise summary of the framework of\\noperational-probabilistic theories, aimed at emphasizing the interaction\\nbetween category-theoretic and probabilistic structures. Within this framework,\\nwe review an operational version of the GNS construction, expressed by the\\nso-called purification principle, which under mild hypotheses leads to an\\noperational version of Stinespring's theorem.\", 'semanticSBML 2.0 is an online collection of services for the work with\\nbiochemical network models in SBML format.', 'This thesis analyses UK SPs with an informetric approach to study (1) the\\nrole of public science and HEIs in research and development (R&D) networks\\nassociated with SPs, and (2) the web-based patterns that reflect the\\nconfiguration of R&D support infrastructures associated with SPs.', 'This document is a short and informal tutorial on some aspects of calculating\\nphase diagrams with the ATAT-tools emc2 and phb and on creating cluster\\nexpansions with maps. It is neither complete, nor in any way an official\\ndocument, but mainly a set of collected notes I took during experimentation\\nwith ATAT.', 'We interpret Linear Logic Proof Nets in a term language based on Solos\\ncalculus. The system includes a synchronisation mechanism, obtained by a\\nconservative extension of the logic, that enables to define non-deterministic\\nbehaviours and multiparty sessions.', 'We describe how Python can be leveraged to streamline the curation, modelling\\nand dissemination of drug discovery data as well as the development of\\ninnovative, freely available tools for the related scientific community. We\\nlook at various examples, such as chemistry toolkits, machine-learning\\napplications and web frameworks and show how Python can glue it all together to\\ncreate efficient data science pipelines.', \"We solve Maxwell's eigenvalue problem via isogeometric boundary elements and\\na contour integral method. We discuss the analytic properties of the\\ndiscretisation, outline the implementation, and showcase numerical examples.\", 'We give an overview of the practice and science of call center workforce\\nplanning, where we evaluate the commonly used methods by their quality and the\\ntheory by its applicability. As such this paper is useful for developers and\\nconsultants interested in the background and advanced methodology of workforce\\nmanagement, and for researchers interested in practically relevant science.', 'In materials science and engineering, one is typically searching for\\nmaterials that exhibit exceptional performance for a certain function, and the\\nnumber of these materials is extremely small. Thus, statistically speaking, we\\nare interested in the identification of *rare phenomena*, and the scientific\\ndiscovery typically resembles the proverbial hunt for the needle in a haystack.', 'We show that, on the abstraction level of quantum circuit diagrams, quantum\\ncircuit algorithms belong to the species of interactive sequential algorithms\\nthat we studied in earlier work. This observation leads to a natural\\nspecification language for quantum circuit algorithms.', 'Many problems in science and engineering can be rigorously recast into\\nminimizing a suitable energy functional. We have been developing efficient and\\nflexible solution strategies to tackle various minimization problems by\\nemploying finite element discretization with P1 triangular elements [1,2]. An\\nextension to rectangular hp-finite elements in 2D is introduced in this\\ncontribution.', 'In this paper, we propose a shot percentage distribution strategy among the\\nplayers of a basketball team to maximize the score that can be achieved by\\nthem. The approach is based on the concepts of game theory related to network\\nflow.', 'The article is devoted Bluetooth wireless personal area networks\\nspecification, which provides standard for exchanging data over short\\ndistances. It is shown how the technology has evolved and its application in\\nthe design of devices. Health Device Profile considered in details, which the\\nmain feature is the work of a medical orientation devices.', 'Secure Multi-Party Computation (SMC) allows parties with similar background\\nto compute results upon their private data, minimizing the threat of\\ndisclosure. The exponential increase in sensitive data that needs to be passed\\nupon networked computers and the stupendous growth of internet has precipitated\\nvast opportunities for cooperative computation, where parties come together to\\nfacilitate computations and draw out conclusions that are mutually beneficial;\\nat the same time aspiring to keep their private data secure. These computations\\nare generally required to be done between competitors, who are obviously weary\\nof each-others intentions. SMC caters not only to the needs of such parties but\\nalso provides plausible solutions to individual organizations for problems like\\nprivacy-preserving database query, privacy-preserving scientific computations,\\nprivacy-preserving intrusion detection and privacy-preserving data mining. This\\npaper is an extension to a previously proposed protocol Encrytpo_Random, which\\npresented a plain sailing yet effective approach to SMC and also put forward an\\naptly crafted architecture, whereby such an efficient protocol, involving the\\nparties that have come forward for joint-computations and the third party who\\nundertakes such computations, can be developed. Through this extended work an\\nattempt has been made to further strengthen the existing protocol thus paving\\nthe way for a more secure multi-party computational process.', 'Gender bias in computing is a hard problem that has resisted decades of\\nresearch. One obstacle has been the absence of systematic data that might\\nindicate when gender bias emerged in computing and how it has changed. This\\narticle presents a new dataset (N=50,000) focusing on formative years of\\ncomputing as a profession (1950-1980) when U.S. government workforce statistics\\nare thin or non-existent. This longitudinal dataset, based on archival records\\nfrom six computer user groups (SHARE, USE, and others) and ACM conference\\nattendees and membership rosters, revises commonly held conjectures that gender\\nbias in computing emerged during professionalization of computer science in the\\n1960s or 1970s and that there was a \\'linear\\' one-time onset of gender bias to\\nthe present. Such a linear view also lent support to the \"pipeline\" model of\\ncomputing\\'s \"losing\" women at successive career stages. Instead, this dataset\\nreveals three distinct periods of gender bias in computing and so invites\\ntemporally distinct explanations for these changing dynamics. It significantly\\nrevises both scholarly assessment and popular understanding about gender bias\\nin computing. It also draws attention to diversity within computing. One\\nconsequence of this research for CS reform efforts today is data-driven\\nrecognition that legacies of gender bias beginning in the mid-1980s (not in\\nearlier decades) is the problem. A second consequence is correcting the public\\nimage of computer science: this research shows that gender bias is a contingent\\naspect of professional computing, not an intrinsic or permanent one.', 'It is indicated that principal models of computation are indeed significantly\\nrelated. The quantum field computation model contains the quantum computation\\nmodel of Feynman. (The term \"quantum field computer\" was used by Freedman.)\\nQuantum field computation (as enhanced by Wightman\\'s model of quantum field\\ntheory) involves computation over the continuum which is remarkably related to\\nthe real computation model of Smale. The latter model was established as a\\ngeneralization of Turing computation. All this is not surprising since it is\\nwell known that the physics of quantum field theory (which includes Einstein\\'s\\nspecial relativity) contains quantum mechanics which in turn contains classical\\nmechanics. The unity of these computing models, which seem to have grown\\nlargely independently, could shed new light into questions of computational\\ncomplexity, into the central P (Polynomial time) versus NP (Non-deterministic\\nPolynomial time) problem of computer science, and also into the description of\\nNature by fundamental physics theories.', 'In mathematics curves are typically defined as the images of continuous real\\nfunctions (parametrizations) defined on a closed interval. They can also be\\ndefined as connected one-dimensional compact subsets of points. For simple\\ncurves of finite lengths, parametrizations can be further required to be\\ninjective or even length-normalized. All of these four approaches to curves are\\nclassically equivalent. In this paper we investigate four different versions of\\ncomputable curves based on these four approaches. It turns out that they are\\nall different, and hence, we get four different classes of computable curves.\\nMore interestingly, these four classes are even point-separable in the sense\\nthat the sets of points covered by computable curves of different versions are\\nalso different. However, if we consider only computable curves of computable\\nlengths, then all four versions of computable curves become equivalent. This\\nshows that the definition of computable curves is robust, at least for those of\\ncomputable lengths. In addition, we show that the class of computable curves of\\ncomputable lengths is point-separable from the other four classes of computable\\ncurves.', \"Quantum computing technologies have become a hot topic in academia and\\nindustry receiving much attention and financial support from all sides.\\nBuilding a quantum computer that can be used practically is in itself an\\noutstanding challenge that has become the 'new race to the moon'. Next to\\nresearchers and vendors of future computing technologies, national authorities\\nare showing strong interest in maturing this technology due to its known\\npotential to break many of today's encryption techniques, which would have\\nsignificant impact on our society. It is however quite likely that quantum\\ncomputing has beneficial impact on many computational disciplines.\\n  In this article we describe our vision of future developments in scientific\\ncomputing that would be enabled by the advent of software-programmable quantum\\ncomputers. We thereby assume that quantum computers will form part of a hybrid\\naccelerated computing platform like GPUs and co-processor cards do today. In\\nparticular, we address the potential of quantum algorithms to bring major\\nbreakthroughs in applied mathematics and its applications. Finally, we give\\nseveral examples that demonstrate the possible impact of quantum-accelerated\\nscientific computing on society.\", 'Internet topology analysis has recently experienced a surge of interest in\\ncomputer science, physics, and the mathematical sciences. However, researchers\\nfrom these different disciplines tend to approach the same problem from\\ndifferent angles. As a result, the field of Internet topology analysis and\\nmodeling must untangle sets of inconsistent findings, conflicting claims, and\\ncontradicting statements.\\n  On May 10-12, 2006, CAIDA hosted the Workshop on Internet topology (WIT). By\\nbringing together a group of researchers spanning the areas of computer\\nscience, physics, and the mathematical sciences, the workshop aimed to improve\\ncommunication across these scientific disciplines, enable interdisciplinary\\ncrossfertilization, identify commonalities in the different approaches, promote\\nsynergy where it exists, and utilize the richness that results from exploring\\nsimilar problems from multiple perspectives.\\n  This report describes the findings of the workshop, outlines a set of\\nrelevant open research problems identified by participants, and concludes with\\nrecommendations that can benefit all scientific communities interested in\\nInternet topology research.', 'Statistics students need to develop the capacity to make sense of the\\nstaggering amount of information collected in our increasingly data-centered\\nworld. Data science is an important part of modern statistics, but our\\nintroductory and second statistics courses often neglect this fact. This paper\\ndiscusses ways to provide a practical foundation for students to learn to\\n\"compute with data\" as defined by Nolan and Temple Lang (2010), as well as\\ndevelop \"data habits of mind\" (Finzer, 2013). We describe how introductory and\\nsecond courses can integrate two key precursors to data science: the use of\\nreproducible analysis tools and access to large databases. By introducing\\nstudents to commonplace tools for data management, visualization, and\\nreproducible analysis in data science and applying these to real-world\\nscenarios, we prepare them to think statistically in the era of big data.', 'The reproduction and replication of reported scientific results is a hot\\ntopic within the academic community. The retraction of numerous studies from a\\nwide range of disciplines, from climate science to bioscience, has drawn the\\nfocus of many commentators, but there exists a wider socio-cultural problem\\nthat pervades the scientific community. Sharing code, data and models often\\nrequires extra effort; this is currently seen as a significant overhead that\\nmay not be worth the time investment.\\n  Automated systems, which allow easy reproduction of results, offer the\\npotential to incentivise a culture change and drive the adoption of new\\ntechniques to improve the efficiency of scientific exploration. In this paper,\\nwe discuss the value of improved access and sharing of the two key types of\\nresults arising from work done in the computational sciences: models and\\nalgorithms. We propose the development of an integrated cloud-based system\\nunderpinning computational science, linking together software and data\\nrepositories, toolchains, workflows and outputs, providing a seamless automated\\ninfrastructure for the verification and validation of scientific models and in\\nparticular, performance benchmarks.', 'This paper proposes a Bibliographic system intends to exchange bibliographic\\ninformation of survey/review articles by relying on Web service technology. It\\nallows researchers and university students to interact with system via single\\nservice using platform-independent standard named Web service to add, search\\nand retrieve bibliographic information of review articles in various science\\nand technology fields and build-up a dedicated database for these articles in\\neach science and technology field. Additionally, different implementation\\nscenarios of the proposed system are presented and described, and rich features\\nthat offered by such system are studied and described. However, this paper\\nexplains the proposed system using computing area due to the existence of\\ndetailed taxonomy of this area, which allows defining the system, their\\nfunctionalities and features provided. However, the proposed system is not only\\nconfined to computing area, it can support any other science and technology\\narea without any need to modify this system.', 'The Petaflops supercomputer \"Zhores\" recently launched in the \"Center for\\nComputational and Data-Intensive Science and Engineering\" (CDISE) of Skolkovo\\nInstitute of Science and Technology (Skoltech) opens up new exciting\\nopportunities for scientific discoveries in the institute especially in the\\nareas of data-driven modeling, machine learning and artificial intelligence.\\nThis supercomputer utilizes the latest generation of Intel and NVidia\\nprocessors to provide resources for the most compute intensive tasks of the\\nSkoltech scientists working in digital pharma, predictive analytics, photonics,\\nmaterial science, image processing, plasma physics and many more. Currently it\\nplaces 6th in the Russian and CIS TOP-50 (2018) supercomputer list. In this\\narticle we summarize the cluster properties and discuss the measured\\nperformance and usage modes of this scientific instrument in Skoltech.', \"We consider the use of Quantifier Elimination (QE) technology for automated\\nreasoning in economics. There is a great body of work considering QE\\napplications in science and engineering but we demonstrate here that it also\\nhas use in the social sciences. We explain how many suggested theorems in\\neconomics could either be proven, or even have their hypotheses shown to be\\ninconsistent, automatically via QE.\\n  However, economists who this technology could benefit are usually unfamiliar\\nwith QE, and the use of mathematical software generally. This motivated the\\ndevelopment of a Mathematica Package TheoryGuru, whose purpose is to lower the\\ncosts of applying QE to economics. We describe the package's functionality and\\ngive examples of its use.\", 'In this work, we exploit the power of \\\\emph{unambiguity} for the\\ncomplementation problem of B\\\\\"uchi automata by utilizing reduced run directed\\nacyclic graphs (DAGs) over infinite words, in which each vertex has at most one\\npredecessor. We then show how to use this type of reduced run DAGs as a\\n\\\\emph{unified tool} to optimize \\\\emph{both} rank-based and slice-based\\ncomplementation constructions for B\\\\\"uchi automata with a finite degree of\\nambiguity. As a result, given a B\\\\\"uchi automaton with $n$ states and a finite\\ndegree of ambiguity, the number of states in the complementary B\\\\\"uchi\\nautomaton constructed by the classical rank-based and slice-based\\ncomplementation constructions can be improved, respectively, to $2^{O(n)}$ from\\n$2^{O(n\\\\log n)}$ and to $O(4^n)$ from $O((3n)^n)$.', 'The European Open Science Cloud (EOSC) aims to create a federated environment\\nfor hosting and processing research data to support science in all disciplines\\nwithout geographical boundaries, such that data, software, methods and\\npublications can be shared as part of an Open Science community of practice.\\nThis work presents the ongoing activities related to the implementation of\\nvisual analytics services, integrated into EOSC, towards addressing the diverse\\nastrophysics user communities needs. These services rely on visualisation to\\nmanage the data life cycle process under FAIR principles, integrating data\\nprocessing for imaging and multidimensional map creation and mosaicing, and\\napplying machine learning techniques for detection of structures in large scale\\nmultidimensional maps.', 'Molecular science is governed by the dynamics of electrons, atomic nuclei,\\nand their interaction with electromagnetic fields. A reliable physicochemical\\nunderstanding of these processes is crucial for the design and synthesis of\\nchemicals and materials of economic value. Although some problems in this field\\nare adequately addressed by classical mechanics, many require an explicit\\nquantum mechanical description. Such quantum problems represented by\\nexponentially large wave function should naturally benefit from quantum\\ncomputation on a number of logical qubits that scales only linearly with system\\nsize. In this perspective, we focus on the potential of quantum computing for\\nsolving relevant problems in the molecular sciences -- molecular physics,\\nchemistry, biochemistry, and materials science.', 'Deep learning in molecular and materials sciences is limited by the lack of\\nintegration between applied science, artificial intelligence, and\\nhigh-performance computing. Bottlenecks with respect to the amount of training\\ndata, the size and complexity of model architectures, and the scale of the\\ncompute infrastructure are all key factors limiting the scaling of deep\\nlearning for molecules and materials. Here, we present $\\\\textit{LitMatter}$, a\\nlightweight framework for scaling molecular deep learning methods. We train\\nfour graph neural network architectures on over 400 GPUs and investigate the\\nscaling behavior of these methods. Depending on the model architecture,\\ntraining time speedups up to $60\\\\times$ are seen. Empirical neural scaling\\nrelations quantify the model-dependent scaling and enable optimal compute\\nresource allocation and the identification of scalable molecular geometric deep\\nlearning model implementations.', \"Data science and machine learning provide indispensable techniques for\\nunderstanding phenomena at scale, but the discretionary choices made when doing\\nthis work are often not recognized. Drawing from qualitative research\\npractices, we describe how the concepts of positionality and reflexivity can be\\nadapted to provide a framework for understanding, discussing, and disclosing\\nthe discretionary choices and subjectivity inherent to data science work. We\\nfirst introduce the concepts of model positionality and computational\\nreflexivity that can help data scientists to reflect on and communicate the\\nsocial and cultural context of a model's development and use, the data\\nannotators and their annotations, and the data scientists themselves. We then\\ndescribe the unique challenges of adapting these concepts for data science work\\nand offer annotator fingerprinting and position mining as promising solutions.\\nFinally, we demonstrate these techniques in a case study of the development of\\nclassifiers for toxic commenting in online communities.\", 'In our current era, numerical simulations have become indispensable\\ntheoretical and experimental tools for use in daily research activities,\\nparticularly in the materials science fields. However, the installation\\nprocesses for such simulations frequently become problematic because they\\ndepend strongly on the device environment, and troubleshooting those processes\\nis a challenging task for beginners. To minimize such difficulties, we created\\nMateriApps LIVE! and MateriApps Installer, which can solve most of the related\\nissues. Specifically, MateriApps LIVE! offers a virtual environment in which\\nusers can quickly try out computational materials science simulations on a\\npersonal computer while MateriApps Installer provides a comprehensive set of\\nshell scripts for use when installing software on Unix, Linux, macOS, and\\nsupercomputer systems. Herein, we provide detailed descriptions of MateriApps\\nLIVE! and MateriApps Installer together with illustrative examples of their\\nuse.', 'Computational thinking is a key skill for space science graduates, who must\\napply advanced problem-solving skills to model complex systems, analyse big\\ndata sets, and develop control software for mission-critical space systems. We\\ndescribe our work using Design Thinking to understand the challenges that\\nstudents face in learning these skills. In the MSc Space Science & Technology\\nat University College Dublin, we have used insights from this process to\\ndevelop new teaching strategies, including improved assessment rubrics,\\nsupported by workshops promoting collaborative programming techniques. We argue\\nthat postgraduate level space science courses play a valuable role in\\ndeveloping more advanced computational skills in early-career space scientists.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import arxiv\n",
        "from bertopic import BERTopic\n",
        "\n",
        "\n",
        "client = arxiv.Client()\n",
        "\n",
        "search = arxiv.Search(\n",
        "   query = \"Computer Science\",\n",
        "   max_results = 1000,\n",
        "   sort_by = arxiv.SortCriterion.Relevance\n",
        "   )\n",
        "\n",
        "#results = client.results(search)\n",
        "\n",
        "# `results` is a generator; you can iterate over its elements one by one...\n",
        "# for r in client.results(search):\n",
        "#    print(r.title)\n",
        "\n",
        "docs = [i.summary for i in client.results(search)]\n",
        "print(docs)\n",
        "topic_model = BERTopic()\n",
        "\n",
        "topics, probs = topic_model.fit_transform(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{-1: [('of', 0.034712953897213905),\n",
              "  ('the', 0.03272117309002147),\n",
              "  ('and', 0.029330099950928515),\n",
              "  ('in', 0.025335227420898103),\n",
              "  ('to', 0.02384405849880116),\n",
              "  ('is', 0.022203977367995096),\n",
              "  ('we', 0.019762845091612323),\n",
              "  ('this', 0.019696370290752278),\n",
              "  ('for', 0.0193329880675043),\n",
              "  ('that', 0.01788166028716705)],\n",
              " 0: [('and', 0.03607240481495613),\n",
              "  ('the', 0.03504603326552741),\n",
              "  ('cloud', 0.02916882680500009),\n",
              "  ('of', 0.028450039102677755),\n",
              "  ('computing', 0.028163281399001087),\n",
              "  ('data', 0.025688767387007646),\n",
              "  ('science', 0.025683612352434573),\n",
              "  ('in', 0.02462553713722123),\n",
              "  ('software', 0.02404580324069987),\n",
              "  ('to', 0.023373275969235083)],\n",
              " 1: [('quantum', 0.11959438475434432),\n",
              "  ('of', 0.03217010643229002),\n",
              "  ('the', 0.03197208747396101),\n",
              "  ('and', 0.029139492801456984),\n",
              "  ('computation', 0.02649160662695576),\n",
              "  ('to', 0.02538645964441019),\n",
              "  ('in', 0.023734871950262335),\n",
              "  ('is', 0.0235252232071906),\n",
              "  ('computing', 0.02219204655203711),\n",
              "  ('classical', 0.022015164249526153)],\n",
              " 2: [('students', 0.04037527943172035),\n",
              "  ('computer', 0.03422277976322599),\n",
              "  ('in', 0.03191238090722936),\n",
              "  ('science', 0.029866576276364836),\n",
              "  ('the', 0.029415760141200255),\n",
              "  ('to', 0.028596142217686218),\n",
              "  ('and', 0.028547117714732378),\n",
              "  ('of', 0.02606836116439766),\n",
              "  ('this', 0.020023592405741846),\n",
              "  ('for', 0.019274419348131178)],\n",
              " 3: [('game', 0.05992831739240417),\n",
              "  ('games', 0.04141991395496364),\n",
              "  ('we', 0.03648826468517426),\n",
              "  ('of', 0.033491510154942464),\n",
              "  ('the', 0.031845585460713685),\n",
              "  ('in', 0.02984599852798968),\n",
              "  ('and', 0.025686266385751957),\n",
              "  ('an', 0.02462618533597089),\n",
              "  ('that', 0.02398315030355377),\n",
              "  ('is', 0.022602432828338206)],\n",
              " 4: [('computable', 0.11760590802813639),\n",
              "  ('functions', 0.03719952133784067),\n",
              "  ('of', 0.03657656112700774),\n",
              "  ('that', 0.03642502793219287),\n",
              "  ('we', 0.03498114166314021),\n",
              "  ('is', 0.03333810314602456),\n",
              "  ('the', 0.03223991776348489),\n",
              "  ('spaces', 0.03212169400537875),\n",
              "  ('space', 0.02918839858425686),\n",
              "  ('computability', 0.026277241392855297)],\n",
              " 5: [('the', 0.036423308405616604),\n",
              "  ('science', 0.035619238606408574),\n",
              "  ('of', 0.03521892088524808),\n",
              "  ('and', 0.031189337272295403),\n",
              "  ('papers', 0.028811765927604663),\n",
              "  ('in', 0.028617441483671915),\n",
              "  ('citation', 0.024100256127709756),\n",
              "  ('computer', 0.023290230323463855),\n",
              "  ('research', 0.022072776095991543),\n",
              "  ('to', 0.022015002124692977)],\n",
              " 6: [('logic', 0.07524352662559781),\n",
              "  ('logics', 0.038518726528104154),\n",
              "  ('the', 0.0317415458289739),\n",
              "  ('of', 0.029818606023541548),\n",
              "  ('to', 0.028759885588149382),\n",
              "  ('is', 0.02712442309016075),\n",
              "  ('and', 0.02630700108431092),\n",
              "  ('for', 0.026233173235028568),\n",
              "  ('modal', 0.025056771591358804),\n",
              "  ('we', 0.023033736356652325)],\n",
              " 7: [('the', 0.040476796497501015),\n",
              "  ('numerical', 0.03838742983715164),\n",
              "  ('method', 0.036170927067601895),\n",
              "  ('of', 0.03612985486898549),\n",
              "  ('is', 0.032610664571888444),\n",
              "  ('and', 0.030104168700516454),\n",
              "  ('for', 0.02825382070319915),\n",
              "  ('by', 0.02272590961622112),\n",
              "  ('with', 0.020374978812078535),\n",
              "  ('order', 0.020355619235035545)],\n",
              " 8: [('computing', 0.035780393853171666),\n",
              "  ('algorithms', 0.034181106514971235),\n",
              "  ('optimization', 0.03173799084935762),\n",
              "  ('the', 0.030112439565485736),\n",
              "  ('to', 0.030066428866946195),\n",
              "  ('in', 0.029707871323632267),\n",
              "  ('of', 0.027505805556341648),\n",
              "  ('biological', 0.025717819653851722),\n",
              "  ('probability', 0.025491445793617413),\n",
              "  ('gradient', 0.024381441174522286)],\n",
              " 9: [('the', 0.04160038843854486),\n",
              "  ('proof', 0.03659889485558058),\n",
              "  ('of', 0.033880995677223365),\n",
              "  ('verification', 0.033083535545611296),\n",
              "  ('and', 0.027377421354561702),\n",
              "  ('to', 0.02733072321107216),\n",
              "  ('we', 0.026046553674923784),\n",
              "  ('epsilon', 0.02509301673483349),\n",
              "  ('for', 0.02491635232091443),\n",
              "  ('in', 0.02485592680933888)],\n",
              " 10: [('data', 0.10057217111850465),\n",
              "  ('science', 0.049626208319646904),\n",
              "  ('and', 0.04000619743859467),\n",
              "  ('to', 0.03150990549324695),\n",
              "  ('in', 0.031207563313559204),\n",
              "  ('the', 0.030893277003982447),\n",
              "  ('of', 0.02935182802980474),\n",
              "  ('research', 0.01914702844847452),\n",
              "  ('is', 0.017781881886072267),\n",
              "  ('scientists', 0.01743947442916981)],\n",
              " 11: [('materials', 0.08047248944767003),\n",
              "  ('science', 0.03920566468731627),\n",
              "  ('and', 0.0310411439639751),\n",
              "  ('the', 0.029515732685606484),\n",
              "  ('in', 0.028746242722519674),\n",
              "  ('for', 0.02839612529973676),\n",
              "  ('language', 0.02783337907909014),\n",
              "  ('models', 0.025752859617951884),\n",
              "  ('to', 0.02524703488801514),\n",
              "  ('tasks', 0.023332023601225015)],\n",
              " 12: [('type', 0.10592218779338647),\n",
              "  ('types', 0.05251307427112931),\n",
              "  ('we', 0.04863942496164792),\n",
              "  ('bisimulation', 0.041168396366189346),\n",
              "  ('of', 0.038126029865657864),\n",
              "  ('system', 0.037301015488383625),\n",
              "  ('theory', 0.035115965162952775),\n",
              "  ('the', 0.03318428835054252),\n",
              "  ('equality', 0.0313352984174185),\n",
              "  ('equivalence', 0.031026613130108828)],\n",
              " 13: [('algebras', 0.06094826701135824),\n",
              "  ('we', 0.0459905364583388),\n",
              "  ('lifting', 0.04073690704257348),\n",
              "  ('the', 0.03808519739450031),\n",
              "  ('of', 0.03541369662553769),\n",
              "  ('monads', 0.032972555372037256),\n",
              "  ('monad', 0.028262190318889078),\n",
              "  ('in', 0.028207241159040646),\n",
              "  ('symmetric', 0.02767540173148269),\n",
              "  ('by', 0.025900389127830176)],\n",
              " 14: [('citizen', 0.08322214621599247),\n",
              "  ('science', 0.04553587100853839),\n",
              "  ('projects', 0.03450731738023279),\n",
              "  ('of', 0.033147635178879524),\n",
              "  ('to', 0.03258162176244725),\n",
              "  ('and', 0.03172134195735447),\n",
              "  ('the', 0.030666259659188944),\n",
              "  ('in', 0.02757063044509414),\n",
              "  ('volunteers', 0.02309662577226082),\n",
              "  ('participation', 0.022627096518760256)],\n",
              " 15: [('analog', 0.04075186768498797),\n",
              "  ('computation', 0.040217137223416737),\n",
              "  ('the', 0.039699285487505384),\n",
              "  ('of', 0.033618620686075754),\n",
              "  ('to', 0.03228086833079389),\n",
              "  ('in', 0.026855748065359633),\n",
              "  ('system', 0.02312347005652384),\n",
              "  ('for', 0.022506819015624818),\n",
              "  ('we', 0.022111703915392706),\n",
              "  ('framework', 0.021931540221493833)],\n",
              " 16: [('data', 0.08319292827005142),\n",
              "  ('science', 0.044118972655579715),\n",
              "  ('statistics', 0.043510284624685786),\n",
              "  ('to', 0.039354535246437915),\n",
              "  ('students', 0.03788005746590809),\n",
              "  ('course', 0.03345378370936244),\n",
              "  ('and', 0.03268940576180894),\n",
              "  ('in', 0.026779438648175502),\n",
              "  ('the', 0.02568602307575675),\n",
              "  ('undergraduate', 0.024472950126320173)],\n",
              " 17: [('social', 0.05811814439435685),\n",
              "  ('ai', 0.04505914222286025),\n",
              "  ('and', 0.037359048321286684),\n",
              "  ('research', 0.03386641803630246),\n",
              "  ('the', 0.03230288334602921),\n",
              "  ('of', 0.029591692978701723),\n",
              "  ('science', 0.028335358818650097),\n",
              "  ('to', 0.02654979457987842),\n",
              "  ('in', 0.025916396740575644),\n",
              "  ('data', 0.02075623309297445)],\n",
              " 18: [('cognitive', 0.06661303977101829),\n",
              "  ('brain', 0.05344590568054503),\n",
              "  ('and', 0.03835668202130145),\n",
              "  ('the', 0.0381935670015615),\n",
              "  ('coc', 0.035433570069577314),\n",
              "  ('in', 0.03163733945670318),\n",
              "  ('of', 0.029483218064916838),\n",
              "  ('research', 0.028521863232652947),\n",
              "  ('to', 0.028521290493927338),\n",
              "  ('science', 0.026746677473635076)],\n",
              " 19: [('social', 0.09182736337399931),\n",
              "  ('the', 0.04165246999443949),\n",
              "  ('sciences', 0.034023676379943586),\n",
              "  ('and', 0.033427075585275946),\n",
              "  ('to', 0.03168836577291771),\n",
              "  ('birth', 0.03157610762715851),\n",
              "  ('of', 0.031132658426356118),\n",
              "  ('weight', 0.030836827893568536),\n",
              "  ('research', 0.026237130779231444),\n",
              "  ('from', 0.025638487309613777)],\n",
              " 20: [('well', 0.04971966500193323),\n",
              "  ('proof', 0.04819751521054766),\n",
              "  ('developments', 0.04698944000968079),\n",
              "  ('finite', 0.04589029004963573),\n",
              "  ('prove', 0.03984053258054309),\n",
              "  ('we', 0.03751673292997696),\n",
              "  ('lambda', 0.03625447945574119),\n",
              "  ('that', 0.033906325697226986),\n",
              "  ('sequential', 0.03344255634283252),\n",
              "  ('simple', 0.03268011029272424)]}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topic_model.get_topics(0)\n",
        "\n",
        "# relevant_topics = []\n",
        "\n",
        "# target_words = [\"computer\", \"science\", \"computational\", \"quantum\"]\n",
        "# for topic_id, topic in topic_model.get_topics().items():\n",
        "#     print(f\"The topic id:{topic_id}\")\n",
        "#     for word in topic:\n",
        "#         if word[0] in target_words:\n",
        "#             print(\"Topic: \", word[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5my-uNmAh86I",
        "outputId": "5371eb74-6012-49db-d1d0-433c83076fef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('analog', 0.052401690349920545),\n",
              " ('the', 0.04513673107241526),\n",
              " ('computation', 0.04499897369638944),\n",
              " ('of', 0.03948882033731411),\n",
              " ('to', 0.03707274772570282),\n",
              " ('in', 0.03150189255090072),\n",
              " ('computational', 0.029011731232998397),\n",
              " ('framework', 0.028638630718774268),\n",
              " ('for', 0.027396068867561626),\n",
              " ('we', 0.023865465779647578)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topic_model.get_topic(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbertopic\u001b[39;00m \u001b[39mimport\u001b[39;00m BERTopic\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m fetch_20newsgroups\n\u001b[0;32m----> 4\u001b[0m docs \u001b[39m=\u001b[39m fetch_20newsgroups(subset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[39m#topic_model = BERTopic()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#topics, probs = topic_model.fit_transform(docs)\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/datasets/_twenty_newsgroups.py:286\u001b[0m, in \u001b[0;36mfetch_20newsgroups\u001b[0;34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m download_if_missing:\n\u001b[1;32m    285\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mDownloading 20news dataset. This may take a few minutes.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 286\u001b[0m     cache \u001b[39m=\u001b[39m _download_20newsgroups(\n\u001b[1;32m    287\u001b[0m         target_dir\u001b[39m=\u001b[39;49mtwenty_home, cache_path\u001b[39m=\u001b[39;49mcache_path\n\u001b[1;32m    288\u001b[0m     )\n\u001b[1;32m    289\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m20Newsgroups dataset not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/datasets/_twenty_newsgroups.py:79\u001b[0m, in \u001b[0;36m_download_20newsgroups\u001b[0;34m(target_dir, cache_path)\u001b[0m\n\u001b[1;32m     76\u001b[0m archive_path \u001b[39m=\u001b[39m _fetch_remote(ARCHIVE, dirname\u001b[39m=\u001b[39mtarget_dir)\n\u001b[1;32m     78\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mDecompressing \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, archive_path)\n\u001b[0;32m---> 79\u001b[0m tarfile\u001b[39m.\u001b[39;49mopen(archive_path, \u001b[39m\"\u001b[39;49m\u001b[39mr:gz\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mextractall(path\u001b[39m=\u001b[39;49mtarget_dir)\n\u001b[1;32m     81\u001b[0m \u001b[39mwith\u001b[39;00m suppress(\u001b[39mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m     os\u001b[39m.\u001b[39mremove(archive_path)\n",
            "File \u001b[0;32m/usr/lib/python3.9/tarfile.py:2036\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   2034\u001b[0m         tarinfo\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m \u001b[39m0o700\u001b[39m\n\u001b[1;32m   2035\u001b[0m     \u001b[39m# Do not set_attrs directories, as we will do that further down\u001b[39;00m\n\u001b[0;32m-> 2036\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract(tarinfo, path, set_attrs\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m tarinfo\u001b[39m.\u001b[39;49misdir(),\n\u001b[1;32m   2037\u001b[0m                  numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[1;32m   2039\u001b[0m \u001b[39m# Reverse sort directories.\u001b[39;00m\n\u001b[1;32m   2040\u001b[0m directories\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m a: a\u001b[39m.\u001b[39mname)\n",
            "File \u001b[0;32m/usr/lib/python3.9/tarfile.py:2077\u001b[0m, in \u001b[0;36mTarFile.extract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2074\u001b[0m     tarinfo\u001b[39m.\u001b[39m_link_target \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, tarinfo\u001b[39m.\u001b[39mlinkname)\n\u001b[1;32m   2076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2077\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(tarinfo, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(path, tarinfo\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m   2078\u001b[0m                          set_attrs\u001b[39m=\u001b[39;49mset_attrs,\n\u001b[1;32m   2079\u001b[0m                          numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[1;32m   2080\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   2081\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrorlevel \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m/usr/lib/python3.9/tarfile.py:2150\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbg(\u001b[39m1\u001b[39m, tarinfo\u001b[39m.\u001b[39mname)\n\u001b[1;32m   2149\u001b[0m \u001b[39mif\u001b[39;00m tarinfo\u001b[39m.\u001b[39misreg():\n\u001b[0;32m-> 2150\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmakefile(tarinfo, targetpath)\n\u001b[1;32m   2151\u001b[0m \u001b[39melif\u001b[39;00m tarinfo\u001b[39m.\u001b[39misdir():\n\u001b[1;32m   2152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmakedir(tarinfo, targetpath)\n",
            "File \u001b[0;32m/usr/lib/python3.9/tarfile.py:2199\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2197\u001b[0m     target\u001b[39m.\u001b[39mtruncate()\n\u001b[1;32m   2198\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2199\u001b[0m     copyfileobj(source, target, tarinfo\u001b[39m.\u001b[39msize, ReadError, bufsize)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from bertopic import BERTopic\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "docs = fetch_20newsgroups(subset='test',  remove=('headers', 'footers', 'quotes'))['data']\n",
        "\n",
        "topic_model = BERTopic()\n",
        "topics, probs = topic_model.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.get_topic(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.get_topic(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.get_document_info(docs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPBIYQh/sAJ8t/FnYzSCbH2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
